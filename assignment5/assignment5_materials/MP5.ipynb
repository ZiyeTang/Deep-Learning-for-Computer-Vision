{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.25.2)\n",
      "Requirement already satisfied: pyvirtualdisplay in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.2.5)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: EasyProcess in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pyvirtualdisplay) (1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'sudo' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gym pyvirtualdisplay\n",
    "!sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (67.7.2)\n",
      "Requirement already satisfied: ez_setup in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.9)\n",
      "Requirement already satisfied: gym[atari] in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (0.0.8)\n",
      "Requirement already satisfied: ale-py~=0.7.5 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[atari]) (0.7.5)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n",
      "Requirement already satisfied: gym[accept-rom-license] in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[accept-rom-license]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[accept-rom-license]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[accept-rom-license]) (0.0.8)\n",
      "Requirement already satisfied: autorom[accept-rom-license]~=0.4.2 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym[accept-rom-license]) (0.4.2)\n",
      "Requirement already satisfied: click in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.3)\n",
      "Requirement already satisfied: requests in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.28.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.64.1)\n",
      "Requirement already satisfied: AutoROM.accept-rom-license in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.6.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from click->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade setuptools --user\n",
    "!pip3 install ez_setup \n",
    "!pip3 install gym[atari] \n",
    "!pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym==0.25.2 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym==0.25.2) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym==0.25.2) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\ziyet\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gym==0.25.2) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym==0.25.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN\n",
    "from config import *\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://www.gymlibrary.dev/environments/atari/breakout/. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziyet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\ziyet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\wrappers\\step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ziyet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\utils\\passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\ziyet\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_4404\\2880046822.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_4404\\2880046822.py:21: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 2.0   memory length: 215   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1   score: 0.0   memory length: 337   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 2   score: 2.0   memory length: 555   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 3   score: 2.0   memory length: 774   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 4   score: 2.0   memory length: 975   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 5   score: 2.0   memory length: 1195   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 6   score: 2.0   memory length: 1412   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.7142857142857142\n",
      "episode: 7   score: 3.0   memory length: 1657   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.875\n",
      "episode: 8   score: 3.0   memory length: 1883   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 9   score: 3.0   memory length: 2096   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 10   score: 2.0   memory length: 2313   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 2.090909090909091\n",
      "episode: 11   score: 3.0   memory length: 2561   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 2.1666666666666665\n",
      "episode: 12   score: 2.0   memory length: 2759   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.1538461538461537\n",
      "episode: 13   score: 1.0   memory length: 2910   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 2.0714285714285716\n",
      "episode: 14   score: 0.0   memory length: 3032   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.9333333333333333\n",
      "episode: 15   score: 2.0   memory length: 3248   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.9375\n",
      "episode: 16   score: 0.0   memory length: 3371   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.8235294117647058\n",
      "episode: 17   score: 0.0   memory length: 3494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7222222222222223\n",
      "episode: 18   score: 1.0   memory length: 3664   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
      "episode: 19   score: 0.0   memory length: 3786   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 20   score: 0.0   memory length: 3909   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5238095238095237\n",
      "episode: 21   score: 0.0   memory length: 4031   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 22   score: 0.0   memory length: 4154   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.391304347826087\n",
      "episode: 23   score: 1.0   memory length: 4326   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 24   score: 2.0   memory length: 4523   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 25   score: 2.0   memory length: 4721   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4230769230769231\n",
      "episode: 26   score: 0.0   memory length: 4844   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3703703703703705\n",
      "episode: 27   score: 0.0   memory length: 4967   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3214285714285714\n",
      "episode: 28   score: 0.0   memory length: 5089   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2758620689655173\n",
      "episode: 29   score: 0.0   memory length: 5211   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2333333333333334\n",
      "episode: 30   score: 2.0   memory length: 5429   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.2580645161290323\n",
      "episode: 31   score: 4.0   memory length: 5705   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.34375\n",
      "episode: 32   score: 1.0   memory length: 5874   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 33   score: 3.0   memory length: 6121   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3823529411764706\n",
      "episode: 34   score: 1.0   memory length: 6271   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3714285714285714\n",
      "episode: 35   score: 0.0   memory length: 6393   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 36   score: 0.0   memory length: 6516   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2972972972972974\n",
      "episode: 37   score: 2.0   memory length: 6714   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3157894736842106\n",
      "episode: 38   score: 0.0   memory length: 6837   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2820512820512822\n",
      "episode: 39   score: 2.0   memory length: 7034   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 40   score: 2.0   memory length: 7232   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3170731707317074\n",
      "episode: 41   score: 2.0   memory length: 7430   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 42   score: 4.0   memory length: 7710   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.3953488372093024\n",
      "episode: 43   score: 3.0   memory length: 7975   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.4318181818181819\n",
      "episode: 44   score: 1.0   memory length: 8126   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4222222222222223\n",
      "episode: 45   score: 0.0   memory length: 8249   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.391304347826087\n",
      "episode: 46   score: 0.0   memory length: 8372   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3617021276595744\n",
      "episode: 47   score: 2.0   memory length: 8589   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 48   score: 4.0   memory length: 8861   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 49   score: 2.0   memory length: 9077   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 50   score: 4.0   memory length: 9376   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.4901960784313726\n",
      "episode: 51   score: 0.0   memory length: 9499   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 52   score: 2.0   memory length: 9697   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.471698113207547\n",
      "episode: 53   score: 1.0   memory length: 9848   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.462962962962963\n",
      "episode: 54   score: 1.0   memory length: 10000   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4545454545454546\n",
      "episode: 55   score: 0.0   memory length: 10123   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 56   score: 0.0   memory length: 10246   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4035087719298245\n",
      "episode: 57   score: 3.0   memory length: 10492   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.4310344827586208\n",
      "episode: 58   score: 2.0   memory length: 10708   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4406779661016949\n",
      "episode: 59   score: 0.0   memory length: 10831   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4166666666666667\n",
      "episode: 60   score: 3.0   memory length: 11077   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.4426229508196722\n",
      "episode: 61   score: 0.0   memory length: 11199   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
      "episode: 62   score: 0.0   memory length: 11321   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3968253968253967\n",
      "episode: 63   score: 3.0   memory length: 11567   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 64   score: 1.0   memory length: 11717   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4153846153846155\n",
      "episode: 65   score: 3.0   memory length: 11964   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4393939393939394\n",
      "episode: 66   score: 1.0   memory length: 12133   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4328358208955223\n",
      "episode: 67   score: 2.0   memory length: 12349   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4411764705882353\n",
      "episode: 68   score: 0.0   memory length: 12472   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4202898550724639\n",
      "episode: 69   score: 0.0   memory length: 12595   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 70   score: 3.0   memory length: 12841   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.4225352112676057\n",
      "episode: 71   score: 0.0   memory length: 12964   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4027777777777777\n",
      "episode: 72   score: 0.0   memory length: 13087   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3835616438356164\n",
      "episode: 73   score: 0.0   memory length: 13210   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.364864864864865\n",
      "episode: 74   score: 2.0   memory length: 13408   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3733333333333333\n",
      "episode: 75   score: 2.0   memory length: 13608   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.381578947368421\n",
      "episode: 76   score: 0.0   memory length: 13730   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 77   score: 0.0   memory length: 13853   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3461538461538463\n",
      "episode: 78   score: 5.0   memory length: 14157   epsilon: 1.0    steps: 304    lr: 0.0001     evaluation reward: 1.3924050632911393\n",
      "episode: 79   score: 2.0   memory length: 14354   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 80   score: 2.0   memory length: 14552   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4074074074074074\n",
      "episode: 81   score: 2.0   memory length: 14771   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4146341463414633\n",
      "episode: 82   score: 1.0   memory length: 14940   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4096385542168675\n",
      "episode: 83   score: 0.0   memory length: 15062   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3928571428571428\n",
      "episode: 84   score: 1.0   memory length: 15213   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.388235294117647\n",
      "episode: 85   score: 1.0   memory length: 15364   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3837209302325582\n",
      "episode: 86   score: 0.0   memory length: 15486   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.367816091954023\n",
      "episode: 87   score: 2.0   memory length: 15703   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.375\n",
      "episode: 88   score: 3.0   memory length: 15950   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3932584269662922\n",
      "episode: 89   score: 0.0   memory length: 16073   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3777777777777778\n",
      "episode: 90   score: 0.0   memory length: 16196   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3626373626373627\n",
      "episode: 91   score: 5.0   memory length: 16523   epsilon: 1.0    steps: 327    lr: 0.0001     evaluation reward: 1.4021739130434783\n",
      "episode: 92   score: 3.0   memory length: 16770   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.4193548387096775\n",
      "episode: 93   score: 1.0   memory length: 16939   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4148936170212767\n",
      "episode: 94   score: 3.0   memory length: 17185   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.431578947368421\n",
      "episode: 95   score: 1.0   memory length: 17337   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.4270833333333333\n",
      "episode: 96   score: 0.0   memory length: 17459   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4123711340206186\n",
      "episode: 97   score: 1.0   memory length: 17610   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4081632653061225\n",
      "episode: 98   score: 0.0   memory length: 17733   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.393939393939394\n",
      "episode: 99   score: 1.0   memory length: 17903   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 100   score: 1.0   memory length: 18053   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 101   score: 2.0   memory length: 18256   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 102   score: 2.0   memory length: 18454   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 103   score: 3.0   memory length: 18717   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 104   score: 1.0   memory length: 18886   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 105   score: 2.0   memory length: 19102   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 106   score: 0.0   memory length: 19225   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 107   score: 2.0   memory length: 19407   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 108   score: 2.0   memory length: 19605   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 109   score: 3.0   memory length: 19831   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 110   score: 1.0   memory length: 20003   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 111   score: 2.0   memory length: 20221   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 112   score: 0.0   memory length: 20343   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 113   score: 4.0   memory length: 20636   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 114   score: 2.0   memory length: 20855   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 115   score: 0.0   memory length: 20978   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 116   score: 0.0   memory length: 21101   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 117   score: 1.0   memory length: 21270   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 118   score: 1.0   memory length: 21421   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 119   score: 2.0   memory length: 21602   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 120   score: 1.0   memory length: 21771   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 121   score: 0.0   memory length: 21894   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 122   score: 1.0   memory length: 22045   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 123   score: 3.0   memory length: 22309   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 124   score: 6.0   memory length: 22722   epsilon: 1.0    steps: 413    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 125   score: 3.0   memory length: 22969   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 126   score: 1.0   memory length: 23138   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 127   score: 2.0   memory length: 23318   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 128   score: 2.0   memory length: 23516   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   score: 2.0   memory length: 23714   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 130   score: 4.0   memory length: 23991   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 131   score: 3.0   memory length: 24237   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 132   score: 4.0   memory length: 24533   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 133   score: 3.0   memory length: 24778   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 134   score: 2.0   memory length: 24995   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 135   score: 2.0   memory length: 25213   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 136   score: 0.0   memory length: 25336   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 137   score: 0.0   memory length: 25459   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 138   score: 4.0   memory length: 25721   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 139   score: 0.0   memory length: 25844   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 140   score: 1.0   memory length: 25995   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 141   score: 1.0   memory length: 26146   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 142   score: 1.0   memory length: 26315   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 143   score: 2.0   memory length: 26513   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 144   score: 2.0   memory length: 26715   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 145   score: 0.0   memory length: 26837   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 146   score: 2.0   memory length: 27034   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 147   score: 3.0   memory length: 27279   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 148   score: 4.0   memory length: 27576   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 149   score: 4.0   memory length: 27838   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 150   score: 1.0   memory length: 28007   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 151   score: 2.0   memory length: 28228   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 152   score: 4.0   memory length: 28544   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 153   score: 4.0   memory length: 28795   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 154   score: 3.0   memory length: 29062   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 155   score: 0.0   memory length: 29185   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 156   score: 2.0   memory length: 29404   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 157   score: 1.0   memory length: 29555   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 158   score: 1.0   memory length: 29723   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 159   score: 2.0   memory length: 29941   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 160   score: 2.0   memory length: 30139   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 161   score: 1.0   memory length: 30290   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 162   score: 0.0   memory length: 30412   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 163   score: 1.0   memory length: 30581   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 164   score: 4.0   memory length: 30876   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 165   score: 1.0   memory length: 31045   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 166   score: 0.0   memory length: 31167   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 167   score: 2.0   memory length: 31365   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 168   score: 0.0   memory length: 31487   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 169   score: 0.0   memory length: 31609   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 170   score: 2.0   memory length: 31807   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 171   score: 2.0   memory length: 32005   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 172   score: 3.0   memory length: 32253   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 173   score: 0.0   memory length: 32376   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 174   score: 2.0   memory length: 32594   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 175   score: 1.0   memory length: 32745   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 176   score: 0.0   memory length: 32868   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 177   score: 2.0   memory length: 33086   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 178   score: 3.0   memory length: 33333   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 179   score: 2.0   memory length: 33548   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 180   score: 0.0   memory length: 33671   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 181   score: 0.0   memory length: 33794   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 182   score: 3.0   memory length: 34020   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 183   score: 0.0   memory length: 34143   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 184   score: 0.0   memory length: 34265   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 185   score: 4.0   memory length: 34543   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 186   score: 2.0   memory length: 34741   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 187   score: 2.0   memory length: 34941   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 188   score: 2.0   memory length: 35138   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 189   score: 2.0   memory length: 35354   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 190   score: 3.0   memory length: 35600   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 191   score: 1.0   memory length: 35750   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 192   score: 2.0   memory length: 35968   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 193   score: 0.0   memory length: 36091   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 194   score: 1.0   memory length: 36260   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 195   score: 1.0   memory length: 36411   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 196   score: 1.0   memory length: 36579   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 197   score: 0.0   memory length: 36702   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 198   score: 1.0   memory length: 36872   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 199   score: 3.0   memory length: 37118   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 200   score: 1.0   memory length: 37288   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 201   score: 3.0   memory length: 37532   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 202   score: 2.0   memory length: 37730   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 203   score: 1.0   memory length: 37902   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 204   score: 0.0   memory length: 38025   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 205   score: 2.0   memory length: 38242   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 206   score: 0.0   memory length: 38364   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 207   score: 0.0   memory length: 38487   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 208   score: 0.0   memory length: 38610   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 209   score: 4.0   memory length: 38925   epsilon: 1.0    steps: 315    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 210   score: 0.0   memory length: 39047   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 211   score: 2.0   memory length: 39264   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 212   score: 4.0   memory length: 39561   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 213   score: 0.0   memory length: 39683   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 214   score: 2.0   memory length: 39901   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 215   score: 3.0   memory length: 40146   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 216   score: 1.0   memory length: 40297   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 217   score: 1.0   memory length: 40448   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 218   score: 2.0   memory length: 40646   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 219   score: 1.0   memory length: 40797   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 220   score: 3.0   memory length: 41046   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 221   score: 1.0   memory length: 41197   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 222   score: 1.0   memory length: 41368   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 223   score: 0.0   memory length: 41490   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 224   score: 1.0   memory length: 41658   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 225   score: 1.0   memory length: 41826   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 226   score: 2.0   memory length: 42045   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 227   score: 2.0   memory length: 42243   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 228   score: 0.0   memory length: 42366   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 229   score: 3.0   memory length: 42613   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 230   score: 0.0   memory length: 42736   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 231   score: 5.0   memory length: 43081   epsilon: 1.0    steps: 345    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 232   score: 0.0   memory length: 43204   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 233   score: 2.0   memory length: 43422   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 234   score: 3.0   memory length: 43665   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 235   score: 4.0   memory length: 43979   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 236   score: 1.0   memory length: 44148   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 237   score: 3.0   memory length: 44375   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 238   score: 3.0   memory length: 44620   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 239   score: 0.0   memory length: 44743   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 240   score: 3.0   memory length: 44989   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 241   score: 2.0   memory length: 45186   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 242   score: 4.0   memory length: 45505   epsilon: 1.0    steps: 319    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 243   score: 1.0   memory length: 45655   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 244   score: 0.0   memory length: 45778   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 245   score: 1.0   memory length: 45947   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 246   score: 2.0   memory length: 46145   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 247   score: 0.0   memory length: 46268   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 248   score: 2.0   memory length: 46465   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 249   score: 0.0   memory length: 46587   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 250   score: 3.0   memory length: 46833   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 251   score: 3.0   memory length: 47080   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 252   score: 2.0   memory length: 47298   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 253   score: 1.0   memory length: 47468   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 254   score: 1.0   memory length: 47618   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 255   score: 0.0   memory length: 47741   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 256   score: 1.0   memory length: 47910   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 257   score: 2.0   memory length: 48108   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 258   score: 1.0   memory length: 48279   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 259   score: 9.0   memory length: 48758   epsilon: 1.0    steps: 479    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 260   score: 2.0   memory length: 48955   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 261   score: 0.0   memory length: 49078   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 262   score: 2.0   memory length: 49277   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 263   score: 3.0   memory length: 49544   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 264   score: 3.0   memory length: 49774   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 265   score: 2.0   memory length: 49972   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 266   score: 2.0   memory length: 50188   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 267   score: 2.0   memory length: 50385   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 268   score: 1.0   memory length: 50554   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 269   score: 3.0   memory length: 50805   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 270   score: 0.0   memory length: 50928   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 271   score: 1.0   memory length: 51100   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 272   score: 1.0   memory length: 51250   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 273   score: 1.0   memory length: 51419   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 274   score: 2.0   memory length: 51636   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 275   score: 2.0   memory length: 51834   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 276   score: 1.0   memory length: 51985   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 277   score: 0.0   memory length: 52108   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 278   score: 0.0   memory length: 52231   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 279   score: 0.0   memory length: 52354   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 280   score: 0.0   memory length: 52476   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 281   score: 0.0   memory length: 52599   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 282   score: 0.0   memory length: 52722   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 283   score: 2.0   memory length: 52941   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 284   score: 1.0   memory length: 53093   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 285   score: 2.0   memory length: 53291   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 286   score: 4.0   memory length: 53605   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 287   score: 0.0   memory length: 53728   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 288   score: 0.0   memory length: 53851   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 289   score: 0.0   memory length: 53974   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 290   score: 1.0   memory length: 54142   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 291   score: 1.0   memory length: 54314   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 292   score: 0.0   memory length: 54437   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 293   score: 2.0   memory length: 54635   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 294   score: 1.0   memory length: 54804   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 295   score: 2.0   memory length: 55002   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 296   score: 0.0   memory length: 55125   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 297   score: 0.0   memory length: 55248   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 298   score: 2.0   memory length: 55446   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 299   score: 0.0   memory length: 55569   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 300   score: 1.0   memory length: 55739   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 301   score: 2.0   memory length: 55937   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 302   score: 2.0   memory length: 56135   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 303   score: 0.0   memory length: 56258   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 304   score: 2.0   memory length: 56475   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 305   score: 3.0   memory length: 56719   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 306   score: 2.0   memory length: 56939   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 307   score: 3.0   memory length: 57165   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 308   score: 1.0   memory length: 57336   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 309   score: 0.0   memory length: 57458   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 310   score: 4.0   memory length: 57736   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 311   score: 1.0   memory length: 57905   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 312   score: 1.0   memory length: 58055   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 313   score: 2.0   memory length: 58234   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 314   score: 1.0   memory length: 58384   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 315   score: 0.0   memory length: 58507   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 316   score: 1.0   memory length: 58677   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 317   score: 1.0   memory length: 58849   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 318   score: 3.0   memory length: 59115   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 319   score: 2.0   memory length: 59332   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 320   score: 1.0   memory length: 59501   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 321   score: 3.0   memory length: 59769   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 322   score: 2.0   memory length: 59967   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 323   score: 1.0   memory length: 60136   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 324   score: 0.0   memory length: 60258   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 325   score: 0.0   memory length: 60381   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 326   score: 1.0   memory length: 60550   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 327   score: 0.0   memory length: 60673   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 328   score: 2.0   memory length: 60871   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 329   score: 0.0   memory length: 60994   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 330   score: 4.0   memory length: 61269   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 331   score: 0.0   memory length: 61392   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 332   score: 2.0   memory length: 61610   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 333   score: 2.0   memory length: 61825   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 334   score: 2.0   memory length: 62023   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 335   score: 5.0   memory length: 62339   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 336   score: 2.0   memory length: 62536   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 337   score: 4.0   memory length: 62792   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 338   score: 0.0   memory length: 62915   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 339   score: 0.0   memory length: 63038   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 340   score: 0.0   memory length: 63161   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 341   score: 2.0   memory length: 63360   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 342   score: 0.0   memory length: 63483   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 343   score: 3.0   memory length: 63711   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 344   score: 1.0   memory length: 63882   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 345   score: 1.0   memory length: 64051   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 346   score: 0.0   memory length: 64174   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 347   score: 0.0   memory length: 64297   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 348   score: 1.0   memory length: 64447   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 349   score: 1.0   memory length: 64616   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 350   score: 3.0   memory length: 64865   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 351   score: 0.0   memory length: 64988   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 352   score: 1.0   memory length: 65139   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 353   score: 2.0   memory length: 65339   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 354   score: 3.0   memory length: 65605   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 355   score: 2.0   memory length: 65823   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 356   score: 2.0   memory length: 66021   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 357   score: 1.0   memory length: 66191   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 358   score: 2.0   memory length: 66394   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 359   score: 0.0   memory length: 66517   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 360   score: 2.0   memory length: 66698   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 361   score: 2.0   memory length: 66917   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 362   score: 0.0   memory length: 67040   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 363   score: 0.0   memory length: 67162   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 364   score: 1.0   memory length: 67331   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 365   score: 3.0   memory length: 67562   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 366   score: 1.0   memory length: 67731   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 367   score: 0.0   memory length: 67854   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 368   score: 3.0   memory length: 68100   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 369   score: 2.0   memory length: 68287   epsilon: 1.0    steps: 187    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 370   score: 3.0   memory length: 68532   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 371   score: 2.0   memory length: 68750   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 372   score: 2.0   memory length: 68948   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 373   score: 2.0   memory length: 69170   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 374   score: 2.0   memory length: 69371   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 375   score: 0.0   memory length: 69494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 376   score: 2.0   memory length: 69710   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 377   score: 1.0   memory length: 69881   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 378   score: 1.0   memory length: 70049   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 379   score: 2.0   memory length: 70247   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 380   score: 0.0   memory length: 70370   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 381   score: 2.0   memory length: 70568   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 382   score: 3.0   memory length: 70834   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 383   score: 1.0   memory length: 70985   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 384   score: 3.0   memory length: 71250   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 385   score: 0.0   memory length: 71372   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 386   score: 1.0   memory length: 71524   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 387   score: 2.0   memory length: 71741   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 388   score: 2.0   memory length: 71940   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 389   score: 2.0   memory length: 72120   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 390   score: 3.0   memory length: 72368   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 391   score: 0.0   memory length: 72491   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 392   score: 2.0   memory length: 72710   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 393   score: 5.0   memory length: 73035   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 394   score: 1.0   memory length: 73205   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 395   score: 2.0   memory length: 73426   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 396   score: 2.0   memory length: 73624   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 397   score: 3.0   memory length: 73886   epsilon: 1.0    steps: 262    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 398   score: 1.0   memory length: 74055   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 399   score: 0.0   memory length: 74178   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 400   score: 1.0   memory length: 74347   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 401   score: 0.0   memory length: 74469   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 402   score: 0.0   memory length: 74592   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 403   score: 3.0   memory length: 74803   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 404   score: 1.0   memory length: 74955   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 405   score: 2.0   memory length: 75156   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 406   score: 0.0   memory length: 75279   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 407   score: 1.0   memory length: 75449   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 408   score: 4.0   memory length: 75746   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 409   score: 3.0   memory length: 75992   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 410   score: 2.0   memory length: 76189   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 411   score: 1.0   memory length: 76339   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 412   score: 2.0   memory length: 76518   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 413   score: 2.0   memory length: 76715   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 414   score: 2.0   memory length: 76913   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 415   score: 3.0   memory length: 77139   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 416   score: 6.0   memory length: 77513   epsilon: 1.0    steps: 374    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 417   score: 0.0   memory length: 77636   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 418   score: 1.0   memory length: 77808   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 419   score: 1.0   memory length: 77980   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 420   score: 1.0   memory length: 78131   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 421   score: 2.0   memory length: 78328   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 422   score: 3.0   memory length: 78594   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 423   score: 4.0   memory length: 78886   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 424   score: 0.0   memory length: 79009   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 425   score: 1.0   memory length: 79177   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 426   score: 1.0   memory length: 79328   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 427   score: 0.0   memory length: 79451   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 428   score: 2.0   memory length: 79649   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 429   score: 3.0   memory length: 79895   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 430   score: 2.0   memory length: 80093   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 431   score: 0.0   memory length: 80215   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 432   score: 1.0   memory length: 80383   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 433   score: 0.0   memory length: 80506   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 434   score: 2.0   memory length: 80706   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 435   score: 2.0   memory length: 80903   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 436   score: 2.0   memory length: 81102   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 437   score: 3.0   memory length: 81331   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 438   score: 1.0   memory length: 81482   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 439   score: 0.0   memory length: 81605   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 440   score: 2.0   memory length: 81821   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 441   score: 1.0   memory length: 81990   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 442   score: 2.0   memory length: 82208   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 443   score: 1.0   memory length: 82359   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 444   score: 2.0   memory length: 82559   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 445   score: 0.0   memory length: 82681   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 446   score: 0.0   memory length: 82804   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 447   score: 1.0   memory length: 82976   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 448   score: 2.0   memory length: 83174   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 449   score: 1.0   memory length: 83344   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 450   score: 3.0   memory length: 83593   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 451   score: 3.0   memory length: 83841   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 452   score: 1.0   memory length: 83993   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 453   score: 3.0   memory length: 84221   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 454   score: 2.0   memory length: 84419   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 455   score: 2.0   memory length: 84617   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 456   score: 2.0   memory length: 84814   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 457   score: 1.0   memory length: 84965   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 458   score: 2.0   memory length: 85162   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 459   score: 0.0   memory length: 85285   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 460   score: 1.0   memory length: 85436   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 461   score: 0.0   memory length: 85558   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 462   score: 4.0   memory length: 85830   epsilon: 1.0    steps: 272    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 463   score: 1.0   memory length: 85999   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 464   score: 3.0   memory length: 86225   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 465   score: 2.0   memory length: 86441   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 466   score: 3.0   memory length: 86653   epsilon: 1.0    steps: 212    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 467   score: 2.0   memory length: 86872   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 468   score: 2.0   memory length: 87087   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 469   score: 0.0   memory length: 87210   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 470   score: 0.0   memory length: 87332   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 471   score: 3.0   memory length: 87599   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 472   score: 2.0   memory length: 87816   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 473   score: 0.0   memory length: 87938   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 474   score: 2.0   memory length: 88156   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 475   score: 2.0   memory length: 88353   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 476   score: 2.0   memory length: 88552   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 477   score: 1.0   memory length: 88703   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 478   score: 2.0   memory length: 88884   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 479   score: 2.0   memory length: 89102   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 480   score: 1.0   memory length: 89271   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 481   score: 2.0   memory length: 89468   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 482   score: 2.0   memory length: 89668   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 483   score: 0.0   memory length: 89791   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 484   score: 1.0   memory length: 89941   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 485   score: 2.0   memory length: 90139   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 486   score: 2.0   memory length: 90339   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 487   score: 1.0   memory length: 90510   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 488   score: 4.0   memory length: 90826   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 489   score: 0.0   memory length: 90948   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 490   score: 2.0   memory length: 91146   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 491   score: 3.0   memory length: 91371   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 492   score: 2.0   memory length: 91569   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 493   score: 2.0   memory length: 91766   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 494   score: 0.0   memory length: 91889   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 495   score: 1.0   memory length: 92058   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 496   score: 0.0   memory length: 92180   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 497   score: 4.0   memory length: 92474   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 498   score: 3.0   memory length: 92720   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 499   score: 4.0   memory length: 93012   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 500   score: 0.0   memory length: 93135   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 501   score: 2.0   memory length: 93333   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 502   score: 0.0   memory length: 93456   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 503   score: 1.0   memory length: 93627   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 504   score: 1.0   memory length: 93795   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 505   score: 1.0   memory length: 93946   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 506   score: 0.0   memory length: 94068   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 507   score: 2.0   memory length: 94268   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 508   score: 0.0   memory length: 94391   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 509   score: 0.0   memory length: 94513   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 510   score: 3.0   memory length: 94738   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 511   score: 0.0   memory length: 94861   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 512   score: 2.0   memory length: 95076   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 513   score: 1.0   memory length: 95228   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 514   score: 0.0   memory length: 95351   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 515   score: 1.0   memory length: 95521   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 516   score: 1.0   memory length: 95691   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 517   score: 0.0   memory length: 95814   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 518   score: 1.0   memory length: 95965   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 519   score: 0.0   memory length: 96087   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 520   score: 3.0   memory length: 96334   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 521   score: 0.0   memory length: 96457   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 522   score: 0.0   memory length: 96580   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 523   score: 4.0   memory length: 96859   epsilon: 1.0    steps: 279    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 524   score: 0.0   memory length: 96981   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 525   score: 2.0   memory length: 97179   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 526   score: 1.0   memory length: 97348   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 527   score: 2.0   memory length: 97566   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 528   score: 1.0   memory length: 97734   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 529   score: 2.0   memory length: 97935   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 530   score: 1.0   memory length: 98106   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 531   score: 2.0   memory length: 98324   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 532   score: 4.0   memory length: 98604   epsilon: 1.0    steps: 280    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 533   score: 0.0   memory length: 98726   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 534   score: 2.0   memory length: 98929   epsilon: 1.0    steps: 203    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 535   score: 2.0   memory length: 99127   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 536   score: 0.0   memory length: 99249   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 537   score: 0.0   memory length: 99372   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 538   score: 3.0   memory length: 99617   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 539   score: 3.0   memory length: 99864   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\assignment5\\assignment5_materials\\memory.py:30: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample = np.array(sample, dtype = object)\n",
      "C:\\assignment5\\assignment5_materials\\agent.py:63: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  mini_batch = np.array(mini_batch, dtype = object).transpose()\n",
      "C:\\assignment5\\assignment5_materials\\agent.py:91: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen/native/IndexingUtils.h:28.)\n",
      "  next_state_values[non_final_mask] = self.policy_net(non_final_next_states).max(1)[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 540   score: 1.0   memory length: 100035   epsilon: 0.9999287200000015    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 541   score: 0.0   memory length: 100158   epsilon: 0.9996851800000068    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 542   score: 2.0   memory length: 100376   epsilon: 0.9992535400000162    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 543   score: 2.0   memory length: 100593   epsilon: 0.9988238800000255    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 544   score: 4.0   memory length: 100870   epsilon: 0.9982754200000374    steps: 277    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 545   score: 2.0   memory length: 101050   epsilon: 0.9979190200000452    steps: 180    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 546   score: 0.0   memory length: 101172   epsilon: 0.9976774600000504    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 547   score: 2.0   memory length: 101388   epsilon: 0.9972497800000597    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 548   score: 3.0   memory length: 101631   epsilon: 0.9967686400000701    steps: 243    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 549   score: 0.0   memory length: 101754   epsilon: 0.9965251000000754    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 550   score: 1.0   memory length: 101923   epsilon: 0.9961904800000827    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 551   score: 0.0   memory length: 102045   epsilon: 0.995948920000088    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 552   score: 0.0   memory length: 102167   epsilon: 0.9957073600000932    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 553   score: 1.0   memory length: 102336   epsilon: 0.9953727400001005    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 554   score: 1.0   memory length: 102488   epsilon: 0.995071780000107    steps: 152    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 555   score: 1.0   memory length: 102639   epsilon: 0.9947728000001135    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 556   score: 1.0   memory length: 102790   epsilon: 0.99447382000012    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 557   score: 2.0   memory length: 102992   epsilon: 0.9940738600001287    steps: 202    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 558   score: 0.0   memory length: 103115   epsilon: 0.9938303200001339    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 559   score: 1.0   memory length: 103284   epsilon: 0.9934957000001412    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 560   score: 1.0   memory length: 103453   epsilon: 0.9931610800001485    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 561   score: 1.0   memory length: 103624   epsilon: 0.9928225000001558    steps: 171    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 562   score: 3.0   memory length: 103872   epsilon: 0.9923314600001665    steps: 248    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 563   score: 2.0   memory length: 104089   epsilon: 0.9919018000001758    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 564   score: 0.0   memory length: 104212   epsilon: 0.9916582600001811    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 565   score: 3.0   memory length: 104439   epsilon: 0.9912088000001908    steps: 227    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 566   score: 0.0   memory length: 104562   epsilon: 0.9909652600001961    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 567   score: 2.0   memory length: 104780   epsilon: 0.9905336200002055    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 568   score: 1.0   memory length: 104950   epsilon: 0.9901970200002128    steps: 170    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 569   score: 1.0   memory length: 105100   epsilon: 0.9899000200002193    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 570   score: 1.0   memory length: 105269   epsilon: 0.9895654000002265    steps: 169    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 571   score: 4.0   memory length: 105586   epsilon: 0.9889377400002402    steps: 317    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 572   score: 0.0   memory length: 105709   epsilon: 0.9886942000002454    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 573   score: 4.0   memory length: 105985   epsilon: 0.9881477200002573    steps: 276    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 574   score: 2.0   memory length: 106182   epsilon: 0.9877576600002658    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 575   score: 2.0   memory length: 106398   epsilon: 0.987329980000275    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 576   score: 0.0   memory length: 106521   epsilon: 0.9870864400002803    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 577   score: 1.0   memory length: 106690   epsilon: 0.9867518200002876    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 578   score: 2.0   memory length: 106887   epsilon: 0.9863617600002961    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 579   score: 2.0   memory length: 107066   epsilon: 0.9860073400003038    steps: 179    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 580   score: 3.0   memory length: 107332   epsilon: 0.9854806600003152    steps: 266    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 581   score: 3.0   memory length: 107558   epsilon: 0.9850331800003249    steps: 226    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 582   score: 3.0   memory length: 107807   epsilon: 0.9845401600003356    steps: 249    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 583   score: 1.0   memory length: 107975   epsilon: 0.9842075200003428    steps: 168    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 584   score: 2.0   memory length: 108174   epsilon: 0.9838135000003514    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 585   score: 4.0   memory length: 108470   epsilon: 0.9832274200003641    steps: 296    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 586   score: 2.0   memory length: 108687   epsilon: 0.9827977600003734    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 587   score: 0.0   memory length: 108809   epsilon: 0.9825562000003787    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 588   score: 2.0   memory length: 109007   epsilon: 0.9821641600003872    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 589   score: 2.0   memory length: 109226   epsilon: 0.9817305400003966    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 590   score: 2.0   memory length: 109423   epsilon: 0.9813404800004051    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 591   score: 1.0   memory length: 109592   epsilon: 0.9810058600004123    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 592   score: 5.0   memory length: 109935   epsilon: 0.9803267200004271    steps: 343    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 593   score: 2.0   memory length: 110133   epsilon: 0.9799346800004356    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 594   score: 0.0   memory length: 110256   epsilon: 0.9796911400004409    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 595   score: 1.0   memory length: 110424   epsilon: 0.9793585000004481    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 596   score: 0.0   memory length: 110547   epsilon: 0.9791149600004534    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 597   score: 0.0   memory length: 110670   epsilon: 0.9788714200004587    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 598   score: 1.0   memory length: 110841   epsilon: 0.978532840000466    steps: 171    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 599   score: 1.0   memory length: 111011   epsilon: 0.9781962400004733    steps: 170    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 600   score: 3.0   memory length: 111257   epsilon: 0.9777091600004839    steps: 246    lr: 0.0001     evaluation reward: 1.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 601   score: 2.0   memory length: 111455   epsilon: 0.9773171200004924    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 602   score: 3.0   memory length: 111704   epsilon: 0.9768241000005031    steps: 249    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 603   score: 1.0   memory length: 111874   epsilon: 0.9764875000005104    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 604   score: 1.0   memory length: 112043   epsilon: 0.9761528800005177    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 605   score: 1.0   memory length: 112212   epsilon: 0.975818260000525    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 606   score: 0.0   memory length: 112335   epsilon: 0.9755747200005302    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 607   score: 1.0   memory length: 112506   epsilon: 0.9752361400005376    steps: 171    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 608   score: 2.0   memory length: 112724   epsilon: 0.974804500000547    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 609   score: 0.0   memory length: 112847   epsilon: 0.9745609600005523    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 610   score: 0.0   memory length: 112969   epsilon: 0.9743194000005575    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 611   score: 1.0   memory length: 113137   epsilon: 0.9739867600005647    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 612   score: 2.0   memory length: 113334   epsilon: 0.9735967000005732    steps: 197    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 613   score: 3.0   memory length: 113598   epsilon: 0.9730739800005845    steps: 264    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 614   score: 0.0   memory length: 113721   epsilon: 0.9728304400005898    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 615   score: 3.0   memory length: 113970   epsilon: 0.9723374200006005    steps: 249    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 616   score: 4.0   memory length: 114230   epsilon: 0.9718226200006117    steps: 260    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 617   score: 0.0   memory length: 114353   epsilon: 0.971579080000617    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 618   score: 2.0   memory length: 114570   epsilon: 0.9711494200006263    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 619   score: 3.0   memory length: 114836   epsilon: 0.9706227400006378    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 620   score: 1.0   memory length: 115004   epsilon: 0.970290100000645    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 621   score: 1.0   memory length: 115176   epsilon: 0.9699495400006524    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 622   score: 3.0   memory length: 115422   epsilon: 0.9694624600006629    steps: 246    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 623   score: 1.0   memory length: 115591   epsilon: 0.9691278400006702    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 624   score: 1.0   memory length: 115761   epsilon: 0.9687912400006775    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 625   score: 1.0   memory length: 115932   epsilon: 0.9684526600006849    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 626   score: 2.0   memory length: 116113   epsilon: 0.9680942800006926    steps: 181    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 627   score: 0.0   memory length: 116236   epsilon: 0.9678507400006979    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 628   score: 2.0   memory length: 116434   epsilon: 0.9674587000007064    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 629   score: 0.0   memory length: 116557   epsilon: 0.9672151600007117    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 630   score: 2.0   memory length: 116754   epsilon: 0.9668251000007202    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 631   score: 0.0   memory length: 116876   epsilon: 0.9665835400007254    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 632   score: 5.0   memory length: 117218   epsilon: 0.9659063800007401    steps: 342    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 633   score: 2.0   memory length: 117437   epsilon: 0.9654727600007496    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 634   score: 2.0   memory length: 117616   epsilon: 0.9651183400007572    steps: 179    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 635   score: 0.0   memory length: 117739   epsilon: 0.9648748000007625    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 636   score: 2.0   memory length: 117958   epsilon: 0.964441180000772    steps: 219    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 637   score: 2.0   memory length: 118176   epsilon: 0.9640095400007813    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 638   score: 1.0   memory length: 118346   epsilon: 0.9636729400007886    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 639   score: 0.0   memory length: 118468   epsilon: 0.9634313800007939    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 640   score: 2.0   memory length: 118666   epsilon: 0.9630393400008024    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 641   score: 2.0   memory length: 118866   epsilon: 0.962643340000811    steps: 200    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 642   score: 2.0   memory length: 119064   epsilon: 0.9622513000008195    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 643   score: 0.0   memory length: 119187   epsilon: 0.9620077600008248    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 644   score: 3.0   memory length: 119434   epsilon: 0.9615187000008354    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 645   score: 5.0   memory length: 119759   epsilon: 0.9608752000008494    steps: 325    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 646   score: 0.0   memory length: 119882   epsilon: 0.9606316600008546    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 647   score: 2.0   memory length: 120100   epsilon: 0.960200020000864    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 648   score: 0.0   memory length: 120223   epsilon: 0.9599564800008693    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 649   score: 1.0   memory length: 120392   epsilon: 0.9596218600008766    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 650   score: 6.0   memory length: 120783   epsilon: 0.9588476800008934    steps: 391    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 651   score: 1.0   memory length: 120954   epsilon: 0.9585091000009007    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 652   score: 2.0   memory length: 121152   epsilon: 0.9581170600009092    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 653   score: 2.0   memory length: 121368   epsilon: 0.9576893800009185    steps: 216    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 654   score: 5.0   memory length: 121718   epsilon: 0.9569963800009336    steps: 350    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 655   score: 3.0   memory length: 121966   epsilon: 0.9565053400009442    steps: 248    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 656   score: 2.0   memory length: 122184   epsilon: 0.9560737000009536    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 657   score: 0.0   memory length: 122307   epsilon: 0.9558301600009589    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 658   score: 2.0   memory length: 122488   epsilon: 0.9554717800009667    steps: 181    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 659   score: 2.0   memory length: 122685   epsilon: 0.9550817200009751    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 660   score: 0.0   memory length: 122808   epsilon: 0.9548381800009804    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 661   score: 1.0   memory length: 122959   epsilon: 0.9545392000009869    steps: 151    lr: 0.0001     evaluation reward: 1.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 662   score: 2.0   memory length: 123178   epsilon: 0.9541055800009963    steps: 219    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 663   score: 0.0   memory length: 123300   epsilon: 0.9538640200010016    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 664   score: 0.0   memory length: 123422   epsilon: 0.9536224600010068    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 665   score: 1.0   memory length: 123593   epsilon: 0.9532838800010142    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 666   score: 2.0   memory length: 123791   epsilon: 0.9528918400010227    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 667   score: 3.0   memory length: 124054   epsilon: 0.952371100001034    steps: 263    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 668   score: 1.0   memory length: 124205   epsilon: 0.9520721200010405    steps: 151    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 669   score: 1.0   memory length: 124377   epsilon: 0.9517315600010479    steps: 172    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 670   score: 1.0   memory length: 124545   epsilon: 0.9513989200010551    steps: 168    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 671   score: 2.0   memory length: 124745   epsilon: 0.9510029200010637    steps: 200    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 672   score: 3.0   memory length: 124992   epsilon: 0.9505138600010743    steps: 247    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 673   score: 2.0   memory length: 125209   epsilon: 0.9500842000010836    steps: 217    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 674   score: 0.0   memory length: 125332   epsilon: 0.9498406600010889    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 675   score: 1.0   memory length: 125483   epsilon: 0.9495416800010954    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 676   score: 2.0   memory length: 125701   epsilon: 0.9491100400011048    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 677   score: 1.0   memory length: 125852   epsilon: 0.9488110600011113    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 678   score: 0.0   memory length: 125975   epsilon: 0.9485675200011165    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 679   score: 1.0   memory length: 126126   epsilon: 0.948268540001123    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 680   score: 3.0   memory length: 126392   epsilon: 0.9477418600011345    steps: 266    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 681   score: 1.0   memory length: 126560   epsilon: 0.9474092200011417    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 682   score: 0.0   memory length: 126683   epsilon: 0.947165680001147    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 683   score: 2.0   memory length: 126883   epsilon: 0.9467696800011556    steps: 200    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 684   score: 1.0   memory length: 127034   epsilon: 0.9464707000011621    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 685   score: 1.0   memory length: 127203   epsilon: 0.9461360800011693    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 686   score: 4.0   memory length: 127479   epsilon: 0.9455896000011812    steps: 276    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 687   score: 0.0   memory length: 127601   epsilon: 0.9453480400011864    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 688   score: 1.0   memory length: 127752   epsilon: 0.9450490600011929    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 689   score: 2.0   memory length: 127949   epsilon: 0.9446590000012014    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 690   score: 2.0   memory length: 128147   epsilon: 0.9442669600012099    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 691   score: 2.0   memory length: 128344   epsilon: 0.9438769000012184    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 692   score: 2.0   memory length: 128545   epsilon: 0.943478920001227    steps: 201    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 693   score: 0.0   memory length: 128667   epsilon: 0.9432373600012323    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 694   score: 1.0   memory length: 128818   epsilon: 0.9429383800012388    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 695   score: 6.0   memory length: 129220   epsilon: 0.942142420001256    steps: 402    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 696   score: 1.0   memory length: 129371   epsilon: 0.9418434400012625    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 697   score: 1.0   memory length: 129543   epsilon: 0.9415028800012699    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 698   score: 0.0   memory length: 129666   epsilon: 0.9412593400012752    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 699   score: 1.0   memory length: 129816   epsilon: 0.9409623400012816    steps: 150    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 700   score: 2.0   memory length: 130034   epsilon: 0.940530700001291    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 701   score: 1.0   memory length: 130203   epsilon: 0.9401960800012983    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 702   score: 3.0   memory length: 130453   epsilon: 0.939701080001309    steps: 250    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 703   score: 0.0   memory length: 130576   epsilon: 0.9394575400013143    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 704   score: 2.0   memory length: 130777   epsilon: 0.939059560001323    steps: 201    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 705   score: 0.0   memory length: 130899   epsilon: 0.9388180000013282    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 706   score: 0.0   memory length: 131021   epsilon: 0.9385764400013334    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 707   score: 1.0   memory length: 131190   epsilon: 0.9382418200013407    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 708   score: 0.0   memory length: 131312   epsilon: 0.938000260001346    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 709   score: 1.0   memory length: 131482   epsilon: 0.9376636600013533    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 710   score: 2.0   memory length: 131683   epsilon: 0.9372656800013619    steps: 201    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 711   score: 0.0   memory length: 131806   epsilon: 0.9370221400013672    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 712   score: 1.0   memory length: 131974   epsilon: 0.9366895000013744    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 713   score: 2.0   memory length: 132171   epsilon: 0.9362994400013829    steps: 197    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 714   score: 6.0   memory length: 132527   epsilon: 0.9355945600013982    steps: 356    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 715   score: 1.0   memory length: 132678   epsilon: 0.9352955800014047    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 716   score: 0.0   memory length: 132801   epsilon: 0.93505204000141    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 717   score: 2.0   memory length: 133017   epsilon: 0.9346243600014192    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 718   score: 1.0   memory length: 133189   epsilon: 0.9342838000014266    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 719   score: 4.0   memory length: 133484   epsilon: 0.9336997000014393    steps: 295    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 720   score: 3.0   memory length: 133729   epsilon: 0.9332146000014498    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 721   score: 2.0   memory length: 133927   epsilon: 0.9328225600014584    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 722   score: 4.0   memory length: 134200   epsilon: 0.9322820200014701    steps: 273    lr: 0.0001     evaluation reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 723   score: 2.0   memory length: 134398   epsilon: 0.9318899800014786    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 724   score: 1.0   memory length: 134568   epsilon: 0.9315533800014859    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 725   score: 1.0   memory length: 134736   epsilon: 0.9312207400014931    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 726   score: 0.0   memory length: 134859   epsilon: 0.9309772000014984    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 727   score: 0.0   memory length: 134982   epsilon: 0.9307336600015037    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 728   score: 0.0   memory length: 135105   epsilon: 0.930490120001509    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 729   score: 1.0   memory length: 135255   epsilon: 0.9301931200015154    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 730   score: 2.0   memory length: 135437   epsilon: 0.9298327600015233    steps: 182    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 731   score: 2.0   memory length: 135655   epsilon: 0.9294011200015326    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 732   score: 1.0   memory length: 135824   epsilon: 0.9290665000015399    steps: 169    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 733   score: 0.0   memory length: 135947   epsilon: 0.9288229600015452    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 734   score: 2.0   memory length: 136165   epsilon: 0.9283913200015546    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 735   score: 1.0   memory length: 136316   epsilon: 0.928092340001561    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 736   score: 2.0   memory length: 136514   epsilon: 0.9277003000015696    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 737   score: 3.0   memory length: 136760   epsilon: 0.9272132200015801    steps: 246    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 738   score: 3.0   memory length: 137024   epsilon: 0.9266905000015915    steps: 264    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 739   score: 2.0   memory length: 137243   epsilon: 0.9262568800016009    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 740   score: 2.0   memory length: 137461   epsilon: 0.9258252400016103    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 741   score: 0.0   memory length: 137584   epsilon: 0.9255817000016155    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 742   score: 3.0   memory length: 137810   epsilon: 0.9251342200016253    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 743   score: 2.0   memory length: 138008   epsilon: 0.9247421800016338    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 744   score: 0.0   memory length: 138131   epsilon: 0.9244986400016391    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 745   score: 0.0   memory length: 138253   epsilon: 0.9242570800016443    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 746   score: 0.0   memory length: 138376   epsilon: 0.9240135400016496    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 747   score: 2.0   memory length: 138574   epsilon: 0.9236215000016581    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 748   score: 1.0   memory length: 138725   epsilon: 0.9233225200016646    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 749   score: 1.0   memory length: 138875   epsilon: 0.923025520001671    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 750   score: 0.0   memory length: 138998   epsilon: 0.9227819800016763    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 751   score: 2.0   memory length: 139180   epsilon: 0.9224216200016842    steps: 182    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 752   score: 2.0   memory length: 139398   epsilon: 0.9219899800016935    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 753   score: 6.0   memory length: 139715   epsilon: 0.9213623200017071    steps: 317    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 754   score: 3.0   memory length: 139960   epsilon: 0.9208772200017177    steps: 245    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 755   score: 1.0   memory length: 140128   epsilon: 0.9205445800017249    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 756   score: 0.0   memory length: 140251   epsilon: 0.9203010400017302    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 757   score: 2.0   memory length: 140449   epsilon: 0.9199090000017387    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 758   score: 1.0   memory length: 140619   epsilon: 0.919572400001746    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 759   score: 1.0   memory length: 140769   epsilon: 0.9192754000017525    steps: 150    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 760   score: 0.0   memory length: 140891   epsilon: 0.9190338400017577    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 761   score: 5.0   memory length: 141237   epsilon: 0.9183487600017726    steps: 346    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 762   score: 2.0   memory length: 141435   epsilon: 0.9179567200017811    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 763   score: 1.0   memory length: 141604   epsilon: 0.9176221000017883    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 764   score: 0.0   memory length: 141726   epsilon: 0.9173805400017936    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 765   score: 2.0   memory length: 141924   epsilon: 0.9169885000018021    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 766   score: 0.0   memory length: 142047   epsilon: 0.9167449600018074    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 767   score: 1.0   memory length: 142217   epsilon: 0.9164083600018147    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 768   score: 2.0   memory length: 142433   epsilon: 0.915980680001824    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 769   score: 0.0   memory length: 142555   epsilon: 0.9157391200018292    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 770   score: 4.0   memory length: 142831   epsilon: 0.9151926400018411    steps: 276    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 771   score: 0.0   memory length: 142954   epsilon: 0.9149491000018464    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 772   score: 2.0   memory length: 143134   epsilon: 0.9145927000018541    steps: 180    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 773   score: 1.0   memory length: 143303   epsilon: 0.9142580800018614    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 774   score: 0.0   memory length: 143426   epsilon: 0.9140145400018667    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 775   score: 3.0   memory length: 143693   epsilon: 0.9134858800018781    steps: 267    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 776   score: 1.0   memory length: 143862   epsilon: 0.9131512600018854    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 777   score: 0.0   memory length: 143985   epsilon: 0.9129077200018907    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 778   score: 1.0   memory length: 144136   epsilon: 0.9126087400018972    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 779   score: 2.0   memory length: 144334   epsilon: 0.9122167000019057    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 780   score: 0.0   memory length: 144457   epsilon: 0.911973160001911    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 781   score: 1.0   memory length: 144626   epsilon: 0.9116385400019182    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 782   score: 1.0   memory length: 144777   epsilon: 0.9113395600019247    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 783   score: 2.0   memory length: 144975   epsilon: 0.9109475200019332    steps: 198    lr: 0.0001     evaluation reward: 1.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 784   score: 2.0   memory length: 145173   epsilon: 0.9105554800019418    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 785   score: 2.0   memory length: 145391   epsilon: 0.9101238400019511    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 786   score: 2.0   memory length: 145609   epsilon: 0.9096922000019605    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 787   score: 0.0   memory length: 145731   epsilon: 0.9094506400019657    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 788   score: 0.0   memory length: 145854   epsilon: 0.909207100001971    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 789   score: 0.0   memory length: 145977   epsilon: 0.9089635600019763    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 790   score: 0.0   memory length: 146099   epsilon: 0.9087220000019816    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 791   score: 1.0   memory length: 146268   epsilon: 0.9083873800019888    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 792   score: 2.0   memory length: 146465   epsilon: 0.9079973200019973    steps: 197    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 793   score: 2.0   memory length: 146662   epsilon: 0.9076072600020058    steps: 197    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 794   score: 2.0   memory length: 146879   epsilon: 0.9071776000020151    steps: 217    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 795   score: 0.0   memory length: 147001   epsilon: 0.9069360400020203    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 796   score: 1.0   memory length: 147152   epsilon: 0.9066370600020268    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 797   score: 2.0   memory length: 147370   epsilon: 0.9062054200020362    steps: 218    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 798   score: 0.0   memory length: 147493   epsilon: 0.9059618800020415    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 799   score: 2.0   memory length: 147712   epsilon: 0.9055282600020509    steps: 219    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 800   score: 1.0   memory length: 147882   epsilon: 0.9051916600020582    steps: 170    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 801   score: 2.0   memory length: 148080   epsilon: 0.9047996200020667    steps: 198    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 802   score: 0.0   memory length: 148203   epsilon: 0.904556080002072    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 803   score: 1.0   memory length: 148354   epsilon: 0.9042571000020785    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 804   score: 1.0   memory length: 148505   epsilon: 0.903958120002085    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 805   score: 3.0   memory length: 148751   epsilon: 0.9034710400020955    steps: 246    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 806   score: 2.0   memory length: 148948   epsilon: 0.903080980002104    steps: 197    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 807   score: 0.0   memory length: 149071   epsilon: 0.9028374400021093    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 808   score: 2.0   memory length: 149269   epsilon: 0.9024454000021178    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 809   score: 1.0   memory length: 149420   epsilon: 0.9021464200021243    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 810   score: 0.0   memory length: 149543   epsilon: 0.9019028800021296    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 811   score: 0.0   memory length: 149665   epsilon: 0.9016613200021348    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 812   score: 0.0   memory length: 149788   epsilon: 0.9014177800021401    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 813   score: 2.0   memory length: 150007   epsilon: 0.9009841600021495    steps: 219    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 814   score: 1.0   memory length: 150177   epsilon: 0.9006475600021568    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 815   score: 3.0   memory length: 150421   epsilon: 0.9001644400021673    steps: 244    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 816   score: 2.0   memory length: 150641   epsilon: 0.8997288400021768    steps: 220    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 817   score: 2.0   memory length: 150857   epsilon: 0.8993011600021861    steps: 216    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 818   score: 0.0   memory length: 150979   epsilon: 0.8990596000021913    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 819   score: 0.0   memory length: 151101   epsilon: 0.8988180400021966    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 820   score: 4.0   memory length: 151417   epsilon: 0.8981923600022101    steps: 316    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 821   score: 0.0   memory length: 151540   epsilon: 0.8979488200022154    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 822   score: 0.0   memory length: 151663   epsilon: 0.8977052800022207    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 823   score: 0.0   memory length: 151786   epsilon: 0.897461740002226    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 824   score: 0.0   memory length: 151909   epsilon: 0.8972182000022313    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 825   score: 0.0   memory length: 152032   epsilon: 0.8969746600022366    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 826   score: 5.0   memory length: 152356   epsilon: 0.8963331400022505    steps: 324    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 827   score: 0.0   memory length: 152479   epsilon: 0.8960896000022558    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 828   score: 2.0   memory length: 152676   epsilon: 0.8956995400022643    steps: 197    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 829   score: 5.0   memory length: 153018   epsilon: 0.895022380002279    steps: 342    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 830   score: 2.0   memory length: 153236   epsilon: 0.8945907400022883    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 831   score: 3.0   memory length: 153462   epsilon: 0.894143260002298    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 832   score: 3.0   memory length: 153687   epsilon: 0.8936977600023077    steps: 225    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 833   score: 1.0   memory length: 153838   epsilon: 0.8933987800023142    steps: 151    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 834   score: 1.0   memory length: 154007   epsilon: 0.8930641600023215    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 835   score: 0.0   memory length: 154129   epsilon: 0.8928226000023267    steps: 122    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 836   score: 2.0   memory length: 154315   epsilon: 0.8924543200023347    steps: 186    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 837   score: 5.0   memory length: 154622   epsilon: 0.8918464600023479    steps: 307    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 838   score: 1.0   memory length: 154791   epsilon: 0.8915118400023552    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 839   score: 1.0   memory length: 154942   epsilon: 0.8912128600023617    steps: 151    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 840   score: 0.0   memory length: 155064   epsilon: 0.8909713000023669    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 841   score: 4.0   memory length: 155357   epsilon: 0.8903911600023795    steps: 293    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 842   score: 4.0   memory length: 155653   epsilon: 0.8898050800023922    steps: 296    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 843   score: 0.0   memory length: 155776   epsilon: 0.8895615400023975    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 844   score: 3.0   memory length: 156022   epsilon: 0.8890744600024081    steps: 246    lr: 0.0001     evaluation reward: 1.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 845   score: 0.0   memory length: 156144   epsilon: 0.8888329000024133    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 846   score: 7.0   memory length: 156463   epsilon: 0.888201280002427    steps: 319    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 847   score: 0.0   memory length: 156586   epsilon: 0.8879577400024323    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 848   score: 4.0   memory length: 156863   epsilon: 0.8874092800024442    steps: 277    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 849   score: 1.0   memory length: 157033   epsilon: 0.8870726800024515    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 850   score: 1.0   memory length: 157184   epsilon: 0.886773700002458    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 851   score: 2.0   memory length: 157399   epsilon: 0.8863480000024673    steps: 215    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 852   score: 2.0   memory length: 157583   epsilon: 0.8859836800024752    steps: 184    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 853   score: 2.0   memory length: 157801   epsilon: 0.8855520400024846    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 854   score: 0.0   memory length: 157924   epsilon: 0.8853085000024898    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 855   score: 3.0   memory length: 158167   epsilon: 0.8848273600025003    steps: 243    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 856   score: 2.0   memory length: 158385   epsilon: 0.8843957200025097    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 857   score: 4.0   memory length: 158680   epsilon: 0.8838116200025223    steps: 295    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 858   score: 0.0   memory length: 158803   epsilon: 0.8835680800025276    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 859   score: 3.0   memory length: 159049   epsilon: 0.8830810000025382    steps: 246    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 860   score: 1.0   memory length: 159218   epsilon: 0.8827463800025455    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 861   score: 2.0   memory length: 159415   epsilon: 0.8823563200025539    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 862   score: 0.0   memory length: 159537   epsilon: 0.8821147600025592    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 863   score: 1.0   memory length: 159709   epsilon: 0.8817742000025666    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 864   score: 2.0   memory length: 159909   epsilon: 0.8813782000025752    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 865   score: 0.0   memory length: 160032   epsilon: 0.8811346600025804    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 866   score: 1.0   memory length: 160200   epsilon: 0.8808020200025877    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 867   score: 2.0   memory length: 160418   epsilon: 0.880370380002597    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 868   score: 3.0   memory length: 160665   epsilon: 0.8798813200026077    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 869   score: 0.0   memory length: 160788   epsilon: 0.879637780002613    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 870   score: 0.0   memory length: 160911   epsilon: 0.8793942400026182    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 871   score: 2.0   memory length: 161109   epsilon: 0.8790022000026267    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 872   score: 1.0   memory length: 161280   epsilon: 0.8786636200026341    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 873   score: 1.0   memory length: 161451   epsilon: 0.8783250400026414    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 874   score: 3.0   memory length: 161677   epsilon: 0.8778775600026512    steps: 226    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 875   score: 0.0   memory length: 161800   epsilon: 0.8776340200026564    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 876   score: 1.0   memory length: 161951   epsilon: 0.8773350400026629    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 877   score: 0.0   memory length: 162074   epsilon: 0.8770915000026682    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 878   score: 1.0   memory length: 162243   epsilon: 0.8767568800026755    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 879   score: 1.0   memory length: 162394   epsilon: 0.876457900002682    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 880   score: 2.0   memory length: 162591   epsilon: 0.8760678400026904    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 881   score: 3.0   memory length: 162816   epsilon: 0.8756223400027001    steps: 225    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 882   score: 5.0   memory length: 163144   epsilon: 0.8749729000027142    steps: 328    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 883   score: 2.0   memory length: 163363   epsilon: 0.8745392800027236    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 884   score: 0.0   memory length: 163486   epsilon: 0.8742957400027289    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 885   score: 0.0   memory length: 163609   epsilon: 0.8740522000027342    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 886   score: 0.0   memory length: 163731   epsilon: 0.8738106400027394    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 887   score: 1.0   memory length: 163900   epsilon: 0.8734760200027467    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 888   score: 1.0   memory length: 164069   epsilon: 0.873141400002754    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 889   score: 2.0   memory length: 164267   epsilon: 0.8727493600027625    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 890   score: 3.0   memory length: 164512   epsilon: 0.872264260002773    steps: 245    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 891   score: 2.0   memory length: 164730   epsilon: 0.8718326200027824    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 892   score: 2.0   memory length: 164929   epsilon: 0.8714386000027909    steps: 199    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 893   score: 2.0   memory length: 165127   epsilon: 0.8710465600027995    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 894   score: 1.0   memory length: 165299   epsilon: 0.8707060000028068    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 895   score: 3.0   memory length: 165543   epsilon: 0.8702228800028173    steps: 244    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 896   score: 2.0   memory length: 165741   epsilon: 0.8698308400028258    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 897   score: 3.0   memory length: 165967   epsilon: 0.8693833600028356    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 898   score: 3.0   memory length: 166213   epsilon: 0.8688962800028461    steps: 246    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 899   score: 1.0   memory length: 166382   epsilon: 0.8685616600028534    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 900   score: 1.0   memory length: 166553   epsilon: 0.8682230800028607    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 901   score: 1.0   memory length: 166704   epsilon: 0.8679241000028672    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 902   score: 2.0   memory length: 166908   epsilon: 0.867520180002876    steps: 204    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 903   score: 4.0   memory length: 167167   epsilon: 0.8670073600028871    steps: 259    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 904   score: 2.0   memory length: 167365   epsilon: 0.8666153200028957    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 905   score: 3.0   memory length: 167629   epsilon: 0.866092600002907    steps: 264    lr: 0.0001     evaluation reward: 1.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 906   score: 0.0   memory length: 167751   epsilon: 0.8658510400029122    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 907   score: 2.0   memory length: 167948   epsilon: 0.8654609800029207    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 908   score: 1.0   memory length: 168100   epsilon: 0.8651600200029272    steps: 152    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 909   score: 2.0   memory length: 168298   epsilon: 0.8647679800029358    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 910   score: 2.0   memory length: 168495   epsilon: 0.8643779200029442    steps: 197    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 911   score: 1.0   memory length: 168647   epsilon: 0.8640769600029508    steps: 152    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 912   score: 1.0   memory length: 168815   epsilon: 0.863744320002958    steps: 168    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 913   score: 5.0   memory length: 169112   epsilon: 0.8631562600029707    steps: 297    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 914   score: 1.0   memory length: 169282   epsilon: 0.862819660002978    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 915   score: 1.0   memory length: 169452   epsilon: 0.8624830600029854    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 916   score: 0.0   memory length: 169575   epsilon: 0.8622395200029906    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 917   score: 0.0   memory length: 169698   epsilon: 0.8619959800029959    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 918   score: 4.0   memory length: 169995   epsilon: 0.8614079200030087    steps: 297    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 919   score: 0.0   memory length: 170118   epsilon: 0.861164380003014    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 920   score: 0.0   memory length: 170241   epsilon: 0.8609208400030193    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 921   score: 0.0   memory length: 170364   epsilon: 0.8606773000030246    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 922   score: 1.0   memory length: 170533   epsilon: 0.8603426800030318    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 923   score: 1.0   memory length: 170684   epsilon: 0.8600437000030383    steps: 151    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 924   score: 2.0   memory length: 170881   epsilon: 0.8596536400030468    steps: 197    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 925   score: 0.0   memory length: 171004   epsilon: 0.8594101000030521    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 926   score: 3.0   memory length: 171270   epsilon: 0.8588834200030635    steps: 266    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 927   score: 2.0   memory length: 171486   epsilon: 0.8584557400030728    steps: 216    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 928   score: 2.0   memory length: 171704   epsilon: 0.8580241000030822    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 929   score: 1.0   memory length: 171874   epsilon: 0.8576875000030895    steps: 170    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 930   score: 0.0   memory length: 171996   epsilon: 0.8574459400030947    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 931   score: 1.0   memory length: 172165   epsilon: 0.857111320003102    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 932   score: 0.0   memory length: 172288   epsilon: 0.8568677800031073    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 933   score: 2.0   memory length: 172505   epsilon: 0.8564381200031166    steps: 217    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 934   score: 0.0   memory length: 172627   epsilon: 0.8561965600031218    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 935   score: 3.0   memory length: 172871   epsilon: 0.8557134400031323    steps: 244    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 936   score: 0.0   memory length: 172993   epsilon: 0.8554718800031376    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 937   score: 2.0   memory length: 173172   epsilon: 0.8551174600031453    steps: 179    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 938   score: 3.0   memory length: 173399   epsilon: 0.854668000003155    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 939   score: 3.0   memory length: 173644   epsilon: 0.8541829000031655    steps: 245    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 940   score: 0.0   memory length: 173766   epsilon: 0.8539413400031708    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 941   score: 0.0   memory length: 173888   epsilon: 0.853699780003176    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 942   score: 0.0   memory length: 174011   epsilon: 0.8534562400031813    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 943   score: 0.0   memory length: 174134   epsilon: 0.8532127000031866    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 944   score: 0.0   memory length: 174257   epsilon: 0.8529691600031919    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 945   score: 4.0   memory length: 174553   epsilon: 0.8523830800032046    steps: 296    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 946   score: 0.0   memory length: 174676   epsilon: 0.8521395400032099    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 947   score: 5.0   memory length: 175000   epsilon: 0.8514980200032238    steps: 324    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 948   score: 2.0   memory length: 175197   epsilon: 0.8511079600032323    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 949   score: 0.0   memory length: 175320   epsilon: 0.8508644200032376    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 950   score: 2.0   memory length: 175517   epsilon: 0.850474360003246    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 951   score: 0.0   memory length: 175640   epsilon: 0.8502308200032513    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 952   score: 3.0   memory length: 175908   epsilon: 0.8497001800032629    steps: 268    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 953   score: 2.0   memory length: 176123   epsilon: 0.8492744800032721    steps: 215    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 954   score: 2.0   memory length: 176341   epsilon: 0.8488428400032815    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 955   score: 0.0   memory length: 176464   epsilon: 0.8485993000032868    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 956   score: 4.0   memory length: 176738   epsilon: 0.8480567800032985    steps: 274    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 957   score: 2.0   memory length: 176954   epsilon: 0.8476291000033078    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 958   score: 0.0   memory length: 177077   epsilon: 0.8473855600033131    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 959   score: 1.0   memory length: 177247   epsilon: 0.8470489600033204    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 960   score: 0.0   memory length: 177370   epsilon: 0.8468054200033257    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 961   score: 2.0   memory length: 177593   epsilon: 0.8463638800033353    steps: 223    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 962   score: 4.0   memory length: 177891   epsilon: 0.8457738400033481    steps: 298    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 963   score: 1.0   memory length: 178061   epsilon: 0.8454372400033554    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 964   score: 3.0   memory length: 178326   epsilon: 0.8449125400033668    steps: 265    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 965   score: 2.0   memory length: 178526   epsilon: 0.8445165400033754    steps: 200    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 966   score: 2.0   memory length: 178726   epsilon: 0.844120540003384    steps: 200    lr: 0.0001     evaluation reward: 1.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 967   score: 3.0   memory length: 178954   epsilon: 0.8436691000033938    steps: 228    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 968   score: 3.0   memory length: 179224   epsilon: 0.8431345000034054    steps: 270    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 969   score: 3.0   memory length: 179494   epsilon: 0.842599900003417    steps: 270    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 970   score: 0.0   memory length: 179617   epsilon: 0.8423563600034223    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 971   score: 2.0   memory length: 179835   epsilon: 0.8419247200034317    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 972   score: 0.0   memory length: 179958   epsilon: 0.841681180003437    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 973   score: 0.0   memory length: 180080   epsilon: 0.8414396200034422    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 974   score: 2.0   memory length: 180277   epsilon: 0.8410495600034507    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 975   score: 0.0   memory length: 180399   epsilon: 0.8408080000034559    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 976   score: 0.0   memory length: 180521   epsilon: 0.8405664400034611    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 977   score: 1.0   memory length: 180693   epsilon: 0.8402258800034685    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 978   score: 2.0   memory length: 180911   epsilon: 0.8397942400034779    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 979   score: 0.0   memory length: 181034   epsilon: 0.8395507000034832    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 980   score: 1.0   memory length: 181184   epsilon: 0.8392537000034896    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 981   score: 2.0   memory length: 181364   epsilon: 0.8388973000034974    steps: 180    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 982   score: 2.0   memory length: 181582   epsilon: 0.8384656600035068    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 983   score: 2.0   memory length: 181780   epsilon: 0.8380736200035153    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 984   score: 0.0   memory length: 181903   epsilon: 0.8378300800035205    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 985   score: 2.0   memory length: 182101   epsilon: 0.8374380400035291    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 986   score: 5.0   memory length: 182437   epsilon: 0.8367727600035435    steps: 336    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 987   score: 3.0   memory length: 182704   epsilon: 0.836244100003555    steps: 267    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 988   score: 6.0   memory length: 183056   epsilon: 0.8355471400035701    steps: 352    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 989   score: 0.0   memory length: 183178   epsilon: 0.8353055800035754    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 990   score: 2.0   memory length: 183376   epsilon: 0.8349135400035839    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 991   score: 1.0   memory length: 183547   epsilon: 0.8345749600035912    steps: 171    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 992   score: 3.0   memory length: 183816   epsilon: 0.8340423400036028    steps: 269    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 993   score: 2.0   memory length: 184017   epsilon: 0.8336443600036114    steps: 201    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 994   score: 3.0   memory length: 184261   epsilon: 0.8331612400036219    steps: 244    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 995   score: 3.0   memory length: 184505   epsilon: 0.8326781200036324    steps: 244    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 996   score: 2.0   memory length: 184703   epsilon: 0.8322860800036409    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 997   score: 1.0   memory length: 184874   epsilon: 0.8319475000036483    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 998   score: 0.0   memory length: 184997   epsilon: 0.8317039600036535    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 999   score: 2.0   memory length: 185217   epsilon: 0.831268360003663    steps: 220    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1000   score: 0.0   memory length: 185340   epsilon: 0.8310248200036683    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1001   score: 2.0   memory length: 185537   epsilon: 0.8306347600036768    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1002   score: 1.0   memory length: 185707   epsilon: 0.8302981600036841    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1003   score: 2.0   memory length: 185925   epsilon: 0.8298665200036934    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1004   score: 0.0   memory length: 186048   epsilon: 0.8296229800036987    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1005   score: 2.0   memory length: 186266   epsilon: 0.8291913400037081    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1006   score: 2.0   memory length: 186483   epsilon: 0.8287616800037174    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1007   score: 2.0   memory length: 186700   epsilon: 0.8283320200037267    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1008   score: 4.0   memory length: 187014   epsilon: 0.8277103000037402    steps: 314    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1009   score: 1.0   memory length: 187164   epsilon: 0.8274133000037467    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1010   score: 0.0   memory length: 187287   epsilon: 0.827169760003752    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1011   score: 0.0   memory length: 187409   epsilon: 0.8269282000037572    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1012   score: 2.0   memory length: 187607   epsilon: 0.8265361600037657    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1013   score: 0.0   memory length: 187729   epsilon: 0.826294600003771    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1014   score: 1.0   memory length: 187898   epsilon: 0.8259599800037782    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1015   score: 0.0   memory length: 188020   epsilon: 0.8257184200037835    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1016   score: 4.0   memory length: 188317   epsilon: 0.8251303600037962    steps: 297    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1017   score: 0.0   memory length: 188440   epsilon: 0.8248868200038015    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1018   score: 4.0   memory length: 188716   epsilon: 0.8243403400038134    steps: 276    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1019   score: 3.0   memory length: 188946   epsilon: 0.8238849400038233    steps: 230    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1020   score: 2.0   memory length: 189145   epsilon: 0.8234909200038318    steps: 199    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1021   score: 1.0   memory length: 189313   epsilon: 0.8231582800038391    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1022   score: 1.0   memory length: 189485   epsilon: 0.8228177200038465    steps: 172    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1023   score: 4.0   memory length: 189780   epsilon: 0.8222336200038591    steps: 295    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1024   score: 2.0   memory length: 189995   epsilon: 0.8218079200038684    steps: 215    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1025   score: 0.0   memory length: 190117   epsilon: 0.8215663600038736    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1026   score: 2.0   memory length: 190338   epsilon: 0.8211287800038831    steps: 221    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1027   score: 4.0   memory length: 190617   epsilon: 0.8205763600038951    steps: 279    lr: 0.0001     evaluation reward: 1.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1028   score: 1.0   memory length: 190785   epsilon: 0.8202437200039023    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1029   score: 2.0   memory length: 190982   epsilon: 0.8198536600039108    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1030   score: 0.0   memory length: 191105   epsilon: 0.8196101200039161    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1031   score: 2.0   memory length: 191302   epsilon: 0.8192200600039246    steps: 197    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1032   score: 1.0   memory length: 191472   epsilon: 0.8188834600039319    steps: 170    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1033   score: 2.0   memory length: 191690   epsilon: 0.8184518200039412    steps: 218    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1034   score: 2.0   memory length: 191908   epsilon: 0.8180201800039506    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1035   score: 0.0   memory length: 192031   epsilon: 0.8177766400039559    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1036   score: 2.0   memory length: 192229   epsilon: 0.8173846000039644    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1037   score: 4.0   memory length: 192524   epsilon: 0.8168005000039771    steps: 295    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1038   score: 0.0   memory length: 192646   epsilon: 0.8165589400039823    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1039   score: 2.0   memory length: 192844   epsilon: 0.8161669000039908    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1040   score: 3.0   memory length: 193111   epsilon: 0.8156382400040023    steps: 267    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1041   score: 1.0   memory length: 193281   epsilon: 0.8153016400040096    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1042   score: 4.0   memory length: 193577   epsilon: 0.8147155600040223    steps: 296    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1043   score: 1.0   memory length: 193747   epsilon: 0.8143789600040297    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 1044   score: 2.0   memory length: 193967   epsilon: 0.8139433600040391    steps: 220    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 1045   score: 3.0   memory length: 194211   epsilon: 0.8134602400040496    steps: 244    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1046   score: 2.0   memory length: 194427   epsilon: 0.8130325600040589    steps: 216    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1047   score: 5.0   memory length: 194733   epsilon: 0.812426680004072    steps: 306    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1048   score: 2.0   memory length: 194951   epsilon: 0.8119950400040814    steps: 218    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1049   score: 3.0   memory length: 195217   epsilon: 0.8114683600040928    steps: 266    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1050   score: 0.0   memory length: 195340   epsilon: 0.8112248200040981    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1051   score: 4.0   memory length: 195628   epsilon: 0.8106545800041105    steps: 288    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1052   score: 0.0   memory length: 195751   epsilon: 0.8104110400041158    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1053   score: 2.0   memory length: 195949   epsilon: 0.8100190000041243    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 1054   score: 0.0   memory length: 196071   epsilon: 0.8097774400041295    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1055   score: 5.0   memory length: 196418   epsilon: 0.8090903800041445    steps: 347    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1056   score: 0.0   memory length: 196541   epsilon: 0.8088468400041497    steps: 123    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1057   score: 4.0   memory length: 196831   epsilon: 0.8082726400041622    steps: 290    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1058   score: 2.0   memory length: 197029   epsilon: 0.8078806000041707    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1059   score: 2.0   memory length: 197209   epsilon: 0.8075242000041785    steps: 180    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1060   score: 0.0   memory length: 197332   epsilon: 0.8072806600041837    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1061   score: 3.0   memory length: 197578   epsilon: 0.8067935800041943    steps: 246    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1062   score: 4.0   memory length: 197856   epsilon: 0.8062431400042063    steps: 278    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1063   score: 1.0   memory length: 198025   epsilon: 0.8059085200042135    steps: 169    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1064   score: 2.0   memory length: 198222   epsilon: 0.805518460004222    steps: 197    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1065   score: 0.0   memory length: 198345   epsilon: 0.8052749200042273    steps: 123    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1066   score: 2.0   memory length: 198542   epsilon: 0.8048848600042358    steps: 197    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1067   score: 2.0   memory length: 198759   epsilon: 0.8044552000042451    steps: 217    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1068   score: 3.0   memory length: 199002   epsilon: 0.8039740600042555    steps: 243    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1069   score: 1.0   memory length: 199154   epsilon: 0.8036731000042621    steps: 152    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 1070   score: 3.0   memory length: 199383   epsilon: 0.8032196800042719    steps: 229    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1071   score: 3.0   memory length: 199609   epsilon: 0.8027722000042816    steps: 226    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1072   score: 0.0   memory length: 199732   epsilon: 0.8025286600042869    steps: 123    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1073   score: 2.0   memory length: 199930   epsilon: 0.8021366200042954    steps: 198    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1074   score: 4.0   memory length: 200202   epsilon: 0.8015980600043071    steps: 272    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1075   score: 0.0   memory length: 200325   epsilon: 0.8013545200043124    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1076   score: 1.0   memory length: 200494   epsilon: 0.8010199000043197    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1077   score: 2.0   memory length: 200694   epsilon: 0.8006239000043283    steps: 200    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1078   score: 0.0   memory length: 200817   epsilon: 0.8003803600043335    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1079   score: 2.0   memory length: 200997   epsilon: 0.8000239600043413    steps: 180    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1080   score: 1.0   memory length: 201148   epsilon: 0.7997249800043478    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1081   score: 3.0   memory length: 201377   epsilon: 0.7992715600043576    steps: 229    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1082   score: 5.0   memory length: 201743   epsilon: 0.7985468800043733    steps: 366    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1083   score: 4.0   memory length: 202041   epsilon: 0.7979568400043862    steps: 298    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1084   score: 3.0   memory length: 202270   epsilon: 0.797503420004396    steps: 229    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 1085   score: 0.0   memory length: 202392   epsilon: 0.7972618600044012    steps: 122    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1086   score: 2.0   memory length: 202590   epsilon: 0.7968698200044098    steps: 198    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1087   score: 3.0   memory length: 202819   epsilon: 0.7964164000044196    steps: 229    lr: 0.0001     evaluation reward: 1.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1088   score: 2.0   memory length: 203017   epsilon: 0.7960243600044281    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1089   score: 1.0   memory length: 203188   epsilon: 0.7956857800044355    steps: 171    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1090   score: 4.0   memory length: 203485   epsilon: 0.7950977200044482    steps: 297    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1091   score: 3.0   memory length: 203731   epsilon: 0.7946106400044588    steps: 246    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1092   score: 0.0   memory length: 203854   epsilon: 0.7943671000044641    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1093   score: 3.0   memory length: 204100   epsilon: 0.7938800200044747    steps: 246    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1094   score: 0.0   memory length: 204223   epsilon: 0.79363648000448    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1095   score: 1.0   memory length: 204392   epsilon: 0.7933018600044872    steps: 169    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1096   score: 1.0   memory length: 204543   epsilon: 0.7930028800044937    steps: 151    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1097   score: 2.0   memory length: 204741   epsilon: 0.7926108400045022    steps: 198    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1098   score: 2.0   memory length: 204938   epsilon: 0.7922207800045107    steps: 197    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1099   score: 1.0   memory length: 205107   epsilon: 0.791886160004518    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1100   score: 0.0   memory length: 205229   epsilon: 0.7916446000045232    steps: 122    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1101   score: 2.0   memory length: 205427   epsilon: 0.7912525600045317    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1102   score: 2.0   memory length: 205625   epsilon: 0.7908605200045402    steps: 198    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1103   score: 2.0   memory length: 205845   epsilon: 0.7904249200045497    steps: 220    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1104   score: 4.0   memory length: 206141   epsilon: 0.7898388400045624    steps: 296    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1105   score: 0.0   memory length: 206264   epsilon: 0.7895953000045677    steps: 123    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1106   score: 1.0   memory length: 206436   epsilon: 0.7892547400045751    steps: 172    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1107   score: 6.0   memory length: 206811   epsilon: 0.7885122400045912    steps: 375    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1108   score: 2.0   memory length: 207008   epsilon: 0.7881221800045997    steps: 197    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1109   score: 2.0   memory length: 207226   epsilon: 0.787690540004609    steps: 218    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1110   score: 2.0   memory length: 207406   epsilon: 0.7873341400046168    steps: 180    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1111   score: 0.0   memory length: 207529   epsilon: 0.787090600004622    steps: 123    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1112   score: 0.0   memory length: 207651   epsilon: 0.7868490400046273    steps: 122    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1113   score: 2.0   memory length: 207867   epsilon: 0.7864213600046366    steps: 216    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1114   score: 2.0   memory length: 208065   epsilon: 0.7860293200046451    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1115   score: 0.0   memory length: 208188   epsilon: 0.7857857800046504    steps: 123    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1116   score: 2.0   memory length: 208386   epsilon: 0.7853937400046589    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1117   score: 3.0   memory length: 208633   epsilon: 0.7849046800046695    steps: 247    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 1118   score: 4.0   memory length: 208890   epsilon: 0.7843958200046806    steps: 257    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 1119   score: 2.0   memory length: 209088   epsilon: 0.7840037800046891    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1120   score: 1.0   memory length: 209240   epsilon: 0.7837028200046956    steps: 152    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1121   score: 1.0   memory length: 209410   epsilon: 0.7833662200047029    steps: 170    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1122   score: 2.0   memory length: 209629   epsilon: 0.7829326000047123    steps: 219    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1123   score: 1.0   memory length: 209798   epsilon: 0.7825979800047196    steps: 169    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1124   score: 2.0   memory length: 210015   epsilon: 0.7821683200047289    steps: 217    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1125   score: 1.0   memory length: 210166   epsilon: 0.7818693400047354    steps: 151    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1126   score: 0.0   memory length: 210288   epsilon: 0.7816277800047406    steps: 122    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1127   score: 1.0   memory length: 210457   epsilon: 0.7812931600047479    steps: 169    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1128   score: 0.0   memory length: 210580   epsilon: 0.7810496200047532    steps: 123    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1129   score: 0.0   memory length: 210703   epsilon: 0.7808060800047585    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1130   score: 0.0   memory length: 210826   epsilon: 0.7805625400047638    steps: 123    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1131   score: 0.0   memory length: 210949   epsilon: 0.7803190000047691    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1132   score: 6.0   memory length: 211288   epsilon: 0.7796477800047836    steps: 339    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1133   score: 1.0   memory length: 211457   epsilon: 0.7793131600047909    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1134   score: 4.0   memory length: 211751   epsilon: 0.7787310400048035    steps: 294    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1135   score: 4.0   memory length: 212068   epsilon: 0.7781033800048172    steps: 317    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1136   score: 1.0   memory length: 212219   epsilon: 0.7778044000048236    steps: 151    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1137   score: 0.0   memory length: 212342   epsilon: 0.7775608600048289    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1138   score: 0.0   memory length: 212465   epsilon: 0.7773173200048342    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1139   score: 0.0   memory length: 212588   epsilon: 0.7770737800048395    steps: 123    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1140   score: 1.0   memory length: 212760   epsilon: 0.7767332200048469    steps: 172    lr: 0.0001     evaluation reward: 1.82\n",
      "episode: 1141   score: 0.0   memory length: 212883   epsilon: 0.7764896800048522    steps: 123    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1142   score: 2.0   memory length: 213101   epsilon: 0.7760580400048616    steps: 218    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 1143   score: 0.0   memory length: 213224   epsilon: 0.7758145000048668    steps: 123    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1144   score: 2.0   memory length: 213442   epsilon: 0.7753828600048762    steps: 218    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1145   score: 0.0   memory length: 213564   epsilon: 0.7751413000048815    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1146   score: 0.0   memory length: 213687   epsilon: 0.7748977600048867    steps: 123    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1147   score: 1.0   memory length: 213856   epsilon: 0.774563140004894    steps: 169    lr: 0.0001     evaluation reward: 1.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1148   score: 0.0   memory length: 213979   epsilon: 0.7743196000048993    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1149   score: 0.0   memory length: 214102   epsilon: 0.7740760600049046    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1150   score: 1.0   memory length: 214253   epsilon: 0.7737770800049111    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1151   score: 3.0   memory length: 214480   epsilon: 0.7733276200049208    steps: 227    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1152   score: 2.0   memory length: 214697   epsilon: 0.7728979600049302    steps: 217    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1153   score: 2.0   memory length: 214915   epsilon: 0.7724663200049395    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1154   score: 2.0   memory length: 215133   epsilon: 0.7720346800049489    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 1155   score: 0.0   memory length: 215256   epsilon: 0.7717911400049542    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1156   score: 1.0   memory length: 215424   epsilon: 0.7714585000049614    steps: 168    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1157   score: 0.0   memory length: 215547   epsilon: 0.7712149600049667    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1158   score: 2.0   memory length: 215745   epsilon: 0.7708229200049752    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1159   score: 2.0   memory length: 215927   epsilon: 0.770462560004983    steps: 182    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1160   score: 3.0   memory length: 216175   epsilon: 0.7699715200049937    steps: 248    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1161   score: 2.0   memory length: 216394   epsilon: 0.7695379000050031    steps: 219    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1162   score: 1.0   memory length: 216563   epsilon: 0.7692032800050104    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1163   score: 2.0   memory length: 216760   epsilon: 0.7688132200050188    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1164   score: 1.0   memory length: 216929   epsilon: 0.7684786000050261    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1165   score: 3.0   memory length: 217196   epsilon: 0.7679499400050376    steps: 267    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1166   score: 4.0   memory length: 217448   epsilon: 0.7674509800050484    steps: 252    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1167   score: 2.0   memory length: 217646   epsilon: 0.7670589400050569    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1168   score: 2.0   memory length: 217846   epsilon: 0.7666629400050655    steps: 200    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1169   score: 0.0   memory length: 217969   epsilon: 0.7664194000050708    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1170   score: 2.0   memory length: 218186   epsilon: 0.7659897400050801    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1171   score: 2.0   memory length: 218404   epsilon: 0.7655581000050895    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1172   score: 2.0   memory length: 218601   epsilon: 0.765168040005098    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1173   score: 4.0   memory length: 218877   epsilon: 0.7646215600051098    steps: 276    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1174   score: 4.0   memory length: 219193   epsilon: 0.7639958800051234    steps: 316    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1175   score: 3.0   memory length: 219419   epsilon: 0.7635484000051331    steps: 226    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1176   score: 3.0   memory length: 219668   epsilon: 0.7630553800051438    steps: 249    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1177   score: 2.0   memory length: 219886   epsilon: 0.7626237400051532    steps: 218    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1178   score: 0.0   memory length: 220009   epsilon: 0.7623802000051585    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1179   score: 2.0   memory length: 220225   epsilon: 0.7619525200051678    steps: 216    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1180   score: 2.0   memory length: 220423   epsilon: 0.7615604800051763    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1181   score: 0.0   memory length: 220546   epsilon: 0.7613169400051816    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1182   score: 6.0   memory length: 220796   epsilon: 0.7608219400051923    steps: 250    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 1183   score: 1.0   memory length: 220967   epsilon: 0.7604833600051997    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1184   score: 2.0   memory length: 221149   epsilon: 0.7601230000052075    steps: 182    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1185   score: 5.0   memory length: 221456   epsilon: 0.7595151400052207    steps: 307    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1186   score: 3.0   memory length: 221704   epsilon: 0.7590241000052314    steps: 248    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1187   score: 3.0   memory length: 221948   epsilon: 0.7585409800052418    steps: 244    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1188   score: 1.0   memory length: 222120   epsilon: 0.7582004200052492    steps: 172    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1189   score: 3.0   memory length: 222367   epsilon: 0.7577113600052598    steps: 247    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1190   score: 0.0   memory length: 222490   epsilon: 0.7574678200052651    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1191   score: 0.0   memory length: 222613   epsilon: 0.7572242800052704    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1192   score: 1.0   memory length: 222763   epsilon: 0.7569272800052769    steps: 150    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1193   score: 0.0   memory length: 222886   epsilon: 0.7566837400052822    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1194   score: 2.0   memory length: 223083   epsilon: 0.7562936800052906    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 1195   score: 0.0   memory length: 223206   epsilon: 0.7560501400052959    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1196   score: 0.0   memory length: 223329   epsilon: 0.7558066000053012    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1197   score: 0.0   memory length: 223451   epsilon: 0.7555650400053064    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1198   score: 2.0   memory length: 223649   epsilon: 0.755173000005315    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1199   score: 1.0   memory length: 223800   epsilon: 0.7548740200053214    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1200   score: 2.0   memory length: 223998   epsilon: 0.75448198000533    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1201   score: 2.0   memory length: 224196   epsilon: 0.7540899400053385    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1202   score: 1.0   memory length: 224365   epsilon: 0.7537553200053457    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1203   score: 2.0   memory length: 224562   epsilon: 0.7533652600053542    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1204   score: 2.0   memory length: 224779   epsilon: 0.7529356000053635    steps: 217    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1205   score: 0.0   memory length: 224902   epsilon: 0.7526920600053688    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1206   score: 1.0   memory length: 225053   epsilon: 0.7523930800053753    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1207   score: 2.0   memory length: 225251   epsilon: 0.7520010400053838    steps: 198    lr: 0.0001     evaluation reward: 1.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1208   score: 2.0   memory length: 225451   epsilon: 0.7516050400053924    steps: 200    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1209   score: 2.0   memory length: 225649   epsilon: 0.7512130000054009    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1210   score: 0.0   memory length: 225771   epsilon: 0.7509714400054062    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1211   score: 0.0   memory length: 225894   epsilon: 0.7507279000054115    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1212   score: 2.0   memory length: 226111   epsilon: 0.7502982400054208    steps: 217    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1213   score: 2.0   memory length: 226309   epsilon: 0.7499062000054293    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1214   score: 1.0   memory length: 226477   epsilon: 0.7495735600054365    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1215   score: 0.0   memory length: 226600   epsilon: 0.7493300200054418    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1216   score: 0.0   memory length: 226723   epsilon: 0.7490864800054471    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1217   score: 2.0   memory length: 226920   epsilon: 0.7486964200054556    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1218   score: 1.0   memory length: 227089   epsilon: 0.7483618000054628    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1219   score: 2.0   memory length: 227287   epsilon: 0.7479697600054713    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1220   score: 9.0   memory length: 227834   epsilon: 0.7468867000054948    steps: 547    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1221   score: 3.0   memory length: 228060   epsilon: 0.7464392200055046    steps: 226    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1222   score: 2.0   memory length: 228258   epsilon: 0.7460471800055131    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1223   score: 0.0   memory length: 228381   epsilon: 0.7458036400055184    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1224   score: 3.0   memory length: 228624   epsilon: 0.7453225000055288    steps: 243    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1225   score: 2.0   memory length: 228842   epsilon: 0.7448908600055382    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1226   score: 3.0   memory length: 229089   epsilon: 0.7444018000055488    steps: 247    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1227   score: 2.0   memory length: 229287   epsilon: 0.7440097600055573    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1228   score: 4.0   memory length: 229580   epsilon: 0.7434296200055699    steps: 293    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1229   score: 2.0   memory length: 229798   epsilon: 0.7429979800055793    steps: 218    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1230   score: 2.0   memory length: 229996   epsilon: 0.7426059400055878    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1231   score: 2.0   memory length: 230215   epsilon: 0.7421723200055972    steps: 219    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 1232   score: 3.0   memory length: 230442   epsilon: 0.7417228600056069    steps: 227    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 1233   score: 2.0   memory length: 230641   epsilon: 0.7413288400056155    steps: 199    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1234   score: 0.0   memory length: 230764   epsilon: 0.7410853000056208    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 1235   score: 3.0   memory length: 231010   epsilon: 0.7405982200056314    steps: 246    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 1236   score: 4.0   memory length: 231267   epsilon: 0.7400893600056424    steps: 257    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 1237   score: 2.0   memory length: 231485   epsilon: 0.7396577200056518    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 1238   score: 4.0   memory length: 231802   epsilon: 0.7390300600056654    steps: 317    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 1239   score: 3.0   memory length: 232031   epsilon: 0.7385766400056752    steps: 229    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 1240   score: 3.0   memory length: 232277   epsilon: 0.7380895600056858    steps: 246    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1241   score: 1.0   memory length: 232446   epsilon: 0.7377549400056931    steps: 169    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1242   score: 1.0   memory length: 232617   epsilon: 0.7374163600057004    steps: 171    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 1243   score: 1.0   memory length: 232789   epsilon: 0.7370758000057078    steps: 172    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1244   score: 2.0   memory length: 232987   epsilon: 0.7366837600057163    steps: 198    lr: 0.0001     evaluation reward: 1.81\n",
      "episode: 1245   score: 2.0   memory length: 233185   epsilon: 0.7362917200057248    steps: 198    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 1246   score: 1.0   memory length: 233336   epsilon: 0.7359927400057313    steps: 151    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 1247   score: 2.0   memory length: 233534   epsilon: 0.7356007000057398    steps: 198    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1248   score: 2.0   memory length: 233714   epsilon: 0.7352443000057476    steps: 180    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1249   score: 3.0   memory length: 233958   epsilon: 0.7347611800057581    steps: 244    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1250   score: 0.0   memory length: 234080   epsilon: 0.7345196200057633    steps: 122    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1251   score: 3.0   memory length: 234306   epsilon: 0.734072140005773    steps: 226    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1252   score: 4.0   memory length: 234581   epsilon: 0.7335276400057849    steps: 275    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1253   score: 2.0   memory length: 234799   epsilon: 0.7330960000057942    steps: 218    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1254   score: 0.0   memory length: 234922   epsilon: 0.7328524600057995    steps: 123    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1255   score: 3.0   memory length: 235149   epsilon: 0.7324030000058093    steps: 227    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1256   score: 2.0   memory length: 235346   epsilon: 0.7320129400058177    steps: 197    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1257   score: 5.0   memory length: 235672   epsilon: 0.7313674600058317    steps: 326    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1258   score: 3.0   memory length: 235898   epsilon: 0.7309199800058415    steps: 226    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1259   score: 2.0   memory length: 236096   epsilon: 0.73052794000585    steps: 198    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1260   score: 2.0   memory length: 236294   epsilon: 0.7301359000058585    steps: 198    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1261   score: 0.0   memory length: 236417   epsilon: 0.7298923600058638    steps: 123    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1262   score: 2.0   memory length: 236635   epsilon: 0.7294607200058731    steps: 218    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1263   score: 2.0   memory length: 236832   epsilon: 0.7290706600058816    steps: 197    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1264   score: 2.0   memory length: 237053   epsilon: 0.7286330800058911    steps: 221    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1265   score: 3.0   memory length: 237279   epsilon: 0.7281856000059008    steps: 226    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1266   score: 3.0   memory length: 237525   epsilon: 0.7276985200059114    steps: 246    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1267   score: 2.0   memory length: 237723   epsilon: 0.7273064800059199    steps: 198    lr: 0.0001     evaluation reward: 1.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1268   score: 2.0   memory length: 237921   epsilon: 0.7269144400059284    steps: 198    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1269   score: 2.0   memory length: 238141   epsilon: 0.7264788400059379    steps: 220    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1270   score: 0.0   memory length: 238264   epsilon: 0.7262353000059432    steps: 123    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1271   score: 1.0   memory length: 238433   epsilon: 0.7259006800059504    steps: 169    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1272   score: 3.0   memory length: 238658   epsilon: 0.7254551800059601    steps: 225    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1273   score: 3.0   memory length: 238925   epsilon: 0.7249265200059716    steps: 267    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1274   score: 0.0   memory length: 239048   epsilon: 0.7246829800059769    steps: 123    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1275   score: 3.0   memory length: 239295   epsilon: 0.7241939200059875    steps: 247    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1276   score: 2.0   memory length: 239493   epsilon: 0.723801880005996    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1277   score: 0.0   memory length: 239615   epsilon: 0.7235603200060012    steps: 122    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1278   score: 2.0   memory length: 239813   epsilon: 0.7231682800060097    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1279   score: 2.0   memory length: 240011   epsilon: 0.7227762400060183    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1280   score: 2.0   memory length: 240209   epsilon: 0.7223842000060268    steps: 198    lr: 0.0001     evaluation reward: 1.91\n",
      "episode: 1281   score: 2.0   memory length: 240407   epsilon: 0.7219921600060353    steps: 198    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1282   score: 3.0   memory length: 240652   epsilon: 0.7215070600060458    steps: 245    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1283   score: 0.0   memory length: 240774   epsilon: 0.721265500006051    steps: 122    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1284   score: 3.0   memory length: 241021   epsilon: 0.7207764400060617    steps: 247    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1285   score: 4.0   memory length: 241294   epsilon: 0.7202359000060734    steps: 273    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1286   score: 2.0   memory length: 241492   epsilon: 0.7198438600060819    steps: 198    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1287   score: 2.0   memory length: 241690   epsilon: 0.7194518200060904    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1288   score: 0.0   memory length: 241813   epsilon: 0.7192082800060957    steps: 123    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 1289   score: 2.0   memory length: 242010   epsilon: 0.7188182200061042    steps: 197    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 1290   score: 2.0   memory length: 242208   epsilon: 0.7184261800061127    steps: 198    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1291   score: 2.0   memory length: 242427   epsilon: 0.7179925600061221    steps: 219    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1292   score: 2.0   memory length: 242645   epsilon: 0.7175609200061315    steps: 218    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1293   score: 3.0   memory length: 242895   epsilon: 0.7170659200061422    steps: 250    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 1294   score: 5.0   memory length: 243221   epsilon: 0.7164204400061562    steps: 326    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1295   score: 4.0   memory length: 243517   epsilon: 0.715834360006169    steps: 296    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1296   score: 2.0   memory length: 243719   epsilon: 0.7154344000061776    steps: 202    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1297   score: 2.0   memory length: 243916   epsilon: 0.7150443400061861    steps: 197    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1298   score: 2.0   memory length: 244113   epsilon: 0.7146542800061946    steps: 197    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1299   score: 0.0   memory length: 244236   epsilon: 0.7144107400061999    steps: 123    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1300   score: 3.0   memory length: 244464   epsilon: 0.7139593000062097    steps: 228    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1301   score: 2.0   memory length: 244662   epsilon: 0.7135672600062182    steps: 198    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1302   score: 0.0   memory length: 244784   epsilon: 0.7133257000062234    steps: 122    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1303   score: 3.0   memory length: 245031   epsilon: 0.712836640006234    steps: 247    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1304   score: 3.0   memory length: 245256   epsilon: 0.7123911400062437    steps: 225    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1305   score: 2.0   memory length: 245453   epsilon: 0.7120010800062522    steps: 197    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1306   score: 4.0   memory length: 245746   epsilon: 0.7114209400062648    steps: 293    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1307   score: 2.0   memory length: 245947   epsilon: 0.7110229600062734    steps: 201    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1308   score: 5.0   memory length: 246291   epsilon: 0.7103418400062882    steps: 344    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1309   score: 2.0   memory length: 246488   epsilon: 0.7099517800062967    steps: 197    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1310   score: 2.0   memory length: 246686   epsilon: 0.7095597400063052    steps: 198    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1311   score: 1.0   memory length: 246857   epsilon: 0.7092211600063125    steps: 171    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1312   score: 2.0   memory length: 247054   epsilon: 0.708831100006321    steps: 197    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1313   score: 0.0   memory length: 247177   epsilon: 0.7085875600063263    steps: 123    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 1314   score: 0.0   memory length: 247300   epsilon: 0.7083440200063316    steps: 123    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1315   score: 3.0   memory length: 247544   epsilon: 0.707860900006342    steps: 244    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1316   score: 2.0   memory length: 247741   epsilon: 0.7074708400063505    steps: 197    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1317   score: 1.0   memory length: 247909   epsilon: 0.7071382000063577    steps: 168    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1318   score: 0.0   memory length: 248032   epsilon: 0.706894660006363    steps: 123    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1319   score: 2.0   memory length: 248250   epsilon: 0.7064630200063724    steps: 218    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1320   score: 2.0   memory length: 248448   epsilon: 0.7060709800063809    steps: 198    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 1321   score: 0.0   memory length: 248570   epsilon: 0.7058294200063862    steps: 122    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1322   score: 2.0   memory length: 248767   epsilon: 0.7054393600063946    steps: 197    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1323   score: 2.0   memory length: 248964   epsilon: 0.7050493000064031    steps: 197    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1324   score: 2.0   memory length: 249162   epsilon: 0.7046572600064116    steps: 198    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1325   score: 2.0   memory length: 249360   epsilon: 0.7042652200064201    steps: 198    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1326   score: 2.0   memory length: 249558   epsilon: 0.7038731800064286    steps: 198    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1327   score: 2.0   memory length: 249756   epsilon: 0.7034811400064371    steps: 198    lr: 0.0001     evaluation reward: 2.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1328   score: 0.0   memory length: 249879   epsilon: 0.7032376000064424    steps: 123    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1329   score: 2.0   memory length: 250077   epsilon: 0.7028455600064509    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1330   score: 2.0   memory length: 250275   epsilon: 0.7024535200064594    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1331   score: 3.0   memory length: 250543   epsilon: 0.701922880006471    steps: 268    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1332   score: 2.0   memory length: 250741   epsilon: 0.7015308400064795    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1333   score: 2.0   memory length: 250939   epsilon: 0.701138800006488    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1334   score: 3.0   memory length: 251185   epsilon: 0.7006517200064986    steps: 246    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1335   score: 2.0   memory length: 251403   epsilon: 0.7002200800065079    steps: 218    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1336   score: 2.0   memory length: 251603   epsilon: 0.6998240800065165    steps: 200    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1337   score: 3.0   memory length: 251851   epsilon: 0.6993330400065272    steps: 248    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1338   score: 0.0   memory length: 251973   epsilon: 0.6990914800065324    steps: 122    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1339   score: 3.0   memory length: 252239   epsilon: 0.6985648000065439    steps: 266    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1340   score: 3.0   memory length: 252486   epsilon: 0.6980757400065545    steps: 247    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1341   score: 0.0   memory length: 252609   epsilon: 0.6978322000065598    steps: 123    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1342   score: 1.0   memory length: 252778   epsilon: 0.697497580006567    steps: 169    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1343   score: 2.0   memory length: 252996   epsilon: 0.6970659400065764    steps: 218    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1344   score: 2.0   memory length: 253194   epsilon: 0.6966739000065849    steps: 198    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1345   score: 3.0   memory length: 253442   epsilon: 0.6961828600065956    steps: 248    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1346   score: 1.0   memory length: 253611   epsilon: 0.6958482400066028    steps: 169    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1347   score: 0.0   memory length: 253733   epsilon: 0.6956066800066081    steps: 122    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1348   score: 1.0   memory length: 253903   epsilon: 0.6952700800066154    steps: 170    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1349   score: 2.0   memory length: 254101   epsilon: 0.6948780400066239    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1350   score: 2.0   memory length: 254322   epsilon: 0.6944404600066334    steps: 221    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1351   score: 3.0   memory length: 254589   epsilon: 0.6939118000066449    steps: 267    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1352   score: 2.0   memory length: 254787   epsilon: 0.6935197600066534    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1353   score: 3.0   memory length: 255033   epsilon: 0.693032680006664    steps: 246    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1354   score: 2.0   memory length: 255231   epsilon: 0.6926406400066725    steps: 198    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 1355   score: 0.0   memory length: 255353   epsilon: 0.6923990800066777    steps: 122    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1356   score: 2.0   memory length: 255551   epsilon: 0.6920070400066862    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1357   score: 4.0   memory length: 255846   epsilon: 0.6914229400066989    steps: 295    lr: 0.0001     evaluation reward: 1.95\n",
      "episode: 1358   score: 0.0   memory length: 255968   epsilon: 0.6911813800067041    steps: 122    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1359   score: 2.0   memory length: 256185   epsilon: 0.6907517200067135    steps: 217    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1360   score: 0.0   memory length: 256308   epsilon: 0.6905081800067188    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1361   score: 2.0   memory length: 256506   epsilon: 0.6901161400067273    steps: 198    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1362   score: 2.0   memory length: 256704   epsilon: 0.6897241000067358    steps: 198    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1363   score: 0.0   memory length: 256827   epsilon: 0.6894805600067411    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1364   score: 2.0   memory length: 257025   epsilon: 0.6890885200067496    steps: 198    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1365   score: 2.0   memory length: 257222   epsilon: 0.688698460006758    steps: 197    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1366   score: 1.0   memory length: 257391   epsilon: 0.6883638400067653    steps: 169    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 1367   score: 3.0   memory length: 257634   epsilon: 0.6878827000067758    steps: 243    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 1368   score: 3.0   memory length: 257900   epsilon: 0.6873560200067872    steps: 266    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 1369   score: 3.0   memory length: 258146   epsilon: 0.6868689400067978    steps: 246    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 1370   score: 2.0   memory length: 258365   epsilon: 0.6864353200068072    steps: 219    lr: 0.0001     evaluation reward: 1.92\n",
      "episode: 1371   score: 6.0   memory length: 258729   epsilon: 0.6857146000068228    steps: 364    lr: 0.0001     evaluation reward: 1.97\n",
      "episode: 1372   score: 2.0   memory length: 258926   epsilon: 0.6853245400068313    steps: 197    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1373   score: 3.0   memory length: 259134   epsilon: 0.6849127000068402    steps: 208    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 1374   score: 2.0   memory length: 259331   epsilon: 0.6845226400068487    steps: 197    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1375   score: 3.0   memory length: 259559   epsilon: 0.6840712000068585    steps: 228    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 1376   score: 5.0   memory length: 259890   epsilon: 0.6834158200068727    steps: 331    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 1377   score: 4.0   memory length: 260150   epsilon: 0.6829010200068839    steps: 260    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1378   score: 2.0   memory length: 260348   epsilon: 0.6825089800068924    steps: 198    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1379   score: 2.0   memory length: 260566   epsilon: 0.6820773400069018    steps: 218    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1380   score: 1.0   memory length: 260735   epsilon: 0.681742720006909    steps: 169    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1381   score: 2.0   memory length: 260934   epsilon: 0.6813487000069176    steps: 199    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1382   score: 0.0   memory length: 261057   epsilon: 0.6811051600069229    steps: 123    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 1383   score: 2.0   memory length: 261273   epsilon: 0.6806774800069322    steps: 216    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1384   score: 2.0   memory length: 261471   epsilon: 0.6802854400069407    steps: 198    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1385   score: 3.0   memory length: 261717   epsilon: 0.6797983600069513    steps: 246    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 1386   score: 2.0   memory length: 261915   epsilon: 0.6794063200069598    steps: 198    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 1387   score: 2.0   memory length: 262133   epsilon: 0.6789746800069691    steps: 218    lr: 0.0001     evaluation reward: 2.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1388   score: 2.0   memory length: 262351   epsilon: 0.6785430400069785    steps: 218    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1389   score: 2.0   memory length: 262568   epsilon: 0.6781133800069878    steps: 217    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1390   score: 2.0   memory length: 262766   epsilon: 0.6777213400069964    steps: 198    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1391   score: 3.0   memory length: 263013   epsilon: 0.677232280007007    steps: 247    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1392   score: 3.0   memory length: 263260   epsilon: 0.6767432200070176    steps: 247    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1393   score: 4.0   memory length: 263557   epsilon: 0.6761551600070304    steps: 297    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1394   score: 1.0   memory length: 263707   epsilon: 0.6758581600070368    steps: 150    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1395   score: 3.0   memory length: 263974   epsilon: 0.6753295000070483    steps: 267    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 1396   score: 3.0   memory length: 264200   epsilon: 0.674882020007058    steps: 226    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1397   score: 4.0   memory length: 264496   epsilon: 0.6742959400070707    steps: 296    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1398   score: 2.0   memory length: 264693   epsilon: 0.6739058800070792    steps: 197    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1399   score: 1.0   memory length: 264863   epsilon: 0.6735692800070865    steps: 170    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1400   score: 5.0   memory length: 265199   epsilon: 0.6729040000071009    steps: 336    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1401   score: 0.0   memory length: 265322   epsilon: 0.6726604600071062    steps: 123    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1402   score: 1.0   memory length: 265491   epsilon: 0.6723258400071135    steps: 169    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1403   score: 3.0   memory length: 265717   epsilon: 0.6718783600071232    steps: 226    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1404   score: 3.0   memory length: 265963   epsilon: 0.6713912800071338    steps: 246    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1405   score: 4.0   memory length: 266259   epsilon: 0.6708052000071465    steps: 296    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1406   score: 3.0   memory length: 266488   epsilon: 0.6703517800071563    steps: 229    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1407   score: 3.0   memory length: 266734   epsilon: 0.6698647000071669    steps: 246    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1408   score: 0.0   memory length: 266857   epsilon: 0.6696211600071722    steps: 123    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1409   score: 1.0   memory length: 267026   epsilon: 0.6692865400071795    steps: 169    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1410   score: 3.0   memory length: 267253   epsilon: 0.6688370800071892    steps: 227    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1411   score: 2.0   memory length: 267471   epsilon: 0.6684054400071986    steps: 218    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1412   score: 1.0   memory length: 267622   epsilon: 0.6681064600072051    steps: 151    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1413   score: 0.0   memory length: 267745   epsilon: 0.6678629200072104    steps: 123    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1414   score: 3.0   memory length: 268011   epsilon: 0.6673362400072218    steps: 266    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 1415   score: 0.0   memory length: 268134   epsilon: 0.6670927000072271    steps: 123    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 1416   score: 1.0   memory length: 268306   epsilon: 0.6667521400072345    steps: 172    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1417   score: 1.0   memory length: 268477   epsilon: 0.6664135600072418    steps: 171    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 1418   score: 2.0   memory length: 268695   epsilon: 0.6659819200072512    steps: 218    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 1419   score: 3.0   memory length: 268942   epsilon: 0.6654928600072618    steps: 247    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1420   score: 2.0   memory length: 269160   epsilon: 0.6650612200072712    steps: 218    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 1421   score: 4.0   memory length: 269417   epsilon: 0.6645523600072822    steps: 257    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 1422   score: 3.0   memory length: 269643   epsilon: 0.664104880007292    steps: 226    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1423   score: 6.0   memory length: 269999   epsilon: 0.6634000000073073    steps: 356    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 1424   score: 5.0   memory length: 270343   epsilon: 0.662718880007322    steps: 344    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1425   score: 2.0   memory length: 270560   epsilon: 0.6622892200073314    steps: 217    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1426   score: 2.0   memory length: 270779   epsilon: 0.6618556000073408    steps: 219    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1427   score: 2.0   memory length: 271000   epsilon: 0.6614180200073503    steps: 221    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1428   score: 2.0   memory length: 271198   epsilon: 0.6610259800073588    steps: 198    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1429   score: 2.0   memory length: 271416   epsilon: 0.6605943400073682    steps: 218    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1430   score: 2.0   memory length: 271613   epsilon: 0.6602042800073766    steps: 197    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1431   score: 3.0   memory length: 271860   epsilon: 0.6597152200073872    steps: 247    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1432   score: 2.0   memory length: 272057   epsilon: 0.6593251600073957    steps: 197    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1433   score: 4.0   memory length: 272370   epsilon: 0.6587054200074092    steps: 313    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1434   score: 4.0   memory length: 272665   epsilon: 0.6581213200074219    steps: 295    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1435   score: 0.0   memory length: 272788   epsilon: 0.6578777800074271    steps: 123    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1436   score: 1.0   memory length: 272939   epsilon: 0.6575788000074336    steps: 151    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1437   score: 2.0   memory length: 273140   epsilon: 0.6571808200074423    steps: 201    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1438   score: 2.0   memory length: 273340   epsilon: 0.6567848200074509    steps: 200    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1439   score: 2.0   memory length: 273538   epsilon: 0.6563927800074594    steps: 198    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1440   score: 2.0   memory length: 273720   epsilon: 0.6560324200074672    steps: 182    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1441   score: 3.0   memory length: 273985   epsilon: 0.6555077200074786    steps: 265    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1442   score: 2.0   memory length: 274183   epsilon: 0.6551156800074871    steps: 198    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1443   score: 3.0   memory length: 274409   epsilon: 0.6546682000074968    steps: 226    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1444   score: 3.0   memory length: 274655   epsilon: 0.6541811200075074    steps: 246    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 1445   score: 4.0   memory length: 274971   epsilon: 0.653555440007521    steps: 316    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1446   score: 2.0   memory length: 275169   epsilon: 0.6531634000075295    steps: 198    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1447   score: 2.0   memory length: 275366   epsilon: 0.652773340007538    steps: 197    lr: 0.0001     evaluation reward: 2.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1448   score: 3.0   memory length: 275611   epsilon: 0.6522882400075485    steps: 245    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1449   score: 2.0   memory length: 275809   epsilon: 0.651896200007557    steps: 198    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1450   score: 2.0   memory length: 276007   epsilon: 0.6515041600075655    steps: 198    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1451   score: 2.0   memory length: 276205   epsilon: 0.651112120007574    steps: 198    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1452   score: 2.0   memory length: 276402   epsilon: 0.6507220600075825    steps: 197    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1453   score: 0.0   memory length: 276524   epsilon: 0.6504805000075877    steps: 122    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1454   score: 2.0   memory length: 276721   epsilon: 0.6500904400075962    steps: 197    lr: 0.0001     evaluation reward: 2.26\n",
      "episode: 1455   score: 3.0   memory length: 276967   epsilon: 0.6496033600076068    steps: 246    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1456   score: 3.0   memory length: 277193   epsilon: 0.6491558800076165    steps: 226    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1457   score: 2.0   memory length: 277408   epsilon: 0.6487301800076257    steps: 215    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1458   score: 2.0   memory length: 277605   epsilon: 0.6483401200076342    steps: 197    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1459   score: 2.0   memory length: 277822   epsilon: 0.6479104600076435    steps: 217    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1460   score: 4.0   memory length: 278104   epsilon: 0.6473521000076556    steps: 282    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1461   score: 2.0   memory length: 278302   epsilon: 0.6469600600076642    steps: 198    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1462   score: 2.0   memory length: 278519   epsilon: 0.6465304000076735    steps: 217    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1463   score: 2.0   memory length: 278699   epsilon: 0.6461740000076812    steps: 180    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1464   score: 3.0   memory length: 278927   epsilon: 0.645722560007691    steps: 228    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1465   score: 2.0   memory length: 279125   epsilon: 0.6453305200076995    steps: 198    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1466   score: 1.0   memory length: 279293   epsilon: 0.6449978800077067    steps: 168    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1467   score: 4.0   memory length: 279550   epsilon: 0.6444890200077178    steps: 257    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1468   score: 3.0   memory length: 279776   epsilon: 0.6440415400077275    steps: 226    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1469   score: 2.0   memory length: 279993   epsilon: 0.6436118800077368    steps: 217    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1470   score: 2.0   memory length: 280191   epsilon: 0.6432198400077453    steps: 198    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 1471   score: 3.0   memory length: 280417   epsilon: 0.6427723600077551    steps: 226    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1472   score: 2.0   memory length: 280638   epsilon: 0.6423347800077646    steps: 221    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1473   score: 4.0   memory length: 280938   epsilon: 0.6417407800077775    steps: 300    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1474   score: 1.0   memory length: 281089   epsilon: 0.641441800007784    steps: 151    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1475   score: 2.0   memory length: 281307   epsilon: 0.6410101600077933    steps: 218    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1476   score: 2.0   memory length: 281505   epsilon: 0.6406181200078018    steps: 198    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1477   score: 2.0   memory length: 281703   epsilon: 0.6402260800078103    steps: 198    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1478   score: 2.0   memory length: 281901   epsilon: 0.6398340400078188    steps: 198    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1479   score: 2.0   memory length: 282098   epsilon: 0.6394439800078273    steps: 197    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1480   score: 0.0   memory length: 282221   epsilon: 0.6392004400078326    steps: 123    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 1481   score: 3.0   memory length: 282488   epsilon: 0.6386717800078441    steps: 267    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1482   score: 1.0   memory length: 282657   epsilon: 0.6383371600078513    steps: 169    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1483   score: 2.0   memory length: 282855   epsilon: 0.6379451200078599    steps: 198    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1484   score: 0.0   memory length: 282977   epsilon: 0.6377035600078651    steps: 122    lr: 0.0001     evaluation reward: 2.27\n",
      "episode: 1485   score: 0.0   memory length: 283099   epsilon: 0.6374620000078703    steps: 122    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 1486   score: 3.0   memory length: 283325   epsilon: 0.6370145200078801    steps: 226    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1487   score: 2.0   memory length: 283543   epsilon: 0.6365828800078894    steps: 218    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1488   score: 2.0   memory length: 283741   epsilon: 0.6361908400078979    steps: 198    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1489   score: 2.0   memory length: 283939   epsilon: 0.6357988000079065    steps: 198    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1490   score: 2.0   memory length: 284137   epsilon: 0.635406760007915    steps: 198    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1491   score: 3.0   memory length: 284404   epsilon: 0.6348781000079264    steps: 267    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 1492   score: 0.0   memory length: 284526   epsilon: 0.6346365400079317    steps: 122    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1493   score: 0.0   memory length: 284649   epsilon: 0.634393000007937    steps: 123    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1494   score: 1.0   memory length: 284821   epsilon: 0.6340524400079444    steps: 172    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1495   score: 4.0   memory length: 285096   epsilon: 0.6335079400079562    steps: 275    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1496   score: 3.0   memory length: 285340   epsilon: 0.6330248200079667    steps: 244    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1497   score: 0.0   memory length: 285462   epsilon: 0.6327832600079719    steps: 122    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1498   score: 0.0   memory length: 285585   epsilon: 0.6325397200079772    steps: 123    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1499   score: 2.0   memory length: 285802   epsilon: 0.6321100600079865    steps: 217    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 1500   score: 0.0   memory length: 285924   epsilon: 0.6318685000079918    steps: 122    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 1501   score: 0.0   memory length: 286047   epsilon: 0.6316249600079971    steps: 123    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 1502   score: 1.0   memory length: 286219   epsilon: 0.6312844000080045    steps: 172    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 1503   score: 2.0   memory length: 286436   epsilon: 0.6308547400080138    steps: 217    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1504   score: 5.0   memory length: 286744   epsilon: 0.630244900008027    steps: 308    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1505   score: 2.0   memory length: 286942   epsilon: 0.6298528600080355    steps: 198    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 1506   score: 2.0   memory length: 287140   epsilon: 0.629460820008044    steps: 198    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 1507   score: 3.0   memory length: 287406   epsilon: 0.6289341400080555    steps: 266    lr: 0.0001     evaluation reward: 2.07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1508   score: 3.0   memory length: 287676   epsilon: 0.6283995400080671    steps: 270    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1509   score: 3.0   memory length: 287902   epsilon: 0.6279520600080768    steps: 226    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 1510   score: 4.0   memory length: 288199   epsilon: 0.6273640000080896    steps: 297    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1511   score: 0.0   memory length: 288321   epsilon: 0.6271224400080948    steps: 122    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 1512   score: 4.0   memory length: 288614   epsilon: 0.6265423000081074    steps: 293    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 1513   score: 3.0   memory length: 288840   epsilon: 0.6260948200081171    steps: 226    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1514   score: 2.0   memory length: 289038   epsilon: 0.6257027800081256    steps: 198    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1515   score: 1.0   memory length: 289208   epsilon: 0.6253661800081329    steps: 170    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1516   score: 4.0   memory length: 289505   epsilon: 0.6247781200081457    steps: 297    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1517   score: 3.0   memory length: 289751   epsilon: 0.6242910400081563    steps: 246    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1518   score: 3.0   memory length: 289977   epsilon: 0.623843560008166    steps: 226    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1519   score: 2.0   memory length: 290195   epsilon: 0.6234119200081754    steps: 218    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1520   score: 3.0   memory length: 290462   epsilon: 0.6228832600081868    steps: 267    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1521   score: 2.0   memory length: 290680   epsilon: 0.6224516200081962    steps: 218    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1522   score: 2.0   memory length: 290880   epsilon: 0.6220556200082048    steps: 200    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1523   score: 4.0   memory length: 291146   epsilon: 0.6215289400082162    steps: 266    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1524   score: 2.0   memory length: 291328   epsilon: 0.6211685800082241    steps: 182    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1525   score: 2.0   memory length: 291526   epsilon: 0.6207765400082326    steps: 198    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1526   score: 2.0   memory length: 291744   epsilon: 0.6203449000082419    steps: 218    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1527   score: 4.0   memory length: 292041   epsilon: 0.6197568400082547    steps: 297    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1528   score: 2.0   memory length: 292222   epsilon: 0.6193984600082625    steps: 181    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1529   score: 3.0   memory length: 292491   epsilon: 0.618865840008274    steps: 269    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1530   score: 2.0   memory length: 292689   epsilon: 0.6184738000082826    steps: 198    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1531   score: 2.0   memory length: 292908   epsilon: 0.618040180008292    steps: 219    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1532   score: 0.0   memory length: 293031   epsilon: 0.6177966400082973    steps: 123    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1533   score: 2.0   memory length: 293250   epsilon: 0.6173630200083067    steps: 219    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1534   score: 1.0   memory length: 293422   epsilon: 0.6170224600083141    steps: 172    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 1535   score: 2.0   memory length: 293620   epsilon: 0.6166304200083226    steps: 198    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 1536   score: 3.0   memory length: 293845   epsilon: 0.6161849200083322    steps: 225    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 1537   score: 3.0   memory length: 294071   epsilon: 0.615737440008342    steps: 226    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1538   score: 3.0   memory length: 294316   epsilon: 0.6152523400083525    steps: 245    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1539   score: 2.0   memory length: 294514   epsilon: 0.614860300008361    steps: 198    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1540   score: 2.0   memory length: 294732   epsilon: 0.6144286600083704    steps: 218    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1541   score: 2.0   memory length: 294930   epsilon: 0.6140366200083789    steps: 198    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1542   score: 2.0   memory length: 295128   epsilon: 0.6136445800083874    steps: 198    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 1543   score: 0.0   memory length: 295251   epsilon: 0.6134010400083927    steps: 123    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 1544   score: 3.0   memory length: 295478   epsilon: 0.6129515800084024    steps: 227    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 1545   score: 3.0   memory length: 295706   epsilon: 0.6125001400084122    steps: 228    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 1546   score: 4.0   memory length: 295984   epsilon: 0.6119497000084242    steps: 278    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 1547   score: 6.0   memory length: 296360   epsilon: 0.6112052200084404    steps: 376    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1548   score: 2.0   memory length: 296578   epsilon: 0.6107735800084497    steps: 218    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 1549   score: 3.0   memory length: 296827   epsilon: 0.6102805600084604    steps: 249    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1550   score: 2.0   memory length: 297024   epsilon: 0.6098905000084689    steps: 197    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1551   score: 2.0   memory length: 297221   epsilon: 0.6095004400084774    steps: 197    lr: 0.0001     evaluation reward: 2.17\n",
      "episode: 1552   score: 3.0   memory length: 297446   epsilon: 0.609054940008487    steps: 225    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1553   score: 2.0   memory length: 297644   epsilon: 0.6086629000084955    steps: 198    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1554   score: 3.0   memory length: 297891   epsilon: 0.6081738400085062    steps: 247    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1555   score: 3.0   memory length: 298138   epsilon: 0.6076847800085168    steps: 247    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1556   score: 0.0   memory length: 298261   epsilon: 0.6074412400085221    steps: 123    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1557   score: 3.0   memory length: 298507   epsilon: 0.6069541600085326    steps: 246    lr: 0.0001     evaluation reward: 2.19\n",
      "episode: 1558   score: 3.0   memory length: 298736   epsilon: 0.6065007400085425    steps: 229    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1559   score: 4.0   memory length: 299011   epsilon: 0.6059562400085543    steps: 275    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1560   score: 5.0   memory length: 299357   epsilon: 0.6052711600085692    steps: 346    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1561   score: 1.0   memory length: 299529   epsilon: 0.6049306000085766    steps: 172    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1562   score: 2.0   memory length: 299727   epsilon: 0.6045385600085851    steps: 198    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1563   score: 3.0   memory length: 299955   epsilon: 0.6040871200085949    steps: 228    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1564   score: 3.0   memory length: 300220   epsilon: 0.6035624200086063    steps: 265    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1565   score: 1.0   memory length: 300389   epsilon: 0.6032278000086135    steps: 169    lr: 0.0001     evaluation reward: 2.22\n",
      "episode: 1566   score: 2.0   memory length: 300586   epsilon: 0.602837740008622    steps: 197    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1567   score: 2.0   memory length: 300765   epsilon: 0.6024833200086297    steps: 179    lr: 0.0001     evaluation reward: 2.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1568   score: 2.0   memory length: 300983   epsilon: 0.6020516800086391    steps: 218    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1569   score: 2.0   memory length: 301181   epsilon: 0.6016596400086476    steps: 198    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1570   score: 3.0   memory length: 301409   epsilon: 0.6012082000086574    steps: 228    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1571   score: 2.0   memory length: 301607   epsilon: 0.6008161600086659    steps: 198    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1572   score: 2.0   memory length: 301805   epsilon: 0.6004241200086744    steps: 198    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1573   score: 2.0   memory length: 302003   epsilon: 0.6000320800086829    steps: 198    lr: 0.0001     evaluation reward: 2.18\n",
      "episode: 1574   score: 3.0   memory length: 302231   epsilon: 0.5995806400086927    steps: 228    lr: 0.0001     evaluation reward: 2.2\n",
      "episode: 1575   score: 3.0   memory length: 302460   epsilon: 0.5991272200087026    steps: 229    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1576   score: 2.0   memory length: 302657   epsilon: 0.598737160008711    steps: 197    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 1577   score: 4.0   memory length: 302954   epsilon: 0.5981491000087238    steps: 297    lr: 0.0001     evaluation reward: 2.23\n",
      "episode: 1578   score: 3.0   memory length: 303180   epsilon: 0.5977016200087335    steps: 226    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 1579   score: 2.0   memory length: 303377   epsilon: 0.597311560008742    steps: 197    lr: 0.0001     evaluation reward: 2.24\n",
      "episode: 1580   score: 5.0   memory length: 303701   epsilon: 0.5966700400087559    steps: 324    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1581   score: 2.0   memory length: 303919   epsilon: 0.5962384000087653    steps: 218    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1582   score: 2.0   memory length: 304116   epsilon: 0.5958483400087737    steps: 197    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 1583   score: 4.0   memory length: 304413   epsilon: 0.5952602800087865    steps: 297    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1584   score: 2.0   memory length: 304631   epsilon: 0.5948286400087959    steps: 218    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1585   score: 1.0   memory length: 304782   epsilon: 0.5945296600088024    steps: 151    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1586   score: 2.0   memory length: 304981   epsilon: 0.5941356400088109    steps: 199    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1587   score: 3.0   memory length: 305209   epsilon: 0.5936842000088207    steps: 228    lr: 0.0001     evaluation reward: 2.34\n",
      "episode: 1588   score: 0.0   memory length: 305331   epsilon: 0.593442640008826    steps: 122    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 1589   score: 3.0   memory length: 305557   epsilon: 0.5929951600088357    steps: 226    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 1590   score: 0.0   memory length: 305680   epsilon: 0.592751620008841    steps: 123    lr: 0.0001     evaluation reward: 2.31\n",
      "episode: 1591   score: 0.0   memory length: 305803   epsilon: 0.5925080800088462    steps: 123    lr: 0.0001     evaluation reward: 2.28\n",
      "episode: 1592   score: 2.0   memory length: 306021   epsilon: 0.5920764400088556    steps: 218    lr: 0.0001     evaluation reward: 2.3\n",
      "episode: 1593   score: 2.0   memory length: 306240   epsilon: 0.591642820008865    steps: 219    lr: 0.0001     evaluation reward: 2.32\n",
      "episode: 1594   score: 4.0   memory length: 306516   epsilon: 0.5910963400088769    steps: 276    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1595   score: 4.0   memory length: 306811   epsilon: 0.5905122400088896    steps: 295    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 1596   score: 4.0   memory length: 307088   epsilon: 0.5899637800089015    steps: 277    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 1597   score: 2.0   memory length: 307285   epsilon: 0.58957372000891    steps: 197    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 1598   score: 1.0   memory length: 307456   epsilon: 0.5892351400089173    steps: 171    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1599   score: 2.0   memory length: 307653   epsilon: 0.5888450800089258    steps: 197    lr: 0.0001     evaluation reward: 2.39\n",
      "episode: 1600   score: 2.0   memory length: 307851   epsilon: 0.5884530400089343    steps: 198    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 1601   score: 3.0   memory length: 308077   epsilon: 0.588005560008944    steps: 226    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1602   score: 1.0   memory length: 308228   epsilon: 0.5877065800089505    steps: 151    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1603   score: 2.0   memory length: 308410   epsilon: 0.5873462200089583    steps: 182    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 1604   score: 3.0   memory length: 308638   epsilon: 0.5868947800089681    steps: 228    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 1605   score: 3.0   memory length: 308865   epsilon: 0.5864453200089779    steps: 227    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 1606   score: 4.0   memory length: 309122   epsilon: 0.5859364600089889    steps: 257    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1607   score: 3.0   memory length: 309348   epsilon: 0.5854889800089986    steps: 226    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 1608   score: 7.0   memory length: 309740   epsilon: 0.5847128200090155    steps: 392    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1609   score: 3.0   memory length: 309965   epsilon: 0.5842673200090251    steps: 225    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1610   score: 2.0   memory length: 310183   epsilon: 0.5838356800090345    steps: 218    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 1611   score: 2.0   memory length: 310381   epsilon: 0.583443640009043    steps: 198    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1612   score: 3.0   memory length: 310625   epsilon: 0.5829605200090535    steps: 244    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1613   score: 4.0   memory length: 310938   epsilon: 0.582340780009067    steps: 313    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1614   score: 3.0   memory length: 311186   epsilon: 0.5818497400090776    steps: 248    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1615   score: 2.0   memory length: 311384   epsilon: 0.5814577000090861    steps: 198    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1616   score: 4.0   memory length: 311663   epsilon: 0.5809052800090981    steps: 279    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1617   score: 2.0   memory length: 311880   epsilon: 0.5804756200091075    steps: 217    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1618   score: 3.0   memory length: 312106   epsilon: 0.5800281400091172    steps: 226    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1619   score: 4.0   memory length: 312381   epsilon: 0.579483640009129    steps: 275    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1620   score: 1.0   memory length: 312552   epsilon: 0.5791450600091363    steps: 171    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1621   score: 0.0   memory length: 312675   epsilon: 0.5789015200091416    steps: 123    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 1622   score: 6.0   memory length: 313042   epsilon: 0.5781748600091574    steps: 367    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1623   score: 3.0   memory length: 313268   epsilon: 0.5777273800091671    steps: 226    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1624   score: 3.0   memory length: 313514   epsilon: 0.5772403000091777    steps: 246    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1625   score: 3.0   memory length: 313742   epsilon: 0.5767888600091875    steps: 228    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1626   score: 4.0   memory length: 313994   epsilon: 0.5762899000091983    steps: 252    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1627   score: 2.0   memory length: 314192   epsilon: 0.5758978600092068    steps: 198    lr: 0.0001     evaluation reward: 2.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1628   score: 2.0   memory length: 314390   epsilon: 0.5755058200092154    steps: 198    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1629   score: 2.0   memory length: 314588   epsilon: 0.5751137800092239    steps: 198    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1630   score: 4.0   memory length: 314845   epsilon: 0.5746049200092349    steps: 257    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1631   score: 1.0   memory length: 314996   epsilon: 0.5743059400092414    steps: 151    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1632   score: 1.0   memory length: 315147   epsilon: 0.5740069600092479    steps: 151    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1633   score: 3.0   memory length: 315394   epsilon: 0.5735179000092585    steps: 247    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1634   score: 3.0   memory length: 315642   epsilon: 0.5730268600092692    steps: 248    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1635   score: 2.0   memory length: 315840   epsilon: 0.5726348200092777    steps: 198    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1636   score: 2.0   memory length: 316038   epsilon: 0.5722427800092862    steps: 198    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1637   score: 3.0   memory length: 316269   epsilon: 0.5717854000092961    steps: 231    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1638   score: 3.0   memory length: 316515   epsilon: 0.5712983200093067    steps: 246    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1639   score: 3.0   memory length: 316759   epsilon: 0.5708152000093172    steps: 244    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1640   score: 2.0   memory length: 316957   epsilon: 0.5704231600093257    steps: 198    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1641   score: 2.0   memory length: 317174   epsilon: 0.569993500009335    steps: 217    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1642   score: 3.0   memory length: 317420   epsilon: 0.5695064200093456    steps: 246    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1643   score: 3.0   memory length: 317645   epsilon: 0.5690609200093553    steps: 225    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1644   score: 3.0   memory length: 317871   epsilon: 0.568613440009365    steps: 226    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 1645   score: 2.0   memory length: 318089   epsilon: 0.5681818000093743    steps: 218    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1646   score: 3.0   memory length: 318336   epsilon: 0.567692740009385    steps: 247    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1647   score: 2.0   memory length: 318534   epsilon: 0.5673007000093935    steps: 198    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1648   score: 3.0   memory length: 318779   epsilon: 0.566815600009404    steps: 245    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1649   score: 2.0   memory length: 318998   epsilon: 0.5663819800094134    steps: 219    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1650   score: 3.0   memory length: 319224   epsilon: 0.5659345000094231    steps: 226    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1651   score: 3.0   memory length: 319452   epsilon: 0.5654830600094329    steps: 228    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1652   score: 2.0   memory length: 319650   epsilon: 0.5650910200094414    steps: 198    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 1653   score: 4.0   memory length: 319924   epsilon: 0.5645485000094532    steps: 274    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1654   score: 2.0   memory length: 320105   epsilon: 0.564190120009461    steps: 181    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1655   score: 1.0   memory length: 320256   epsilon: 0.5638911400094675    steps: 151    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1656   score: 3.0   memory length: 320481   epsilon: 0.5634456400094772    steps: 225    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1657   score: 3.0   memory length: 320727   epsilon: 0.5629585600094877    steps: 246    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1658   score: 4.0   memory length: 320973   epsilon: 0.5624714800094983    steps: 246    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1659   score: 5.0   memory length: 321319   epsilon: 0.5617864000095132    steps: 346    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1660   score: 2.0   memory length: 321536   epsilon: 0.5613567400095225    steps: 217    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1661   score: 3.0   memory length: 321782   epsilon: 0.5608696600095331    steps: 246    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1662   score: 1.0   memory length: 321951   epsilon: 0.5605350400095404    steps: 169    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1663   score: 3.0   memory length: 322178   epsilon: 0.5600855800095501    steps: 227    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1664   score: 2.0   memory length: 322376   epsilon: 0.5596935400095586    steps: 198    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1665   score: 3.0   memory length: 322601   epsilon: 0.5592480400095683    steps: 225    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1666   score: 3.0   memory length: 322849   epsilon: 0.558757000009579    steps: 248    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1667   score: 1.0   memory length: 322999   epsilon: 0.5584600000095854    steps: 150    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1668   score: 7.0   memory length: 323390   epsilon: 0.5576858200096022    steps: 391    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1669   score: 2.0   memory length: 323608   epsilon: 0.5572541800096116    steps: 218    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1670   score: 3.0   memory length: 323833   epsilon: 0.5568086800096212    steps: 225    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1671   score: 3.0   memory length: 324078   epsilon: 0.5563235800096318    steps: 245    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 1672   score: 4.0   memory length: 324353   epsilon: 0.5557790800096436    steps: 275    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 1673   score: 3.0   memory length: 324579   epsilon: 0.5553316000096533    steps: 226    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1674   score: 3.0   memory length: 324848   epsilon: 0.5547989800096649    steps: 269    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1675   score: 6.0   memory length: 325207   epsilon: 0.5540881600096803    steps: 359    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 1676   score: 3.0   memory length: 325433   epsilon: 0.55364068000969    steps: 226    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1677   score: 3.0   memory length: 325677   epsilon: 0.5531575600097005    steps: 244    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 1678   score: 2.0   memory length: 325875   epsilon: 0.552765520009709    steps: 198    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1679   score: 0.0   memory length: 325997   epsilon: 0.5525239600097143    steps: 122    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1680   score: 3.0   memory length: 326264   epsilon: 0.5519953000097257    steps: 267    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1681   score: 4.0   memory length: 326542   epsilon: 0.5514448600097377    steps: 278    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1682   score: 3.0   memory length: 326786   epsilon: 0.5509617400097482    steps: 244    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1683   score: 3.0   memory length: 327012   epsilon: 0.5505142600097579    steps: 226    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1684   score: 2.0   memory length: 327210   epsilon: 0.5501222200097664    steps: 198    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1685   score: 3.0   memory length: 327454   epsilon: 0.5496391000097769    steps: 244    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1686   score: 4.0   memory length: 327750   epsilon: 0.5490530200097896    steps: 296    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1687   score: 6.0   memory length: 328114   epsilon: 0.5483323000098053    steps: 364    lr: 0.0001     evaluation reward: 2.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1688   score: 3.0   memory length: 328360   epsilon: 0.5478452200098158    steps: 246    lr: 0.0001     evaluation reward: 2.78\n",
      "episode: 1689   score: 1.0   memory length: 328530   epsilon: 0.5475086200098231    steps: 170    lr: 0.0001     evaluation reward: 2.76\n",
      "episode: 1690   score: 3.0   memory length: 328755   epsilon: 0.5470631200098328    steps: 225    lr: 0.0001     evaluation reward: 2.79\n",
      "episode: 1691   score: 2.0   memory length: 328953   epsilon: 0.5466710800098413    steps: 198    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 1692   score: 3.0   memory length: 329203   epsilon: 0.5461760800098521    steps: 250    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1693   score: 3.0   memory length: 329429   epsilon: 0.5457286000098618    steps: 226    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 1694   score: 3.0   memory length: 329673   epsilon: 0.5452454800098723    steps: 244    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1695   score: 3.0   memory length: 329917   epsilon: 0.5447623600098828    steps: 244    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 1696   score: 2.0   memory length: 330115   epsilon: 0.5443703200098913    steps: 198    lr: 0.0001     evaluation reward: 2.79\n",
      "episode: 1697   score: 3.0   memory length: 330362   epsilon: 0.5438812600099019    steps: 247    lr: 0.0001     evaluation reward: 2.8\n",
      "episode: 1698   score: 2.0   memory length: 330560   epsilon: 0.5434892200099104    steps: 198    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 1699   score: 8.0   memory length: 330986   epsilon: 0.5426457400099287    steps: 426    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1700   score: 2.0   memory length: 331204   epsilon: 0.5422141000099381    steps: 218    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1701   score: 2.0   memory length: 331402   epsilon: 0.5418220600099466    steps: 198    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1702   score: 0.0   memory length: 331525   epsilon: 0.5415785200099519    steps: 123    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 1703   score: 2.0   memory length: 331723   epsilon: 0.5411864800099604    steps: 198    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 1704   score: 2.0   memory length: 331921   epsilon: 0.5407944400099689    steps: 198    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 1705   score: 3.0   memory length: 332165   epsilon: 0.5403113200099794    steps: 244    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 1706   score: 3.0   memory length: 332392   epsilon: 0.5398618600099891    steps: 227    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 1707   score: 9.0   memory length: 332877   epsilon: 0.53890156001001    steps: 485    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 1708   score: 2.0   memory length: 333075   epsilon: 0.5385095200100185    steps: 198    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 1709   score: 2.0   memory length: 333273   epsilon: 0.538117480010027    steps: 198    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 1710   score: 5.0   memory length: 333569   epsilon: 0.5375314000100397    steps: 296    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1711   score: 4.0   memory length: 333835   epsilon: 0.5370047200100512    steps: 266    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 1712   score: 3.0   memory length: 334061   epsilon: 0.5365572400100609    steps: 226    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 1713   score: 3.0   memory length: 334306   epsilon: 0.5360721400100714    steps: 245    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1714   score: 3.0   memory length: 334531   epsilon: 0.5356266400100811    steps: 225    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1715   score: 3.0   memory length: 334757   epsilon: 0.5351791600100908    steps: 226    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 1716   score: 3.0   memory length: 334983   epsilon: 0.5347316800101005    steps: 226    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1717   score: 4.0   memory length: 335257   epsilon: 0.5341891600101123    steps: 274    lr: 0.0001     evaluation reward: 2.89\n",
      "episode: 1718   score: 2.0   memory length: 335455   epsilon: 0.5337971200101208    steps: 198    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 1719   score: 3.0   memory length: 335681   epsilon: 0.5333496400101305    steps: 226    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1720   score: 2.0   memory length: 335878   epsilon: 0.532959580010139    steps: 197    lr: 0.0001     evaluation reward: 2.88\n",
      "episode: 1721   score: 4.0   memory length: 336154   epsilon: 0.5324131000101509    steps: 276    lr: 0.0001     evaluation reward: 2.92\n",
      "episode: 1722   score: 6.0   memory length: 336520   epsilon: 0.5316884200101666    steps: 366    lr: 0.0001     evaluation reward: 2.92\n",
      "episode: 1723   score: 4.0   memory length: 336795   epsilon: 0.5311439200101784    steps: 275    lr: 0.0001     evaluation reward: 2.93\n",
      "episode: 1724   score: 1.0   memory length: 336946   epsilon: 0.5308449400101849    steps: 151    lr: 0.0001     evaluation reward: 2.91\n",
      "episode: 1725   score: 6.0   memory length: 337295   epsilon: 0.5301539200101999    steps: 349    lr: 0.0001     evaluation reward: 2.94\n",
      "episode: 1726   score: 3.0   memory length: 337545   epsilon: 0.5296589200102106    steps: 250    lr: 0.0001     evaluation reward: 2.93\n",
      "episode: 1727   score: 3.0   memory length: 337771   epsilon: 0.5292114400102204    steps: 226    lr: 0.0001     evaluation reward: 2.94\n",
      "episode: 1728   score: 4.0   memory length: 338037   epsilon: 0.5286847600102318    steps: 266    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1729   score: 3.0   memory length: 338281   epsilon: 0.5282016400102423    steps: 244    lr: 0.0001     evaluation reward: 2.97\n",
      "episode: 1730   score: 1.0   memory length: 338450   epsilon: 0.5278670200102495    steps: 169    lr: 0.0001     evaluation reward: 2.94\n",
      "episode: 1731   score: 6.0   memory length: 338814   epsilon: 0.5271463000102652    steps: 364    lr: 0.0001     evaluation reward: 2.99\n",
      "episode: 1732   score: 2.0   memory length: 339014   epsilon: 0.5267503000102738    steps: 200    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1733   score: 6.0   memory length: 339357   epsilon: 0.5260711600102885    steps: 343    lr: 0.0001     evaluation reward: 3.03\n",
      "episode: 1734   score: 2.0   memory length: 339555   epsilon: 0.525679120010297    steps: 198    lr: 0.0001     evaluation reward: 3.02\n",
      "episode: 1735   score: 3.0   memory length: 339781   epsilon: 0.5252316400103068    steps: 226    lr: 0.0001     evaluation reward: 3.03\n",
      "episode: 1736   score: 6.0   memory length: 340145   epsilon: 0.5245109200103224    steps: 364    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1737   score: 3.0   memory length: 340370   epsilon: 0.5240654200103321    steps: 225    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1738   score: 3.0   memory length: 340596   epsilon: 0.5236179400103418    steps: 226    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1739   score: 2.0   memory length: 340794   epsilon: 0.5232259000103503    steps: 198    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1740   score: 2.0   memory length: 340992   epsilon: 0.5228338600103588    steps: 198    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1741   score: 3.0   memory length: 341257   epsilon: 0.5223091600103702    steps: 265    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1742   score: 4.0   memory length: 341512   epsilon: 0.5218042600103812    steps: 255    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1743   score: 2.0   memory length: 341730   epsilon: 0.5213726200103905    steps: 218    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1744   score: 2.0   memory length: 341928   epsilon: 0.520980580010399    steps: 198    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1745   score: 6.0   memory length: 342301   epsilon: 0.5202420400104151    steps: 373    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1746   score: 1.0   memory length: 342452   epsilon: 0.5199430600104216    steps: 151    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1747   score: 3.0   memory length: 342699   epsilon: 0.5194540000104322    steps: 247    lr: 0.0001     evaluation reward: 3.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1748   score: 2.0   memory length: 342897   epsilon: 0.5190619600104407    steps: 198    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1749   score: 2.0   memory length: 343094   epsilon: 0.5186719000104492    steps: 197    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1750   score: 4.0   memory length: 343369   epsilon: 0.518127400010461    steps: 275    lr: 0.0001     evaluation reward: 3.09\n",
      "episode: 1751   score: 3.0   memory length: 343595   epsilon: 0.5176799200104707    steps: 226    lr: 0.0001     evaluation reward: 3.09\n",
      "episode: 1752   score: 3.0   memory length: 343821   epsilon: 0.5172324400104804    steps: 226    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1753   score: 3.0   memory length: 344068   epsilon: 0.516743380010491    steps: 247    lr: 0.0001     evaluation reward: 3.09\n",
      "episode: 1754   score: 3.0   memory length: 344294   epsilon: 0.5162959000105007    steps: 226    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1755   score: 3.0   memory length: 344525   epsilon: 0.5158385200105107    steps: 231    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1756   score: 2.0   memory length: 344723   epsilon: 0.5154464800105192    steps: 198    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1757   score: 4.0   memory length: 344998   epsilon: 0.514901980010531    steps: 275    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1758   score: 5.0   memory length: 345303   epsilon: 0.5142980800105441    steps: 305    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1759   score: 6.0   memory length: 345663   epsilon: 0.5135852800105596    steps: 360    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1760   score: 3.0   memory length: 345909   epsilon: 0.5130982000105702    steps: 246    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1761   score: 1.0   memory length: 346078   epsilon: 0.5127635800105774    steps: 169    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1762   score: 6.0   memory length: 346395   epsilon: 0.512135920010591    steps: 317    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1763   score: 4.0   memory length: 346689   epsilon: 0.5115538000106037    steps: 294    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1764   score: 3.0   memory length: 346935   epsilon: 0.5110667200106143    steps: 246    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1765   score: 4.0   memory length: 347250   epsilon: 0.5104430200106278    steps: 315    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1766   score: 3.0   memory length: 347496   epsilon: 0.5099559400106384    steps: 246    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1767   score: 6.0   memory length: 347814   epsilon: 0.509326300010652    steps: 318    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1768   score: 3.0   memory length: 348040   epsilon: 0.5088788200106618    steps: 226    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1769   score: 3.0   memory length: 348266   epsilon: 0.5084313400106715    steps: 226    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1770   score: 3.0   memory length: 348492   epsilon: 0.5079838600106812    steps: 226    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1771   score: 3.0   memory length: 348721   epsilon: 0.507530440010691    steps: 229    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1772   score: 3.0   memory length: 348946   epsilon: 0.5070849400107007    steps: 225    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1773   score: 3.0   memory length: 349172   epsilon: 0.5066374600107104    steps: 226    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1774   score: 3.0   memory length: 349398   epsilon: 0.5061899800107201    steps: 226    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1775   score: 3.0   memory length: 349666   epsilon: 0.5056593400107317    steps: 268    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1776   score: 4.0   memory length: 349941   epsilon: 0.5051148400107435    steps: 275    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1777   score: 3.0   memory length: 350211   epsilon: 0.5045802400107551    steps: 270    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1778   score: 3.0   memory length: 350456   epsilon: 0.5040951400107656    steps: 245    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1779   score: 5.0   memory length: 350757   epsilon: 0.5034991600107785    steps: 301    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1780   score: 4.0   memory length: 351032   epsilon: 0.5029546600107904    steps: 275    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1781   score: 2.0   memory length: 351231   epsilon: 0.5025606400107989    steps: 199    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1782   score: 2.0   memory length: 351448   epsilon: 0.5021309800108082    steps: 217    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1783   score: 3.0   memory length: 351674   epsilon: 0.501683500010818    steps: 226    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1784   score: 7.0   memory length: 352060   epsilon: 0.5009192200108346    steps: 386    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1785   score: 5.0   memory length: 352374   epsilon: 0.500297500010848    steps: 314    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1786   score: 3.0   memory length: 352604   epsilon: 0.4998421000108535    steps: 230    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1787   score: 2.0   memory length: 352802   epsilon: 0.499450060010851    steps: 198    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1788   score: 6.0   memory length: 353154   epsilon: 0.4987531000108466    steps: 352    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1789   score: 3.0   memory length: 353380   epsilon: 0.4983056200108438    steps: 226    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1790   score: 2.0   memory length: 353578   epsilon: 0.4979135800108413    steps: 198    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1791   score: 2.0   memory length: 353796   epsilon: 0.49748194001083856    steps: 218    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1792   score: 2.0   memory length: 353994   epsilon: 0.4970899000108361    steps: 198    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1793   score: 6.0   memory length: 354322   epsilon: 0.496440460010832    steps: 328    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1794   score: 4.0   memory length: 354598   epsilon: 0.4958939800108285    steps: 276    lr: 0.0001     evaluation reward: 3.33\n",
      "episode: 1795   score: 5.0   memory length: 354944   epsilon: 0.4952089000108242    steps: 346    lr: 0.0001     evaluation reward: 3.35\n",
      "episode: 1796   score: 3.0   memory length: 355170   epsilon: 0.49476142001082135    steps: 226    lr: 0.0001     evaluation reward: 3.36\n",
      "episode: 1797   score: 3.0   memory length: 355396   epsilon: 0.4943139400108185    steps: 226    lr: 0.0001     evaluation reward: 3.36\n",
      "episode: 1798   score: 3.0   memory length: 355622   epsilon: 0.4938664600108157    steps: 226    lr: 0.0001     evaluation reward: 3.37\n",
      "episode: 1799   score: 4.0   memory length: 355901   epsilon: 0.4933140400108122    steps: 279    lr: 0.0001     evaluation reward: 3.33\n",
      "episode: 1800   score: 2.0   memory length: 356099   epsilon: 0.4929220000108097    steps: 198    lr: 0.0001     evaluation reward: 3.33\n",
      "episode: 1801   score: 4.0   memory length: 356373   epsilon: 0.4923794800108063    steps: 274    lr: 0.0001     evaluation reward: 3.35\n",
      "episode: 1802   score: 5.0   memory length: 356701   epsilon: 0.49173004001080217    steps: 328    lr: 0.0001     evaluation reward: 3.4\n",
      "episode: 1803   score: 2.0   memory length: 356898   epsilon: 0.4913399800107997    steps: 197    lr: 0.0001     evaluation reward: 3.4\n",
      "episode: 1804   score: 2.0   memory length: 357096   epsilon: 0.4909479400107972    steps: 198    lr: 0.0001     evaluation reward: 3.4\n",
      "episode: 1805   score: 3.0   memory length: 357363   epsilon: 0.4904192800107939    steps: 267    lr: 0.0001     evaluation reward: 3.4\n",
      "episode: 1806   score: 3.0   memory length: 357609   epsilon: 0.4899322000107908    steps: 246    lr: 0.0001     evaluation reward: 3.4\n",
      "episode: 1807   score: 3.0   memory length: 357853   epsilon: 0.48944908001078774    steps: 244    lr: 0.0001     evaluation reward: 3.34\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1808   score: 0.0   memory length: 357976   epsilon: 0.4892055400107862    steps: 123    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1809   score: 2.0   memory length: 358174   epsilon: 0.4888135000107837    steps: 198    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1810   score: 2.0   memory length: 358371   epsilon: 0.48842344001078125    steps: 197    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1811   score: 2.0   memory length: 358589   epsilon: 0.4879918000107785    steps: 218    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1812   score: 4.0   memory length: 358855   epsilon: 0.4874651200107752    steps: 266    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1813   score: 5.0   memory length: 359171   epsilon: 0.48683944001077123    steps: 316    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1814   score: 3.0   memory length: 359397   epsilon: 0.4863919600107684    steps: 226    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1815   score: 3.0   memory length: 359622   epsilon: 0.4859464600107656    steps: 225    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1816   score: 3.0   memory length: 359847   epsilon: 0.48550096001076276    steps: 225    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1817   score: 3.0   memory length: 360073   epsilon: 0.48505348001075993    steps: 226    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1818   score: 4.0   memory length: 360330   epsilon: 0.4845446200107567    steps: 257    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1819   score: 3.0   memory length: 360555   epsilon: 0.4840991200107539    steps: 225    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1820   score: 3.0   memory length: 360780   epsilon: 0.4836536200107511    steps: 225    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1821   score: 1.0   memory length: 360930   epsilon: 0.4833566200107492    steps: 150    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1822   score: 2.0   memory length: 361148   epsilon: 0.48292498001074646    steps: 218    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1823   score: 2.0   memory length: 361346   epsilon: 0.482532940010744    steps: 198    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1824   score: 5.0   memory length: 361672   epsilon: 0.4818874600107399    steps: 326    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1825   score: 4.0   memory length: 361991   epsilon: 0.4812558400107359    steps: 319    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1826   score: 3.0   memory length: 362217   epsilon: 0.48080836001073307    steps: 226    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1827   score: 3.0   memory length: 362443   epsilon: 0.48036088001073024    steps: 226    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1828   score: 3.0   memory length: 362669   epsilon: 0.4799134000107274    steps: 226    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1829   score: 3.0   memory length: 362895   epsilon: 0.4794659200107246    steps: 226    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1830   score: 3.0   memory length: 363142   epsilon: 0.4789768600107215    steps: 247    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1831   score: 3.0   memory length: 363367   epsilon: 0.47853136001071866    steps: 225    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1832   score: 3.0   memory length: 363593   epsilon: 0.47808388001071583    steps: 226    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1833   score: 1.0   memory length: 363743   epsilon: 0.47778688001071395    steps: 150    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1834   score: 2.0   memory length: 363941   epsilon: 0.4773948400107115    steps: 198    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1835   score: 3.0   memory length: 364167   epsilon: 0.47694736001070864    steps: 226    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1836   score: 3.0   memory length: 364415   epsilon: 0.47645632001070554    steps: 248    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1837   score: 0.0   memory length: 364538   epsilon: 0.476212780010704    steps: 123    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1838   score: 3.0   memory length: 364785   epsilon: 0.4757237200107009    steps: 247    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1839   score: 2.0   memory length: 364982   epsilon: 0.47533366001069843    steps: 197    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1840   score: 4.0   memory length: 365257   epsilon: 0.474789160010695    steps: 275    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1841   score: 0.0   memory length: 365379   epsilon: 0.47454760001069346    steps: 122    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1842   score: 3.0   memory length: 365605   epsilon: 0.47410012001069063    steps: 226    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1843   score: 5.0   memory length: 365921   epsilon: 0.47347444001068667    steps: 316    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1844   score: 4.0   memory length: 366215   epsilon: 0.472892320010683    steps: 294    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1845   score: 4.0   memory length: 366472   epsilon: 0.47238346001067977    steps: 257    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1846   score: 2.0   memory length: 366669   epsilon: 0.4719934000106773    steps: 197    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1847   score: 3.0   memory length: 366895   epsilon: 0.47154592001067447    steps: 226    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1848   score: 4.0   memory length: 367152   epsilon: 0.47103706001067125    steps: 257    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1849   score: 3.0   memory length: 367381   epsilon: 0.4705836400106684    steps: 229    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1850   score: 3.0   memory length: 367609   epsilon: 0.4701322000106655    steps: 228    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1851   score: 4.0   memory length: 367885   epsilon: 0.46958572001066207    steps: 276    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1852   score: 3.0   memory length: 368111   epsilon: 0.46913824001065924    steps: 226    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1853   score: 2.0   memory length: 368309   epsilon: 0.46874620001065676    steps: 198    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1854   score: 4.0   memory length: 368585   epsilon: 0.4681997200106533    steps: 276    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1855   score: 3.0   memory length: 368811   epsilon: 0.46775224001065047    steps: 226    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1856   score: 7.0   memory length: 369192   epsilon: 0.4669978600106457    steps: 381    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1857   score: 4.0   memory length: 369467   epsilon: 0.46645336001064225    steps: 275    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1858   score: 3.0   memory length: 369693   epsilon: 0.4660058800106394    steps: 226    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1859   score: 4.0   memory length: 369968   epsilon: 0.46546138001063597    steps: 275    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1860   score: 3.0   memory length: 370194   epsilon: 0.46501390001063314    steps: 226    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1861   score: 3.0   memory length: 370419   epsilon: 0.4645684000106303    steps: 225    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1862   score: 3.0   memory length: 370645   epsilon: 0.4641209200106275    steps: 226    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1863   score: 4.0   memory length: 370942   epsilon: 0.46353286001062377    steps: 297    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1864   score: 1.0   memory length: 371114   epsilon: 0.4631923000106216    steps: 172    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1865   score: 6.0   memory length: 371468   epsilon: 0.4624913800106172    steps: 354    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1866   score: 4.0   memory length: 371764   epsilon: 0.4619053000106135    steps: 296    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1867   score: 4.0   memory length: 372039   epsilon: 0.46136080001061003    steps: 275    lr: 0.0001     evaluation reward: 3.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1868   score: 3.0   memory length: 372284   epsilon: 0.46087570001060696    steps: 245    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1869   score: 3.0   memory length: 372510   epsilon: 0.46042822001060413    steps: 226    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1870   score: 7.0   memory length: 372888   epsilon: 0.4596797800105994    steps: 378    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1871   score: 4.0   memory length: 373163   epsilon: 0.45913528001059595    steps: 275    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1872   score: 4.0   memory length: 373421   epsilon: 0.4586244400105927    steps: 258    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1873   score: 3.0   memory length: 373647   epsilon: 0.4581769600105899    steps: 226    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1874   score: 3.0   memory length: 373872   epsilon: 0.45773146001058707    steps: 225    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1875   score: 4.0   memory length: 374166   epsilon: 0.4571493400105834    steps: 294    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1876   score: 6.0   memory length: 374538   epsilon: 0.4564127800105787    steps: 372    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1877   score: 5.0   memory length: 374834   epsilon: 0.455826700010575    steps: 296    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1878   score: 6.0   memory length: 375198   epsilon: 0.45510598001057045    steps: 364    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1879   score: 4.0   memory length: 375464   epsilon: 0.4545793000105671    steps: 266    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1880   score: 3.0   memory length: 375708   epsilon: 0.45409618001056407    steps: 244    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1881   score: 3.0   memory length: 375934   epsilon: 0.45364870001056123    steps: 226    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1882   score: 4.0   memory length: 376209   epsilon: 0.4531042000105578    steps: 275    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1883   score: 2.0   memory length: 376407   epsilon: 0.4527121600105553    steps: 198    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1884   score: 5.0   memory length: 376716   epsilon: 0.45210034001055144    steps: 309    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1885   score: 5.0   memory length: 377041   epsilon: 0.45145684001054737    steps: 325    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1886   score: 3.0   memory length: 377266   epsilon: 0.45101134001054455    steps: 225    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1887   score: 3.0   memory length: 377491   epsilon: 0.45056584001054173    steps: 225    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1888   score: 1.0   memory length: 377663   epsilon: 0.4502252800105396    steps: 172    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1889   score: 6.0   memory length: 378007   epsilon: 0.44954416001053527    steps: 344    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1890   score: 4.0   memory length: 378304   epsilon: 0.44895610001053154    steps: 297    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1891   score: 3.0   memory length: 378529   epsilon: 0.4485106000105287    steps: 225    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1892   score: 3.0   memory length: 378755   epsilon: 0.4480631200105259    steps: 226    lr: 0.0001     evaluation reward: 3.32\n",
      "episode: 1893   score: 3.0   memory length: 378983   epsilon: 0.44761168001052304    steps: 228    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1894   score: 6.0   memory length: 379338   epsilon: 0.4469087800105186    steps: 355    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1895   score: 2.0   memory length: 379536   epsilon: 0.4465167400105161    steps: 198    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1896   score: 3.0   memory length: 379761   epsilon: 0.4460712400105133    steps: 225    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1897   score: 2.0   memory length: 379960   epsilon: 0.4456772200105108    steps: 199    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1898   score: 3.0   memory length: 380189   epsilon: 0.44522380001050793    steps: 229    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1899   score: 4.0   memory length: 380468   epsilon: 0.44467138001050444    steps: 279    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1900   score: 6.0   memory length: 380833   epsilon: 0.44394868001049986    steps: 365    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1901   score: 3.0   memory length: 381079   epsilon: 0.4434616000104968    steps: 246    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1902   score: 0.0   memory length: 381201   epsilon: 0.44322004001049525    steps: 122    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1903   score: 5.0   memory length: 381519   epsilon: 0.44259040001049127    steps: 318    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1904   score: 4.0   memory length: 381791   epsilon: 0.44205184001048786    steps: 272    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1905   score: 4.0   memory length: 382072   epsilon: 0.44149546001048434    steps: 281    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1906   score: 3.0   memory length: 382285   epsilon: 0.4410737200104817    steps: 213    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1907   score: 3.0   memory length: 382510   epsilon: 0.44062822001047885    steps: 225    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1908   score: 3.0   memory length: 382755   epsilon: 0.4401431200104758    steps: 245    lr: 0.0001     evaluation reward: 3.34\n",
      "episode: 1909   score: 5.0   memory length: 383099   epsilon: 0.4394620000104715    steps: 344    lr: 0.0001     evaluation reward: 3.37\n",
      "episode: 1910   score: 6.0   memory length: 383443   epsilon: 0.43878088001046717    steps: 344    lr: 0.0001     evaluation reward: 3.41\n",
      "episode: 1911   score: 3.0   memory length: 383669   epsilon: 0.43833340001046434    steps: 226    lr: 0.0001     evaluation reward: 3.42\n",
      "episode: 1912   score: 3.0   memory length: 383894   epsilon: 0.4378879000104615    steps: 225    lr: 0.0001     evaluation reward: 3.41\n",
      "episode: 1913   score: 5.0   memory length: 384213   epsilon: 0.4372562800104575    steps: 319    lr: 0.0001     evaluation reward: 3.41\n",
      "episode: 1914   score: 4.0   memory length: 384469   epsilon: 0.4367494000104543    steps: 256    lr: 0.0001     evaluation reward: 3.42\n",
      "episode: 1915   score: 4.0   memory length: 384727   epsilon: 0.4362385600104511    steps: 258    lr: 0.0001     evaluation reward: 3.43\n",
      "episode: 1916   score: 5.0   memory length: 385036   epsilon: 0.4356267400104472    steps: 309    lr: 0.0001     evaluation reward: 3.45\n",
      "episode: 1917   score: 2.0   memory length: 385233   epsilon: 0.43523668001044474    steps: 197    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1918   score: 3.0   memory length: 385459   epsilon: 0.4347892000104419    steps: 226    lr: 0.0001     evaluation reward: 3.43\n",
      "episode: 1919   score: 5.0   memory length: 385764   epsilon: 0.4341853000104381    steps: 305    lr: 0.0001     evaluation reward: 3.45\n",
      "episode: 1920   score: 6.0   memory length: 386107   epsilon: 0.4335061600104338    steps: 343    lr: 0.0001     evaluation reward: 3.48\n",
      "episode: 1921   score: 4.0   memory length: 386373   epsilon: 0.43297948001043046    steps: 266    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1922   score: 4.0   memory length: 386648   epsilon: 0.432434980010427    steps: 275    lr: 0.0001     evaluation reward: 3.53\n",
      "episode: 1923   score: 5.0   memory length: 386972   epsilon: 0.43179346001042296    steps: 324    lr: 0.0001     evaluation reward: 3.56\n",
      "episode: 1924   score: 3.0   memory length: 387221   epsilon: 0.43130044001041984    steps: 249    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1925   score: 4.0   memory length: 387498   epsilon: 0.43075198001041637    steps: 277    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1926   score: 3.0   memory length: 387724   epsilon: 0.43030450001041354    steps: 226    lr: 0.0001     evaluation reward: 3.54\n",
      "episode: 1927   score: 9.0   memory length: 388204   epsilon: 0.4293541000104075    steps: 480    lr: 0.0001     evaluation reward: 3.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1928   score: 2.0   memory length: 388401   epsilon: 0.42896404001040506    steps: 197    lr: 0.0001     evaluation reward: 3.59\n",
      "episode: 1929   score: 5.0   memory length: 388722   epsilon: 0.42832846001040104    steps: 321    lr: 0.0001     evaluation reward: 3.61\n",
      "episode: 1930   score: 4.0   memory length: 388997   epsilon: 0.4277839600103976    steps: 275    lr: 0.0001     evaluation reward: 3.62\n",
      "episode: 1931   score: 5.0   memory length: 389303   epsilon: 0.42717808001039376    steps: 306    lr: 0.0001     evaluation reward: 3.64\n",
      "episode: 1932   score: 6.0   memory length: 389666   epsilon: 0.4264593400103892    steps: 363    lr: 0.0001     evaluation reward: 3.67\n",
      "episode: 1933   score: 5.0   memory length: 389962   epsilon: 0.4258732600103855    steps: 296    lr: 0.0001     evaluation reward: 3.71\n",
      "episode: 1934   score: 3.0   memory length: 390187   epsilon: 0.4254277600103827    steps: 225    lr: 0.0001     evaluation reward: 3.72\n",
      "episode: 1935   score: 3.0   memory length: 390413   epsilon: 0.42498028001037985    steps: 226    lr: 0.0001     evaluation reward: 3.72\n",
      "episode: 1936   score: 3.0   memory length: 390639   epsilon: 0.424532800010377    steps: 226    lr: 0.0001     evaluation reward: 3.72\n",
      "episode: 1937   score: 3.0   memory length: 390865   epsilon: 0.4240853200103742    steps: 226    lr: 0.0001     evaluation reward: 3.75\n",
      "episode: 1938   score: 4.0   memory length: 391160   epsilon: 0.4235012200103705    steps: 295    lr: 0.0001     evaluation reward: 3.76\n",
      "episode: 1939   score: 4.0   memory length: 391435   epsilon: 0.42295672001036705    steps: 275    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1940   score: 5.0   memory length: 391731   epsilon: 0.42237064001036334    steps: 296    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1941   score: 3.0   memory length: 391981   epsilon: 0.4218756400103602    steps: 250    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1942   score: 3.0   memory length: 392207   epsilon: 0.4214281600103574    steps: 226    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1943   score: 5.0   memory length: 392531   epsilon: 0.4207866400103533    steps: 324    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1944   score: 3.0   memory length: 392778   epsilon: 0.4202975800103502    steps: 247    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1945   score: 4.0   memory length: 393054   epsilon: 0.41975110001034677    steps: 276    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1946   score: 3.0   memory length: 393267   epsilon: 0.4193293600103441    steps: 213    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1947   score: 1.0   memory length: 393417   epsilon: 0.4190323600103422    steps: 150    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1948   score: 6.0   memory length: 393759   epsilon: 0.41835520001033794    steps: 342    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1949   score: 2.0   memory length: 393939   epsilon: 0.4179988000103357    steps: 180    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1950   score: 2.0   memory length: 394156   epsilon: 0.41756914001033296    steps: 217    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1951   score: 5.0   memory length: 394457   epsilon: 0.4169731600103292    steps: 301    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1952   score: 3.0   memory length: 394685   epsilon: 0.41652172001032634    steps: 228    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1953   score: 3.0   memory length: 394910   epsilon: 0.4160762200103235    steps: 225    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1954   score: 2.0   memory length: 395090   epsilon: 0.41571982001032126    steps: 180    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1955   score: 3.0   memory length: 395316   epsilon: 0.41527234001031843    steps: 226    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1956   score: 3.0   memory length: 395547   epsilon: 0.41481496001031554    steps: 231    lr: 0.0001     evaluation reward: 3.76\n",
      "episode: 1957   score: 4.0   memory length: 395864   epsilon: 0.41418730001031157    steps: 317    lr: 0.0001     evaluation reward: 3.76\n",
      "episode: 1958   score: 2.0   memory length: 396062   epsilon: 0.4137952600103091    steps: 198    lr: 0.0001     evaluation reward: 3.75\n",
      "episode: 1959   score: 4.0   memory length: 396328   epsilon: 0.41326858001030575    steps: 266    lr: 0.0001     evaluation reward: 3.75\n",
      "episode: 1960   score: 4.0   memory length: 396605   epsilon: 0.4127201200103023    steps: 277    lr: 0.0001     evaluation reward: 3.76\n",
      "episode: 1961   score: 4.0   memory length: 396880   epsilon: 0.41217562001029884    steps: 275    lr: 0.0001     evaluation reward: 3.77\n",
      "episode: 1962   score: 4.0   memory length: 397177   epsilon: 0.4115875600102951    steps: 297    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1963   score: 3.0   memory length: 397402   epsilon: 0.4111420600102923    steps: 225    lr: 0.0001     evaluation reward: 3.77\n",
      "episode: 1964   score: 6.0   memory length: 397744   epsilon: 0.410464900010288    steps: 342    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1965   score: 5.0   memory length: 398069   epsilon: 0.40982140001028394    steps: 325    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1966   score: 4.0   memory length: 398344   epsilon: 0.4092769000102805    steps: 275    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1967   score: 4.0   memory length: 398661   epsilon: 0.4086492400102765    steps: 317    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1968   score: 4.0   memory length: 398935   epsilon: 0.4081067200102731    steps: 274    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1969   score: 4.0   memory length: 399191   epsilon: 0.4075998400102699    steps: 256    lr: 0.0001     evaluation reward: 3.83\n",
      "episode: 1970   score: 3.0   memory length: 399416   epsilon: 0.40715434001026707    steps: 225    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1971   score: 4.0   memory length: 399690   epsilon: 0.40661182001026364    steps: 274    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1972   score: 3.0   memory length: 399903   epsilon: 0.40619008001026097    steps: 213    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1973   score: 3.0   memory length: 400129   epsilon: 0.40574260001025814    steps: 226    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1974   score: 3.0   memory length: 400355   epsilon: 0.4052951200102553    steps: 226    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1975   score: 4.0   memory length: 400613   epsilon: 0.4047842800102521    steps: 258    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1976   score: 7.0   memory length: 400989   epsilon: 0.40403980001024736    steps: 376    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1977   score: 4.0   memory length: 401247   epsilon: 0.40352896001024413    steps: 258    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1978   score: 7.0   memory length: 401601   epsilon: 0.4028280400102397    steps: 354    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1979   score: 3.0   memory length: 401826   epsilon: 0.4023825400102369    steps: 225    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1980   score: 3.0   memory length: 402038   epsilon: 0.4019627800102342    steps: 212    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1981   score: 3.0   memory length: 402264   epsilon: 0.4015153000102314    steps: 226    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1982   score: 6.0   memory length: 402607   epsilon: 0.4008361600102271    steps: 343    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1983   score: 5.0   memory length: 402930   epsilon: 0.40019662001022305    steps: 323    lr: 0.0001     evaluation reward: 3.83\n",
      "episode: 1984   score: 5.0   memory length: 403236   epsilon: 0.3995907400102192    steps: 306    lr: 0.0001     evaluation reward: 3.83\n",
      "episode: 1985   score: 4.0   memory length: 403528   epsilon: 0.39901258001021556    steps: 292    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1986   score: 3.0   memory length: 403754   epsilon: 0.3985651000102127    steps: 226    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1987   score: 6.0   memory length: 404105   epsilon: 0.39787012001020833    steps: 351    lr: 0.0001     evaluation reward: 3.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1988   score: 3.0   memory length: 404331   epsilon: 0.3974226400102055    steps: 226    lr: 0.0001     evaluation reward: 3.87\n",
      "episode: 1989   score: 2.0   memory length: 404513   epsilon: 0.3970622800102032    steps: 182    lr: 0.0001     evaluation reward: 3.83\n",
      "episode: 1990   score: 7.0   memory length: 404892   epsilon: 0.39631186001019847    steps: 379    lr: 0.0001     evaluation reward: 3.86\n",
      "episode: 1991   score: 5.0   memory length: 405217   epsilon: 0.3956683600101944    steps: 325    lr: 0.0001     evaluation reward: 3.88\n",
      "episode: 1992   score: 4.0   memory length: 405513   epsilon: 0.3950822800101907    steps: 296    lr: 0.0001     evaluation reward: 3.89\n",
      "episode: 1993   score: 5.0   memory length: 405809   epsilon: 0.394496200010187    steps: 296    lr: 0.0001     evaluation reward: 3.91\n",
      "episode: 1994   score: 7.0   memory length: 406217   epsilon: 0.39368836001018187    steps: 408    lr: 0.0001     evaluation reward: 3.92\n",
      "episode: 1995   score: 3.0   memory length: 406443   epsilon: 0.39324088001017904    steps: 226    lr: 0.0001     evaluation reward: 3.93\n",
      "episode: 1996   score: 3.0   memory length: 406653   epsilon: 0.3928250800101764    steps: 210    lr: 0.0001     evaluation reward: 3.93\n",
      "episode: 1997   score: 6.0   memory length: 407047   epsilon: 0.3920449600101715    steps: 394    lr: 0.0001     evaluation reward: 3.97\n",
      "episode: 1998   score: 3.0   memory length: 407272   epsilon: 0.39159946001016865    steps: 225    lr: 0.0001     evaluation reward: 3.97\n",
      "episode: 1999   score: 3.0   memory length: 407484   epsilon: 0.391179700010166    steps: 212    lr: 0.0001     evaluation reward: 3.96\n",
      "episode: 2000   score: 4.0   memory length: 407760   epsilon: 0.39063322001016254    steps: 276    lr: 0.0001     evaluation reward: 3.94\n",
      "episode: 2001   score: 4.0   memory length: 408003   epsilon: 0.3901520800101595    steps: 243    lr: 0.0001     evaluation reward: 3.95\n",
      "episode: 2002   score: 3.0   memory length: 408229   epsilon: 0.38970460001015667    steps: 226    lr: 0.0001     evaluation reward: 3.98\n",
      "episode: 2003   score: 10.0   memory length: 408747   epsilon: 0.3886789600101502    steps: 518    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2004   score: 2.0   memory length: 408929   epsilon: 0.3883186000101479    steps: 182    lr: 0.0001     evaluation reward: 4.01\n",
      "episode: 2005   score: 5.0   memory length: 409209   epsilon: 0.3877642000101444    steps: 280    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 2006   score: 4.0   memory length: 409475   epsilon: 0.38723752001014106    steps: 266    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2007   score: 3.0   memory length: 409700   epsilon: 0.38679202001013824    steps: 225    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2008   score: 3.0   memory length: 409926   epsilon: 0.3863445400101354    steps: 226    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2009   score: 5.0   memory length: 410221   epsilon: 0.3857604400101317    steps: 295    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2010   score: 5.0   memory length: 410557   epsilon: 0.3850951600101275    steps: 336    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 2011   score: 2.0   memory length: 410774   epsilon: 0.3846655000101248    steps: 217    lr: 0.0001     evaluation reward: 4.01\n",
      "episode: 2012   score: 5.0   memory length: 411099   epsilon: 0.3840220000101207    steps: 325    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2013   score: 2.0   memory length: 411279   epsilon: 0.38366560001011846    steps: 180    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 2014   score: 6.0   memory length: 411596   epsilon: 0.3830379400101145    steps: 317    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 2015   score: 4.0   memory length: 411877   epsilon: 0.38248156001011097    steps: 281    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 2016   score: 3.0   memory length: 412125   epsilon: 0.38199052001010786    steps: 248    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 2017   score: 4.0   memory length: 412401   epsilon: 0.3814440400101044    steps: 276    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 2018   score: 4.0   memory length: 412655   epsilon: 0.3809411200101012    steps: 254    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2019   score: 5.0   memory length: 413000   epsilon: 0.3802580200100969    steps: 345    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2020   score: 4.0   memory length: 413278   epsilon: 0.3797075800100934    steps: 278    lr: 0.0001     evaluation reward: 4.01\n",
      "episode: 2021   score: 3.0   memory length: 413527   epsilon: 0.3792145600100903    steps: 249    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 2022   score: 5.0   memory length: 413835   epsilon: 0.37860472001008644    steps: 308    lr: 0.0001     evaluation reward: 4.01\n",
      "episode: 2023   score: 6.0   memory length: 414190   epsilon: 0.377901820010082    steps: 355    lr: 0.0001     evaluation reward: 4.02\n",
      "episode: 2024   score: 4.0   memory length: 414466   epsilon: 0.37735534001007853    steps: 276    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 2025   score: 8.0   memory length: 414890   epsilon: 0.3765158200100732    steps: 424    lr: 0.0001     evaluation reward: 4.07\n",
      "episode: 2026   score: 6.0   memory length: 415232   epsilon: 0.37583866001006894    steps: 342    lr: 0.0001     evaluation reward: 4.1\n",
      "episode: 2027   score: 4.0   memory length: 415489   epsilon: 0.3753298000100657    steps: 257    lr: 0.0001     evaluation reward: 4.05\n",
      "episode: 2028   score: 5.0   memory length: 415813   epsilon: 0.37468828001006166    steps: 324    lr: 0.0001     evaluation reward: 4.08\n",
      "episode: 2029   score: 6.0   memory length: 416155   epsilon: 0.3740111200100574    steps: 342    lr: 0.0001     evaluation reward: 4.09\n",
      "episode: 2030   score: 4.0   memory length: 416447   epsilon: 0.3734329600100537    steps: 292    lr: 0.0001     evaluation reward: 4.09\n",
      "episode: 2031   score: 5.0   memory length: 416752   epsilon: 0.3728290600100499    steps: 305    lr: 0.0001     evaluation reward: 4.09\n",
      "episode: 2032   score: 9.0   memory length: 417227   epsilon: 0.37188856001004394    steps: 475    lr: 0.0001     evaluation reward: 4.12\n",
      "episode: 2033   score: 4.0   memory length: 417502   epsilon: 0.3713440600100405    steps: 275    lr: 0.0001     evaluation reward: 4.11\n",
      "episode: 2034   score: 2.0   memory length: 417700   epsilon: 0.370952020010038    steps: 198    lr: 0.0001     evaluation reward: 4.1\n",
      "episode: 2035   score: 4.0   memory length: 417959   epsilon: 0.3704392000100348    steps: 259    lr: 0.0001     evaluation reward: 4.11\n",
      "episode: 2036   score: 5.0   memory length: 418286   epsilon: 0.3697917400100307    steps: 327    lr: 0.0001     evaluation reward: 4.13\n",
      "episode: 2037   score: 6.0   memory length: 418655   epsilon: 0.36906112001002606    steps: 369    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 2038   score: 4.0   memory length: 418930   epsilon: 0.3685166200100226    steps: 275    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 2039   score: 3.0   memory length: 419161   epsilon: 0.3680592400100197    steps: 231    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 2040   score: 4.0   memory length: 419416   epsilon: 0.3675543400100165    steps: 255    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 2041   score: 3.0   memory length: 419626   epsilon: 0.3671385400100139    steps: 210    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 2042   score: 3.0   memory length: 419852   epsilon: 0.36669106001001106    steps: 226    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 2043   score: 7.0   memory length: 420234   epsilon: 0.3659347000100063    steps: 382    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 2044   score: 1.0   memory length: 420385   epsilon: 0.3656357200100044    steps: 151    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 2045   score: 3.0   memory length: 420610   epsilon: 0.36519022001000156    steps: 225    lr: 0.0001     evaluation reward: 4.13\n",
      "episode: 2046   score: 5.0   memory length: 420918   epsilon: 0.3645803800099977    steps: 308    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 2047   score: 3.0   memory length: 421146   epsilon: 0.36412894000999485    steps: 228    lr: 0.0001     evaluation reward: 4.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2048   score: 3.0   memory length: 421371   epsilon: 0.36368344000999203    steps: 225    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 2049   score: 3.0   memory length: 421596   epsilon: 0.3632379400099892    steps: 225    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 2050   score: 3.0   memory length: 421808   epsilon: 0.36281818000998656    steps: 212    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 2051   score: 3.0   memory length: 422034   epsilon: 0.3623707000099837    steps: 226    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 2052   score: 4.0   memory length: 422327   epsilon: 0.36179056000998006    steps: 293    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 2053   score: 6.0   memory length: 422690   epsilon: 0.3610718200099755    steps: 363    lr: 0.0001     evaluation reward: 4.18\n",
      "episode: 2054   score: 6.0   memory length: 423033   epsilon: 0.3603926800099712    steps: 343    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 2055   score: 3.0   memory length: 423259   epsilon: 0.3599452000099684    steps: 226    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 2056   score: 4.0   memory length: 423533   epsilon: 0.35940268000996495    steps: 274    lr: 0.0001     evaluation reward: 4.23\n",
      "episode: 2057   score: 3.0   memory length: 423746   epsilon: 0.3589809400099623    steps: 213    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 2058   score: 6.0   memory length: 424099   epsilon: 0.35828200000995786    steps: 353    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 2059   score: 5.0   memory length: 424407   epsilon: 0.357672160009954    steps: 308    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2060   score: 4.0   memory length: 424683   epsilon: 0.35712568000995054    steps: 276    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2061   score: 6.0   memory length: 425054   epsilon: 0.3563911000099459    steps: 371    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2062   score: 4.0   memory length: 425349   epsilon: 0.3558070000099422    steps: 295    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2063   score: 4.0   memory length: 425646   epsilon: 0.3552189400099385    steps: 297    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 2064   score: 6.0   memory length: 425989   epsilon: 0.3545398000099342    steps: 343    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 2065   score: 3.0   memory length: 426240   epsilon: 0.35404282000993104    steps: 251    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 2066   score: 3.0   memory length: 426467   epsilon: 0.3535933600099282    steps: 227    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2067   score: 1.0   memory length: 426617   epsilon: 0.3532963600099263    steps: 150    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 2068   score: 6.0   memory length: 426945   epsilon: 0.3526469200099222    steps: 328    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 2069   score: 4.0   memory length: 427220   epsilon: 0.35210242000991876    steps: 275    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 2070   score: 4.0   memory length: 427514   epsilon: 0.3515203000099151    steps: 294    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2071   score: 3.0   memory length: 427740   epsilon: 0.35107282000991225    steps: 226    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 2072   score: 5.0   memory length: 428043   epsilon: 0.35047288000990845    steps: 303    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 2073   score: 4.0   memory length: 428335   epsilon: 0.3498947200099048    steps: 292    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2074   score: 3.0   memory length: 428547   epsilon: 0.34947496000990214    steps: 212    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2075   score: 4.0   memory length: 428822   epsilon: 0.3489304600098987    steps: 275    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2076   score: 3.0   memory length: 429048   epsilon: 0.34848298000989586    steps: 226    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 2077   score: 4.0   memory length: 429288   epsilon: 0.34800778000989285    steps: 240    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 2078   score: 4.0   memory length: 429564   epsilon: 0.3474613000098894    steps: 276    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 2079   score: 3.0   memory length: 429778   epsilon: 0.3470375800098867    steps: 214    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 2080   score: 8.0   memory length: 430225   epsilon: 0.3461525200098811    steps: 447    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2081   score: 4.0   memory length: 430483   epsilon: 0.3456416800098779    steps: 258    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 2082   score: 5.0   memory length: 430808   epsilon: 0.3449981800098738    steps: 325    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2083   score: 4.0   memory length: 431073   epsilon: 0.3444734800098705    steps: 265    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 2084   score: 6.0   memory length: 431437   epsilon: 0.34375276000986593    steps: 364    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2085   score: 3.0   memory length: 431650   epsilon: 0.34333102000986326    steps: 213    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 2086   score: 6.0   memory length: 432022   epsilon: 0.3425944600098586    steps: 372    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2087   score: 4.0   memory length: 432298   epsilon: 0.34204798000985515    steps: 276    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 2088   score: 5.0   memory length: 432605   epsilon: 0.3414401200098513    steps: 307    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 2089   score: 3.0   memory length: 432818   epsilon: 0.34101838000984863    steps: 213    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 2090   score: 12.0   memory length: 433273   epsilon: 0.34011748000984293    steps: 455    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 2091   score: 3.0   memory length: 433523   epsilon: 0.3396224800098398    steps: 250    lr: 0.0001     evaluation reward: 4.33\n",
      "episode: 2092   score: 7.0   memory length: 433930   epsilon: 0.3388166200098347    steps: 407    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2093   score: 3.0   memory length: 434159   epsilon: 0.33836320000983183    steps: 229    lr: 0.0001     evaluation reward: 4.34\n",
      "episode: 2094   score: 6.0   memory length: 434529   epsilon: 0.3376306000098272    steps: 370    lr: 0.0001     evaluation reward: 4.33\n",
      "episode: 2095   score: 3.0   memory length: 434740   epsilon: 0.33721282000982455    steps: 211    lr: 0.0001     evaluation reward: 4.33\n",
      "episode: 2096   score: 5.0   memory length: 435064   epsilon: 0.3365713000098205    steps: 324    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 2097   score: 5.0   memory length: 435403   epsilon: 0.33590008000981625    steps: 339    lr: 0.0001     evaluation reward: 4.34\n",
      "episode: 2098   score: 4.0   memory length: 435680   epsilon: 0.3353516200098128    steps: 277    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 2099   score: 5.0   memory length: 435987   epsilon: 0.33474376000980893    steps: 307    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2100   score: 6.0   memory length: 436340   epsilon: 0.3340448200098045    steps: 353    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 2101   score: 4.0   memory length: 436636   epsilon: 0.3334587400098008    steps: 296    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 2102   score: 5.0   memory length: 436943   epsilon: 0.33285088000979696    steps: 307    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2103   score: 5.0   memory length: 437250   epsilon: 0.3322430200097931    steps: 307    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2104   score: 3.0   memory length: 437481   epsilon: 0.3317856400097902    steps: 231    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2105   score: 4.0   memory length: 437777   epsilon: 0.3311995600097865    steps: 296    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2106   score: 3.0   memory length: 438003   epsilon: 0.3307520800097837    steps: 226    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 2107   score: 7.0   memory length: 438397   epsilon: 0.32997196000977874    steps: 394    lr: 0.0001     evaluation reward: 4.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2108   score: 4.0   memory length: 438674   epsilon: 0.32942350000977527    steps: 277    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 2109   score: 4.0   memory length: 438932   epsilon: 0.32891266000977204    steps: 258    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 2110   score: 3.0   memory length: 439144   epsilon: 0.3284929000097694    steps: 212    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2111   score: 5.0   memory length: 439466   epsilon: 0.32785534000976535    steps: 322    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 2112   score: 2.0   memory length: 439667   epsilon: 0.32745736000976283    steps: 201    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2113   score: 3.0   memory length: 439898   epsilon: 0.32699998000975994    steps: 231    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2114   score: 3.0   memory length: 440128   epsilon: 0.32654458000975706    steps: 230    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 2115   score: 7.0   memory length: 440550   epsilon: 0.32570902000975177    steps: 422    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2116   score: 6.0   memory length: 440893   epsilon: 0.3250298800097475    steps: 343    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2117   score: 4.0   memory length: 441153   epsilon: 0.3245150800097442    steps: 260    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2118   score: 5.0   memory length: 441443   epsilon: 0.3239408800097406    steps: 290    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 2119   score: 7.0   memory length: 441817   epsilon: 0.3232003600097359    steps: 374    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 2120   score: 3.0   memory length: 442042   epsilon: 0.3227548600097331    steps: 225    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 2121   score: 9.0   memory length: 442494   epsilon: 0.3218599000097274    steps: 452    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 2122   score: 4.0   memory length: 442768   epsilon: 0.321317380009724    steps: 274    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 2123   score: 3.0   memory length: 443014   epsilon: 0.3208303000097209    steps: 246    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2124   score: 4.0   memory length: 443290   epsilon: 0.32028382000971745    steps: 276    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2125   score: 5.0   memory length: 443633   epsilon: 0.31960468000971315    steps: 343    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 2126   score: 4.0   memory length: 443931   epsilon: 0.3190146400097094    steps: 298    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 2127   score: 5.0   memory length: 444276   epsilon: 0.3183315400097051    steps: 345    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2128   score: 3.0   memory length: 444489   epsilon: 0.3179098000097024    steps: 213    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 2129   score: 3.0   memory length: 444717   epsilon: 0.31745836000969957    steps: 228    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2130   score: 6.0   memory length: 445035   epsilon: 0.3168287200096956    steps: 318    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2131   score: 4.0   memory length: 445328   epsilon: 0.3162485800096919    steps: 293    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2132   score: 8.0   memory length: 445798   epsilon: 0.315317980009686    steps: 470    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2133   score: 5.0   memory length: 446105   epsilon: 0.3147101200096822    steps: 307    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2134   score: 6.0   memory length: 446453   epsilon: 0.3140210800096778    steps: 348    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2135   score: 4.0   memory length: 446727   epsilon: 0.3134785600096744    steps: 274    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2136   score: 4.0   memory length: 447001   epsilon: 0.31293604000967096    steps: 274    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 2137   score: 4.0   memory length: 447242   epsilon: 0.31245886000966794    steps: 241    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2138   score: 4.0   memory length: 447517   epsilon: 0.3119143600096645    steps: 275    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2139   score: 3.0   memory length: 447729   epsilon: 0.31149460000966184    steps: 212    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2140   score: 3.0   memory length: 447960   epsilon: 0.31103722000965894    steps: 231    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 2141   score: 4.0   memory length: 448255   epsilon: 0.31045312000965525    steps: 295    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2142   score: 3.0   memory length: 448468   epsilon: 0.3100313800096526    steps: 213    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2143   score: 5.0   memory length: 448760   epsilon: 0.3094532200096489    steps: 292    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2144   score: 3.0   memory length: 448973   epsilon: 0.30903148000964625    steps: 213    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2145   score: 3.0   memory length: 449204   epsilon: 0.30857410000964336    steps: 231    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 2146   score: 3.0   memory length: 449416   epsilon: 0.3081543400096407    steps: 212    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 2147   score: 6.0   memory length: 449761   epsilon: 0.3074712400096364    steps: 345    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 2148   score: 7.0   memory length: 450181   epsilon: 0.3066396400096311    steps: 420    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 2149   score: 5.0   memory length: 450506   epsilon: 0.30599614000962705    steps: 325    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2150   score: 3.0   memory length: 450733   epsilon: 0.3055466800096242    steps: 227    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2151   score: 3.0   memory length: 450945   epsilon: 0.30512692000962155    steps: 212    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2152   score: 4.0   memory length: 451202   epsilon: 0.30461806000961833    steps: 257    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2153   score: 4.0   memory length: 451443   epsilon: 0.3041408800096153    steps: 241    lr: 0.0001     evaluation reward: 4.43\n",
      "episode: 2154   score: 5.0   memory length: 451754   epsilon: 0.3035251000096114    steps: 311    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 2155   score: 6.0   memory length: 452130   epsilon: 0.3027806200096067    steps: 376    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2156   score: 5.0   memory length: 452456   epsilon: 0.3021351400096026    steps: 326    lr: 0.0001     evaluation reward: 4.46\n",
      "episode: 2157   score: 4.0   memory length: 452737   epsilon: 0.3015787600095991    steps: 281    lr: 0.0001     evaluation reward: 4.47\n",
      "episode: 2158   score: 3.0   memory length: 452968   epsilon: 0.3011213800095962    steps: 231    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 2159   score: 6.0   memory length: 453332   epsilon: 0.30040066000959165    steps: 364    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2160   score: 3.0   memory length: 453559   epsilon: 0.2999512000095888    steps: 227    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 2161   score: 4.0   memory length: 453839   epsilon: 0.2993968000095853    steps: 280    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 2162   score: 4.0   memory length: 454101   epsilon: 0.298878040009582    steps: 262    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 2163   score: 3.0   memory length: 454327   epsilon: 0.2984305600095792    steps: 226    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2164   score: 5.0   memory length: 454630   epsilon: 0.2978306200095754    steps: 303    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 2165   score: 4.0   memory length: 454943   epsilon: 0.29721088000957147    steps: 313    lr: 0.0001     evaluation reward: 4.41\n",
      "episode: 2166   score: 6.0   memory length: 455299   epsilon: 0.296506000009567    steps: 356    lr: 0.0001     evaluation reward: 4.44\n",
      "episode: 2167   score: 3.0   memory length: 455546   epsilon: 0.2960169400095639    steps: 247    lr: 0.0001     evaluation reward: 4.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2168   score: 6.0   memory length: 455881   epsilon: 0.2953536400095597    steps: 335    lr: 0.0001     evaluation reward: 4.46\n",
      "episode: 2169   score: 3.0   memory length: 456110   epsilon: 0.29490022000955685    steps: 229    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 2170   score: 5.0   memory length: 456417   epsilon: 0.294292360009553    steps: 307    lr: 0.0001     evaluation reward: 4.46\n",
      "episode: 2171   score: 5.0   memory length: 456725   epsilon: 0.29368252000954914    steps: 308    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 2172   score: 3.0   memory length: 456937   epsilon: 0.2932627600095465    steps: 212    lr: 0.0001     evaluation reward: 4.46\n",
      "episode: 2173   score: 6.0   memory length: 457291   epsilon: 0.29256184000954205    steps: 354    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 2174   score: 4.0   memory length: 457548   epsilon: 0.29205298000953883    steps: 257    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 2175   score: 4.0   memory length: 457823   epsilon: 0.2915084800095354    steps: 275    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 2176   score: 10.0   memory length: 458340   epsilon: 0.2904848200095289    steps: 517    lr: 0.0001     evaluation reward: 4.56\n",
      "episode: 2177   score: 5.0   memory length: 458664   epsilon: 0.28984330000952485    steps: 324    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 2178   score: 7.0   memory length: 459070   epsilon: 0.28903942000951977    steps: 406    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 2179   score: 3.0   memory length: 459283   epsilon: 0.2886176800095171    steps: 213    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 2180   score: 6.0   memory length: 459618   epsilon: 0.2879543800095129    steps: 335    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 2181   score: 4.0   memory length: 459878   epsilon: 0.28743958000950964    steps: 260    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 2182   score: 5.0   memory length: 460187   epsilon: 0.28682776000950577    steps: 309    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 2183   score: 6.0   memory length: 460504   epsilon: 0.2862001000095018    steps: 317    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 2184   score: 5.0   memory length: 460831   epsilon: 0.2855526400094977    steps: 327    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2185   score: 5.0   memory length: 461140   epsilon: 0.28494082000949383    steps: 309    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2186   score: 3.0   memory length: 461368   epsilon: 0.284489380009491    steps: 228    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 2187   score: 7.0   memory length: 461762   epsilon: 0.28370926000948604    steps: 394    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2188   score: 4.0   memory length: 462007   epsilon: 0.283224160009483    steps: 245    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 2189   score: 7.0   memory length: 462399   epsilon: 0.28244800000947806    steps: 392    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2190   score: 8.0   memory length: 462779   epsilon: 0.2816956000094733    steps: 380    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 2191   score: 4.0   memory length: 463058   epsilon: 0.2811431800094698    steps: 279    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2192   score: 4.0   memory length: 463333   epsilon: 0.28059868000946636    steps: 275    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 2193   score: 6.0   memory length: 463676   epsilon: 0.27991954000946206    steps: 343    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2194   score: 4.0   memory length: 463953   epsilon: 0.2793710800094586    steps: 277    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2195   score: 8.0   memory length: 464330   epsilon: 0.27862462000945387    steps: 377    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2196   score: 6.0   memory length: 464660   epsilon: 0.27797122000944974    steps: 330    lr: 0.0001     evaluation reward: 4.65\n",
      "episode: 2197   score: 3.0   memory length: 464873   epsilon: 0.27754948000944707    steps: 213    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 2198   score: 5.0   memory length: 465179   epsilon: 0.27694360000944324    steps: 306    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2199   score: 6.0   memory length: 465538   epsilon: 0.27623278000943874    steps: 359    lr: 0.0001     evaluation reward: 4.65\n",
      "episode: 2200   score: 3.0   memory length: 465763   epsilon: 0.2757872800094359    steps: 225    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2201   score: 4.0   memory length: 466038   epsilon: 0.2752427800094325    steps: 275    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2202   score: 4.0   memory length: 466331   epsilon: 0.2746626400094288    steps: 293    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2203   score: 3.0   memory length: 466543   epsilon: 0.27424288000942615    steps: 212    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2204   score: 6.0   memory length: 466885   epsilon: 0.27356572000942186    steps: 342    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2205   score: 4.0   memory length: 467161   epsilon: 0.2730192400094184    steps: 276    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2206   score: 8.0   memory length: 467546   epsilon: 0.2722569400094136    steps: 385    lr: 0.0001     evaluation reward: 4.67\n",
      "episode: 2207   score: 5.0   memory length: 467891   epsilon: 0.27157384000940926    steps: 345    lr: 0.0001     evaluation reward: 4.65\n",
      "episode: 2208   score: 3.0   memory length: 468104   epsilon: 0.2711521000094066    steps: 213    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2209   score: 4.0   memory length: 468365   epsilon: 0.2706353200094033    steps: 261    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2210   score: 3.0   memory length: 468574   epsilon: 0.2702215000094007    steps: 209    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2211   score: 7.0   memory length: 468949   epsilon: 0.269479000009396    steps: 375    lr: 0.0001     evaluation reward: 4.66\n",
      "episode: 2212   score: 5.0   memory length: 469255   epsilon: 0.2688731200093922    steps: 306    lr: 0.0001     evaluation reward: 4.69\n",
      "episode: 2213   score: 5.0   memory length: 469527   epsilon: 0.26833456000938877    steps: 272    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2214   score: 3.0   memory length: 469737   epsilon: 0.26791876000938614    steps: 210    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2215   score: 8.0   memory length: 470154   epsilon: 0.2670931000093809    steps: 417    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2216   score: 3.0   memory length: 470381   epsilon: 0.26664364000937807    steps: 227    lr: 0.0001     evaluation reward: 4.69\n",
      "episode: 2217   score: 4.0   memory length: 470656   epsilon: 0.2660991400093746    steps: 275    lr: 0.0001     evaluation reward: 4.69\n",
      "episode: 2218   score: 4.0   memory length: 470915   epsilon: 0.2655863200093714    steps: 259    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 2219   score: 3.0   memory length: 471128   epsilon: 0.2651645800093687    steps: 213    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2220   score: 6.0   memory length: 471486   epsilon: 0.2644557400093642    steps: 358    lr: 0.0001     evaluation reward: 4.67\n",
      "episode: 2221   score: 4.0   memory length: 471730   epsilon: 0.26397262000936117    steps: 244    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2222   score: 3.0   memory length: 471941   epsilon: 0.2635548400093585    steps: 211    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2223   score: 3.0   memory length: 472169   epsilon: 0.26310340000935567    steps: 228    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2224   score: 4.0   memory length: 472427   epsilon: 0.26259256000935244    steps: 258    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2225   score: 5.0   memory length: 472769   epsilon: 0.26191540000934815    steps: 342    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2226   score: 4.0   memory length: 473028   epsilon: 0.2614025800093449    steps: 259    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2227   score: 3.0   memory length: 473256   epsilon: 0.26095114000934205    steps: 228    lr: 0.0001     evaluation reward: 4.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2228   score: 7.0   memory length: 473674   epsilon: 0.2601235000093368    steps: 418    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 2229   score: 3.0   memory length: 473904   epsilon: 0.25966810000933394    steps: 230    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 2230   score: 3.0   memory length: 474115   epsilon: 0.2592503200093313    steps: 211    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 2231   score: 6.0   memory length: 474509   epsilon: 0.25847020000932636    steps: 394    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2232   score: 5.0   memory length: 474815   epsilon: 0.2578643200093225    steps: 306    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2233   score: 5.0   memory length: 475161   epsilon: 0.2571792400093182    steps: 346    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2234   score: 3.0   memory length: 475392   epsilon: 0.2567218600093153    steps: 231    lr: 0.0001     evaluation reward: 4.56\n",
      "episode: 2235   score: 5.0   memory length: 475665   epsilon: 0.2561813200093119    steps: 273    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 2236   score: 3.0   memory length: 475878   epsilon: 0.2557595800093092    steps: 213    lr: 0.0001     evaluation reward: 4.56\n",
      "episode: 2237   score: 3.0   memory length: 476109   epsilon: 0.2553022000093063    steps: 231    lr: 0.0001     evaluation reward: 4.55\n",
      "episode: 2238   score: 3.0   memory length: 476322   epsilon: 0.25488046000930364    steps: 213    lr: 0.0001     evaluation reward: 4.54\n",
      "episode: 2239   score: 8.0   memory length: 476765   epsilon: 0.2540033200092981    steps: 443    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2240   score: 3.0   memory length: 476978   epsilon: 0.2535815800092954    steps: 213    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2241   score: 4.0   memory length: 477217   epsilon: 0.25310836000929243    steps: 239    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 2242   score: 5.0   memory length: 477524   epsilon: 0.2525005000092886    steps: 307    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2243   score: 6.0   memory length: 477860   epsilon: 0.2518352200092844    steps: 336    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2244   score: 3.0   memory length: 478072   epsilon: 0.2514154600092817    steps: 212    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2245   score: 8.0   memory length: 478494   epsilon: 0.25057990000927644    steps: 422    lr: 0.0001     evaluation reward: 4.67\n",
      "episode: 2246   score: 4.0   memory length: 478738   epsilon: 0.2500967800092734    steps: 244    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 2247   score: 2.0   memory length: 478919   epsilon: 0.2497384000092711    steps: 181    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2248   score: 7.0   memory length: 479308   epsilon: 0.24896818000926624    steps: 389    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2249   score: 7.0   memory length: 479680   epsilon: 0.24823162000926158    steps: 372    lr: 0.0001     evaluation reward: 4.66\n",
      "episode: 2250   score: 8.0   memory length: 480069   epsilon: 0.2474614000092567    steps: 389    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2251   score: 4.0   memory length: 480308   epsilon: 0.2469881800092537    steps: 239    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2252   score: 4.0   memory length: 480567   epsilon: 0.24647536000925047    steps: 259    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2253   score: 4.0   memory length: 480827   epsilon: 0.2459605600092472    steps: 260    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2254   score: 3.0   memory length: 481055   epsilon: 0.24550912000924435    steps: 228    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 2255   score: 7.0   memory length: 481420   epsilon: 0.24478642000923978    steps: 365    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2256   score: 4.0   memory length: 481682   epsilon: 0.2442676600092365    steps: 262    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 2257   score: 4.0   memory length: 481944   epsilon: 0.24374890000923322    steps: 262    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 2258   score: 5.0   memory length: 482212   epsilon: 0.24321826000922986    steps: 268    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2259   score: 5.0   memory length: 482536   epsilon: 0.2425767400092258    steps: 324    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2260   score: 4.0   memory length: 482778   epsilon: 0.24209758000922277    steps: 242    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2261   score: 5.0   memory length: 483053   epsilon: 0.24155308000921932    steps: 275    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 2262   score: 5.0   memory length: 483345   epsilon: 0.24097492000921567    steps: 292    lr: 0.0001     evaluation reward: 4.74\n",
      "episode: 2263   score: 5.0   memory length: 483634   epsilon: 0.24040270000921204    steps: 289    lr: 0.0001     evaluation reward: 4.76\n",
      "episode: 2264   score: 3.0   memory length: 483846   epsilon: 0.2399829400092094    steps: 212    lr: 0.0001     evaluation reward: 4.74\n",
      "episode: 2265   score: 3.0   memory length: 484077   epsilon: 0.2395255600092065    steps: 231    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 2266   score: 3.0   memory length: 484287   epsilon: 0.23910976000920386    steps: 210    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 2267   score: 8.0   memory length: 484736   epsilon: 0.23822074000919824    steps: 449    lr: 0.0001     evaluation reward: 4.75\n",
      "episode: 2268   score: 5.0   memory length: 485045   epsilon: 0.23760892000919437    steps: 309    lr: 0.0001     evaluation reward: 4.74\n",
      "episode: 2269   score: 6.0   memory length: 485398   epsilon: 0.23690998000918995    steps: 353    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 2270   score: 11.0   memory length: 485831   epsilon: 0.23605264000918452    steps: 433    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 2271   score: 9.0   memory length: 486324   epsilon: 0.23507650000917835    steps: 493    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 2272   score: 3.0   memory length: 486534   epsilon: 0.23466070000917572    steps: 210    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 2273   score: 4.0   memory length: 486793   epsilon: 0.23414788000917247    steps: 259    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2274   score: 4.0   memory length: 487067   epsilon: 0.23360536000916904    steps: 274    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2275   score: 4.0   memory length: 487329   epsilon: 0.23308660000916576    steps: 262    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2276   score: 3.0   memory length: 487539   epsilon: 0.23267080000916313    steps: 210    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 2277   score: 7.0   memory length: 487913   epsilon: 0.23193028000915844    steps: 374    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 2278   score: 5.0   memory length: 488193   epsilon: 0.23137588000915493    steps: 280    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 2279   score: 4.0   memory length: 488468   epsilon: 0.2308313800091515    steps: 275    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2280   score: 6.0   memory length: 488832   epsilon: 0.23011066000914693    steps: 364    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2281   score: 4.0   memory length: 489128   epsilon: 0.22952458000914322    steps: 296    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2282   score: 4.0   memory length: 489403   epsilon: 0.22898008000913977    steps: 275    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 2283   score: 10.0   memory length: 489885   epsilon: 0.22802572000913374    steps: 482    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 2284   score: 3.0   memory length: 490114   epsilon: 0.22757230000913087    steps: 229    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 2285   score: 4.0   memory length: 490393   epsilon: 0.22701988000912737    steps: 279    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2286   score: 5.0   memory length: 490683   epsilon: 0.22644568000912374    steps: 290    lr: 0.0001     evaluation reward: 4.81\n",
      "episode: 2287   score: 4.0   memory length: 490958   epsilon: 0.2259011800091203    steps: 275    lr: 0.0001     evaluation reward: 4.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2288   score: 3.0   memory length: 491170   epsilon: 0.22548142000911764    steps: 212    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 2289   score: 3.0   memory length: 491399   epsilon: 0.22502800000911477    steps: 229    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 2290   score: 8.0   memory length: 491798   epsilon: 0.22423798000910977    steps: 399    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 2291   score: 5.0   memory length: 492102   epsilon: 0.22363606000910596    steps: 304    lr: 0.0001     evaluation reward: 4.74\n",
      "episode: 2292   score: 5.0   memory length: 492423   epsilon: 0.22300048000910194    steps: 321    lr: 0.0001     evaluation reward: 4.75\n",
      "episode: 2293   score: 4.0   memory length: 492685   epsilon: 0.22248172000909866    steps: 262    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 2294   score: 7.0   memory length: 493107   epsilon: 0.22164616000909337    steps: 422    lr: 0.0001     evaluation reward: 4.76\n",
      "episode: 2295   score: 3.0   memory length: 493319   epsilon: 0.22122640000909072    steps: 212    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2296   score: 5.0   memory length: 493623   epsilon: 0.2206244800090869    steps: 304    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 2297   score: 5.0   memory length: 493916   epsilon: 0.22004434000908324    steps: 293    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2298   score: 5.0   memory length: 494196   epsilon: 0.21948994000907973    steps: 280    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 2299   score: 3.0   memory length: 494443   epsilon: 0.21900088000907664    steps: 247    lr: 0.0001     evaluation reward: 4.69\n",
      "episode: 2300   score: 3.0   memory length: 494656   epsilon: 0.21857914000907397    steps: 213    lr: 0.0001     evaluation reward: 4.69\n",
      "episode: 2301   score: 3.0   memory length: 494869   epsilon: 0.2181574000090713    steps: 213    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 2302   score: 4.0   memory length: 495148   epsilon: 0.2176049800090678    steps: 279    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 2303   score: 6.0   memory length: 495491   epsilon: 0.2169258400090635    steps: 343    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 2304   score: 3.0   memory length: 495701   epsilon: 0.21651004000906088    steps: 210    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 2305   score: 3.0   memory length: 495911   epsilon: 0.21609424000905825    steps: 210    lr: 0.0001     evaluation reward: 4.67\n",
      "episode: 2306   score: 3.0   memory length: 496139   epsilon: 0.2156428000090554    steps: 228    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2307   score: 6.0   memory length: 496453   epsilon: 0.21502108000905146    steps: 314    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 2308   score: 5.0   memory length: 496776   epsilon: 0.2143815400090474    steps: 323    lr: 0.0001     evaluation reward: 4.65\n",
      "episode: 2309   score: 3.0   memory length: 496987   epsilon: 0.21396376000904477    steps: 211    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2310   score: 3.0   memory length: 497200   epsilon: 0.2135420200090421    steps: 213    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 2311   score: 5.0   memory length: 497509   epsilon: 0.21293020000903823    steps: 309    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2312   score: 4.0   memory length: 497750   epsilon: 0.2124530200090352    steps: 241    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 2313   score: 6.0   memory length: 498102   epsilon: 0.2117560600090308    steps: 352    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 2314   score: 7.0   memory length: 498470   epsilon: 0.2110274200090262    steps: 368    lr: 0.0001     evaluation reward: 4.66\n",
      "episode: 2315   score: 5.0   memory length: 498779   epsilon: 0.21041560000902232    steps: 309    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 2316   score: 8.0   memory length: 499200   epsilon: 0.20958202000901704    steps: 421    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 2317   score: 10.0   memory length: 499715   epsilon: 0.2085623200090106    steps: 515    lr: 0.0001     evaluation reward: 4.74\n",
      "episode: 2318   score: 9.0   memory length: 500034   epsilon: 0.2079307000090066    steps: 319    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2319   score: 4.0   memory length: 500276   epsilon: 0.20745154000900357    steps: 242    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 2320   score: 4.0   memory length: 500556   epsilon: 0.20689714000900006    steps: 280    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 2321   score: 3.0   memory length: 500768   epsilon: 0.2064773800089974    steps: 212    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 2322   score: 10.0   memory length: 501311   epsilon: 0.2054022400089906    steps: 543    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2323   score: 3.0   memory length: 501524   epsilon: 0.20498050000898793    steps: 213    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2324   score: 3.0   memory length: 501753   epsilon: 0.20452708000898506    steps: 229    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 2325   score: 8.0   memory length: 502044   epsilon: 0.20395090000898142    steps: 291    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 2326   score: 6.0   memory length: 502362   epsilon: 0.20332126000897743    steps: 318    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 2327   score: 4.0   memory length: 502621   epsilon: 0.2028084400089742    steps: 259    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2328   score: 2.0   memory length: 502821   epsilon: 0.20241244000897168    steps: 200    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2329   score: 7.0   memory length: 503225   epsilon: 0.20161252000896662    steps: 404    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 2330   score: 8.0   memory length: 503632   epsilon: 0.20080666000896152    steps: 407    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 2331   score: 9.0   memory length: 504102   epsilon: 0.19987606000895564    steps: 470    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2332   score: 6.0   memory length: 504458   epsilon: 0.19917118000895118    steps: 356    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2333   score: 8.0   memory length: 504879   epsilon: 0.1983376000089459    steps: 421    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2334   score: 3.0   memory length: 505092   epsilon: 0.19791586000894323    steps: 213    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2335   score: 3.0   memory length: 505304   epsilon: 0.19749610000894058    steps: 212    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2336   score: 7.0   memory length: 505710   epsilon: 0.1966922200089355    steps: 406    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2337   score: 7.0   memory length: 506104   epsilon: 0.19591210000893056    steps: 394    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2338   score: 3.0   memory length: 506336   epsilon: 0.19545274000892765    steps: 232    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2339   score: 2.0   memory length: 506518   epsilon: 0.19509238000892537    steps: 182    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2340   score: 7.0   memory length: 506922   epsilon: 0.1942924600089203    steps: 404    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2341   score: 5.0   memory length: 507204   epsilon: 0.19373410000891678    steps: 282    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2342   score: 6.0   memory length: 507524   epsilon: 0.19310050000891277    steps: 320    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2343   score: 5.0   memory length: 507828   epsilon: 0.19249858000890896    steps: 304    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2344   score: 5.0   memory length: 508118   epsilon: 0.19192438000890533    steps: 290    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2345   score: 3.0   memory length: 508331   epsilon: 0.19150264000890266    steps: 213    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2346   score: 3.0   memory length: 508542   epsilon: 0.19108486000890001    steps: 211    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2347   score: 3.0   memory length: 508754   epsilon: 0.19066510000889736    steps: 212    lr: 0.0001     evaluation reward: 5.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2348   score: 3.0   memory length: 508999   epsilon: 0.1901800000088943    steps: 245    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2349   score: 5.0   memory length: 509326   epsilon: 0.1895325400088902    steps: 327    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2350   score: 3.0   memory length: 509573   epsilon: 0.1890434800088871    steps: 247    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2351   score: 5.0   memory length: 509846   epsilon: 0.18850294000888368    steps: 273    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2352   score: 4.0   memory length: 510106   epsilon: 0.18798814000888042    steps: 260    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2353   score: 3.0   memory length: 510318   epsilon: 0.18756838000887777    steps: 212    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2354   score: 4.0   memory length: 510595   epsilon: 0.1870199200088743    steps: 277    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2355   score: 4.0   memory length: 510853   epsilon: 0.18650908000887106    steps: 258    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2356   score: 6.0   memory length: 511204   epsilon: 0.18581410000886667    steps: 351    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2357   score: 9.0   memory length: 511677   epsilon: 0.18487756000886074    steps: 473    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2358   score: 9.0   memory length: 512131   epsilon: 0.18397864000885505    steps: 454    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2359   score: 4.0   memory length: 512392   epsilon: 0.18346186000885178    steps: 261    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2360   score: 8.0   memory length: 512818   epsilon: 0.18261838000884645    steps: 426    lr: 0.0001     evaluation reward: 5.03\n",
      "episode: 2361   score: 3.0   memory length: 513028   epsilon: 0.18220258000884382    steps: 210    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2362   score: 4.0   memory length: 513304   epsilon: 0.18165610000884036    steps: 276    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2363   score: 3.0   memory length: 513550   epsilon: 0.18116902000883728    steps: 246    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2364   score: 4.0   memory length: 513792   epsilon: 0.18068986000883425    steps: 242    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2365   score: 3.0   memory length: 514005   epsilon: 0.18026812000883158    steps: 213    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2366   score: 6.0   memory length: 514381   epsilon: 0.17952364000882687    steps: 376    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2367   score: 8.0   memory length: 514802   epsilon: 0.1786900600088216    steps: 421    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2368   score: 5.0   memory length: 515096   epsilon: 0.1781079400088179    steps: 294    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2369   score: 4.0   memory length: 515354   epsilon: 0.17759710000881468    steps: 258    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2370   score: 5.0   memory length: 515674   epsilon: 0.17696350000881067    steps: 320    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2371   score: 4.0   memory length: 515958   epsilon: 0.1764011800088071    steps: 284    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2372   score: 3.0   memory length: 516170   epsilon: 0.17598142000880446    steps: 212    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2373   score: 6.0   memory length: 516512   epsilon: 0.17530426000880017    steps: 342    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2374   score: 6.0   memory length: 516849   epsilon: 0.17463700000879595    steps: 337    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 2375   score: 6.0   memory length: 517192   epsilon: 0.17395786000879165    steps: 343    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2376   score: 8.0   memory length: 517632   epsilon: 0.17308666000878614    steps: 440    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2377   score: 4.0   memory length: 517893   epsilon: 0.17256988000878287    steps: 261    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2378   score: 3.0   memory length: 518105   epsilon: 0.17215012000878022    steps: 212    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2379   score: 13.0   memory length: 518602   epsilon: 0.171166060008774    steps: 497    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2380   score: 3.0   memory length: 518815   epsilon: 0.17074432000877132    steps: 213    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2381   score: 3.0   memory length: 519025   epsilon: 0.1703285200087687    steps: 210    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2382   score: 6.0   memory length: 519386   epsilon: 0.16961374000876417    steps: 361    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2383   score: 3.0   memory length: 519599   epsilon: 0.1691920000087615    steps: 213    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2384   score: 3.0   memory length: 519812   epsilon: 0.16877026000875883    steps: 213    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2385   score: 3.0   memory length: 520043   epsilon: 0.16831288000875594    steps: 231    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2386   score: 3.0   memory length: 520256   epsilon: 0.16789114000875327    steps: 213    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2387   score: 8.0   memory length: 520668   epsilon: 0.1670753800087481    steps: 412    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2388   score: 4.0   memory length: 520927   epsilon: 0.16656256000874486    steps: 259    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2389   score: 3.0   memory length: 521173   epsilon: 0.16607548000874178    steps: 246    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2390   score: 6.0   memory length: 521513   epsilon: 0.16540228000873752    steps: 340    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2391   score: 5.0   memory length: 521786   epsilon: 0.1648617400087341    steps: 273    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2392   score: 4.0   memory length: 522028   epsilon: 0.16438258000873107    steps: 242    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2393   score: 5.0   memory length: 522336   epsilon: 0.1637727400087272    steps: 308    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2394   score: 8.0   memory length: 522779   epsilon: 0.16289560000872166    steps: 443    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2395   score: 6.0   memory length: 523116   epsilon: 0.16222834000871744    steps: 337    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2396   score: 5.0   memory length: 523446   epsilon: 0.1615749400087133    steps: 330    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2397   score: 3.0   memory length: 523712   epsilon: 0.16104826000870998    steps: 266    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2398   score: 4.0   memory length: 523989   epsilon: 0.1604998000087065    steps: 277    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2399   score: 6.0   memory length: 524326   epsilon: 0.15983254000870228    steps: 337    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2400   score: 5.0   memory length: 524636   epsilon: 0.1592187400086984    steps: 310    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2401   score: 8.0   memory length: 525058   epsilon: 0.15838318000869311    steps: 422    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2402   score: 5.0   memory length: 525345   epsilon: 0.15781492000868952    steps: 287    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2403   score: 3.0   memory length: 525575   epsilon: 0.15735952000868664    steps: 230    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2404   score: 3.0   memory length: 525803   epsilon: 0.15690808000868378    steps: 228    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2405   score: 5.0   memory length: 526095   epsilon: 0.15632992000868012    steps: 292    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2406   score: 3.0   memory length: 526307   epsilon: 0.15591016000867747    steps: 212    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2407   score: 4.0   memory length: 526566   epsilon: 0.15539734000867422    steps: 259    lr: 0.0001     evaluation reward: 5.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2408   score: 9.0   memory length: 527054   epsilon: 0.1544311000086681    steps: 488    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2409   score: 5.0   memory length: 527341   epsilon: 0.15386284000866451    steps: 287    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2410   score: 3.0   memory length: 527553   epsilon: 0.15344308000866186    steps: 212    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2411   score: 9.0   memory length: 528024   epsilon: 0.15251050000865596    steps: 471    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2412   score: 8.0   memory length: 528469   epsilon: 0.15162940000865038    steps: 445    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2413   score: 3.0   memory length: 528681   epsilon: 0.15120964000864773    steps: 212    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2414   score: 5.0   memory length: 528991   epsilon: 0.15059584000864384    steps: 310    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2415   score: 6.0   memory length: 529347   epsilon: 0.14989096000863938    steps: 356    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2416   score: 5.0   memory length: 529621   epsilon: 0.14934844000863595    steps: 274    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2417   score: 3.0   memory length: 529834   epsilon: 0.14892670000863328    steps: 213    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2418   score: 4.0   memory length: 530092   epsilon: 0.14841586000863005    steps: 258    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2419   score: 5.0   memory length: 530383   epsilon: 0.1478396800086264    steps: 291    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2420   score: 4.0   memory length: 530662   epsilon: 0.1472872600086229    steps: 279    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2421   score: 3.0   memory length: 530872   epsilon: 0.14687146000862028    steps: 210    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2422   score: 4.0   memory length: 531114   epsilon: 0.14639230000861725    steps: 242    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2423   score: 3.0   memory length: 531327   epsilon: 0.14597056000861458    steps: 213    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2424   score: 3.0   memory length: 531555   epsilon: 0.14551912000861172    steps: 228    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2425   score: 5.0   memory length: 531881   epsilon: 0.14487364000860764    steps: 326    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2426   score: 4.0   memory length: 532143   epsilon: 0.14435488000860436    steps: 262    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2427   score: 5.0   memory length: 532451   epsilon: 0.1437450400086005    steps: 308    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 2428   score: 4.0   memory length: 532728   epsilon: 0.14319658000859703    steps: 277    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2429   score: 7.0   memory length: 533131   epsilon: 0.14239864000859198    steps: 403    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2430   score: 6.0   memory length: 533487   epsilon: 0.14169376000858752    steps: 356    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 2431   score: 5.0   memory length: 533779   epsilon: 0.14111560000858386    steps: 292    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 2432   score: 3.0   memory length: 533989   epsilon: 0.14069980000858123    steps: 210    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 2433   score: 5.0   memory length: 534298   epsilon: 0.14008798000857736    steps: 309    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 2434   score: 7.0   memory length: 534656   epsilon: 0.13937914000857288    steps: 358    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2435   score: 7.0   memory length: 535042   epsilon: 0.13861486000856804    steps: 386    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 2436   score: 4.0   memory length: 535286   epsilon: 0.13813174000856498    steps: 244    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2437   score: 4.0   memory length: 535561   epsilon: 0.13758724000856154    steps: 275    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 2438   score: 5.0   memory length: 535873   epsilon: 0.13696948000855763    steps: 312    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2439   score: 9.0   memory length: 536281   epsilon: 0.13616164000855252    steps: 408    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2440   score: 3.0   memory length: 536494   epsilon: 0.13573990000854985    steps: 213    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 2441   score: 3.0   memory length: 536707   epsilon: 0.13531816000854718    steps: 213    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2442   score: 7.0   memory length: 537134   epsilon: 0.13447270000854183    steps: 427    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 2443   score: 5.0   memory length: 537461   epsilon: 0.13382524000853774    steps: 327    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 2444   score: 8.0   memory length: 537882   epsilon: 0.13299166000853246    steps: 421    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2445   score: 9.0   memory length: 538354   epsilon: 0.13205710000852655    steps: 472    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2446   score: 6.0   memory length: 538672   epsilon: 0.13142746000852257    steps: 318    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2447   score: 12.0   memory length: 539274   epsilon: 0.13023550000851503    steps: 602    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2448   score: 7.0   memory length: 539677   epsilon: 0.12943756000850998    steps: 403    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2449   score: 4.0   memory length: 539938   epsilon: 0.1289207800085067    steps: 261    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2450   score: 7.0   memory length: 540294   epsilon: 0.12821590000850225    steps: 356    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2451   score: 3.0   memory length: 540506   epsilon: 0.1277961400084996    steps: 212    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2452   score: 3.0   memory length: 540718   epsilon: 0.12737638000849694    steps: 212    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2453   score: 4.0   memory length: 540959   epsilon: 0.12689920000849392    steps: 241    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2454   score: 11.0   memory length: 541531   epsilon: 0.12576664000848675    steps: 572    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2455   score: 4.0   memory length: 541787   epsilon: 0.12525976000848354    steps: 256    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2456   score: 6.0   memory length: 542126   epsilon: 0.12458854000848218    steps: 339    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2457   score: 3.0   memory length: 542335   epsilon: 0.12417472000848247    steps: 209    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2458   score: 6.0   memory length: 542654   epsilon: 0.1235431000084829    steps: 319    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2459   score: 5.0   memory length: 542927   epsilon: 0.12300256000848327    steps: 273    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2460   score: 6.0   memory length: 543265   epsilon: 0.12233332000848372    steps: 338    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2461   score: 3.0   memory length: 543511   epsilon: 0.12184624000848406    steps: 246    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2462   score: 5.0   memory length: 543817   epsilon: 0.12124036000848447    steps: 306    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2463   score: 8.0   memory length: 544221   epsilon: 0.12044044000848501    steps: 404    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2464   score: 4.0   memory length: 544462   epsilon: 0.11996326000848534    steps: 241    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2465   score: 4.0   memory length: 544724   epsilon: 0.1194445000084857    steps: 262    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2466   score: 3.0   memory length: 544954   epsilon: 0.118989100008486    steps: 230    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2467   score: 5.0   memory length: 545257   epsilon: 0.11838916000848641    steps: 303    lr: 0.0001     evaluation reward: 5.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2468   score: 7.0   memory length: 545678   epsilon: 0.11755558000848698    steps: 421    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2469   score: 5.0   memory length: 546000   epsilon: 0.11691802000848742    steps: 322    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2470   score: 3.0   memory length: 546213   epsilon: 0.1164962800084877    steps: 213    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2471   score: 3.0   memory length: 546444   epsilon: 0.11603890000848802    steps: 231    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2472   score: 6.0   memory length: 546794   epsilon: 0.11534590000848849    steps: 350    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2473   score: 6.0   memory length: 547132   epsilon: 0.11467666000848895    steps: 338    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2474   score: 3.0   memory length: 547344   epsilon: 0.11425690000848923    steps: 212    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2475   score: 3.0   memory length: 547557   epsilon: 0.11383516000848952    steps: 213    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2476   score: 6.0   memory length: 547912   epsilon: 0.11313226000849    steps: 355    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2477   score: 5.0   memory length: 548202   epsilon: 0.11255806000849039    steps: 290    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2478   score: 4.0   memory length: 548463   epsilon: 0.11204128000849074    steps: 261    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2479   score: 3.0   memory length: 548690   epsilon: 0.11159182000849105    steps: 227    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2480   score: 5.0   memory length: 548996   epsilon: 0.11098594000849146    steps: 306    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2481   score: 4.0   memory length: 549256   epsilon: 0.11047114000849181    steps: 260    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2482   score: 5.0   memory length: 549562   epsilon: 0.10986526000849223    steps: 306    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2483   score: 4.0   memory length: 549821   epsilon: 0.10935244000849258    steps: 259    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2484   score: 6.0   memory length: 550158   epsilon: 0.10868518000849303    steps: 337    lr: 0.0001     evaluation reward: 5.03\n",
      "episode: 2485   score: 5.0   memory length: 550480   epsilon: 0.10804762000849347    steps: 322    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2486   score: 5.0   memory length: 550770   epsilon: 0.10747342000849386    steps: 290    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2487   score: 5.0   memory length: 551093   epsilon: 0.1068338800084943    steps: 323    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2488   score: 3.0   memory length: 551322   epsilon: 0.1063804600084946    steps: 229    lr: 0.0001     evaluation reward: 5.03\n",
      "episode: 2489   score: 4.0   memory length: 551564   epsilon: 0.10590130000849493    steps: 242    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2490   score: 7.0   memory length: 551949   epsilon: 0.10513900000849545    steps: 385    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2491   score: 7.0   memory length: 552321   epsilon: 0.10440244000849595    steps: 372    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2492   score: 6.0   memory length: 552650   epsilon: 0.1037510200084964    steps: 329    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2493   score: 4.0   memory length: 552907   epsilon: 0.10324216000849674    steps: 257    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2494   score: 7.0   memory length: 553308   epsilon: 0.10244818000849729    steps: 401    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2495   score: 5.0   memory length: 553614   epsilon: 0.1018423000084977    steps: 306    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2496   score: 5.0   memory length: 553942   epsilon: 0.10119286000849814    steps: 328    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2497   score: 7.0   memory length: 554324   epsilon: 0.10043650000849866    steps: 382    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2498   score: 6.0   memory length: 554702   epsilon: 0.09968806000849917    steps: 378    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2499   score: 4.0   memory length: 554946   epsilon: 0.0992049400084995    steps: 244    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2500   score: 3.0   memory length: 555158   epsilon: 0.09878518000849978    steps: 212    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2501   score: 3.0   memory length: 555371   epsilon: 0.09836344000850007    steps: 213    lr: 0.0001     evaluation reward: 5.03\n",
      "episode: 2502   score: 7.0   memory length: 555792   epsilon: 0.09752986000850064    steps: 421    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2503   score: 5.0   memory length: 556082   epsilon: 0.09695566000850103    steps: 290    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2504   score: 3.0   memory length: 556294   epsilon: 0.09653590000850132    steps: 212    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2505   score: 10.0   memory length: 556823   epsilon: 0.09548848000850203    steps: 529    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2506   score: 4.0   memory length: 557083   epsilon: 0.09497368000850238    steps: 260    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2507   score: 5.0   memory length: 557391   epsilon: 0.0943638400085028    steps: 308    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2508   score: 4.0   memory length: 557632   epsilon: 0.09388666000850313    steps: 241    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2509   score: 5.0   memory length: 557938   epsilon: 0.09328078000850354    steps: 306    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2510   score: 7.0   memory length: 558339   epsilon: 0.09248680000850408    steps: 401    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2511   score: 4.0   memory length: 558614   epsilon: 0.09194230000850445    steps: 275    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2512   score: 7.0   memory length: 558979   epsilon: 0.09121960000850494    steps: 365    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2513   score: 5.0   memory length: 559269   epsilon: 0.09064540000850534    steps: 290    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2514   score: 7.0   memory length: 559623   epsilon: 0.08994448000850581    steps: 354    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2515   score: 4.0   memory length: 559864   epsilon: 0.08946730000850614    steps: 241    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2516   score: 8.0   memory length: 560319   epsilon: 0.08856640000850675    steps: 455    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2517   score: 3.0   memory length: 560532   epsilon: 0.08814466000850704    steps: 213    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2518   score: 7.0   memory length: 560956   epsilon: 0.08730514000850761    steps: 424    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2519   score: 4.0   memory length: 561217   epsilon: 0.08678836000850797    steps: 261    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2520   score: 5.0   memory length: 561549   epsilon: 0.08613100000850842    steps: 332    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2521   score: 5.0   memory length: 561842   epsilon: 0.08555086000850881    steps: 293    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2522   score: 5.0   memory length: 562166   epsilon: 0.08490934000850925    steps: 324    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2523   score: 5.0   memory length: 562456   epsilon: 0.08433514000850964    steps: 290    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2524   score: 5.0   memory length: 562726   epsilon: 0.08380054000851    steps: 270    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 2525   score: 4.0   memory length: 562988   epsilon: 0.08328178000851036    steps: 262    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2526   score: 6.0   memory length: 563310   epsilon: 0.0826442200085108    steps: 322    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2527   score: 8.0   memory length: 563763   epsilon: 0.0817472800085114    steps: 453    lr: 0.0001     evaluation reward: 5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2528   score: 6.0   memory length: 564121   epsilon: 0.08103844000851189    steps: 358    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 2529   score: 4.0   memory length: 564396   epsilon: 0.08049394000851226    steps: 275    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 2530   score: 4.0   memory length: 564654   epsilon: 0.07998310000851261    steps: 258    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2531   score: 5.0   memory length: 564964   epsilon: 0.07936930000851303    steps: 310    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2532   score: 4.0   memory length: 565245   epsilon: 0.0788129200085134    steps: 281    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2533   score: 4.0   memory length: 565487   epsilon: 0.07833376000851373    steps: 242    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2534   score: 3.0   memory length: 565700   epsilon: 0.07791202000851402    steps: 213    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2535   score: 6.0   memory length: 566036   epsilon: 0.07724674000851448    steps: 336    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2536   score: 5.0   memory length: 566339   epsilon: 0.07664680000851488    steps: 303    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2537   score: 4.0   memory length: 566596   epsilon: 0.07613794000851523    steps: 257    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2538   score: 4.0   memory length: 566838   epsilon: 0.07565878000851556    steps: 242    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2539   score: 4.0   memory length: 567080   epsilon: 0.07517962000851588    steps: 242    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2540   score: 5.0   memory length: 567404   epsilon: 0.07453810000851632    steps: 324    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2541   score: 5.0   memory length: 567692   epsilon: 0.07396786000851671    steps: 288    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2542   score: 4.0   memory length: 567949   epsilon: 0.07345900000851706    steps: 257    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2543   score: 6.0   memory length: 568342   epsilon: 0.07268086000851759    steps: 393    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2544   score: 5.0   memory length: 568653   epsilon: 0.07206508000851801    steps: 311    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2545   score: 4.0   memory length: 568913   epsilon: 0.07155028000851836    steps: 260    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2546   score: 7.0   memory length: 569306   epsilon: 0.07077214000851889    steps: 393    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2547   score: 5.0   memory length: 569595   epsilon: 0.07019992000851928    steps: 289    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2548   score: 3.0   memory length: 569822   epsilon: 0.06975046000851959    steps: 227    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2549   score: 5.0   memory length: 570150   epsilon: 0.06910102000852003    steps: 328    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2550   score: 9.0   memory length: 570657   epsilon: 0.06809716000852072    steps: 507    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2551   score: 3.0   memory length: 570885   epsilon: 0.06764572000852102    steps: 228    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2552   score: 4.0   memory length: 571144   epsilon: 0.06713290000852137    steps: 259    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2553   score: 5.0   memory length: 571469   epsilon: 0.06648940000852181    steps: 325    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2554   score: 5.0   memory length: 571795   epsilon: 0.06584392000852225    steps: 326    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2555   score: 5.0   memory length: 572101   epsilon: 0.06523804000852267    steps: 306    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2556   score: 8.0   memory length: 572500   epsilon: 0.0644480200085232    steps: 399    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2557   score: 3.0   memory length: 572727   epsilon: 0.06399856000852351    steps: 227    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2558   score: 9.0   memory length: 573181   epsilon: 0.06309964000852412    steps: 454    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2559   score: 8.0   memory length: 573603   epsilon: 0.062264080008524694    steps: 422    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2560   score: 7.0   memory length: 574010   epsilon: 0.061458220008525244    steps: 407    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2561   score: 5.0   memory length: 574302   epsilon: 0.06088006000852564    steps: 292    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2562   score: 6.0   memory length: 574622   epsilon: 0.06024646000852607    steps: 320    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2563   score: 3.0   memory length: 574852   epsilon: 0.05979106000852638    steps: 230    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2564   score: 5.0   memory length: 575142   epsilon: 0.05921686000852677    steps: 290    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2565   score: 3.0   memory length: 575371   epsilon: 0.05876344000852708    steps: 229    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2566   score: 7.0   memory length: 575741   epsilon: 0.05803084000852758    steps: 370    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2567   score: 6.0   memory length: 576060   epsilon: 0.05739922000852801    steps: 319    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2568   score: 6.0   memory length: 576396   epsilon: 0.056733940008528466    steps: 336    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2569   score: 3.0   memory length: 576625   epsilon: 0.056280520008528775    steps: 229    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2570   score: 4.0   memory length: 576901   epsilon: 0.05573404000852915    steps: 276    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2571   score: 4.0   memory length: 577143   epsilon: 0.055254880008529475    steps: 242    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2572   score: 6.0   memory length: 577517   epsilon: 0.05451436000852998    steps: 374    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2573   score: 5.0   memory length: 577807   epsilon: 0.05394016000853037    steps: 290    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2574   score: 4.0   memory length: 578048   epsilon: 0.0534629800085307    steps: 241    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2575   score: 3.0   memory length: 578257   epsilon: 0.05304916000853098    steps: 209    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2576   score: 7.0   memory length: 578659   epsilon: 0.05225320000853152    steps: 402    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2577   score: 7.0   memory length: 579050   epsilon: 0.05147902000853205    steps: 391    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2578   score: 3.0   memory length: 579278   epsilon: 0.05102758000853236    steps: 228    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2579   score: 4.0   memory length: 579555   epsilon: 0.05047912000853273    steps: 277    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2580   score: 5.0   memory length: 579859   epsilon: 0.04987720000853314    steps: 304    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2581   score: 8.0   memory length: 580155   epsilon: 0.04929112000853354    steps: 296    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2582   score: 4.0   memory length: 580397   epsilon: 0.04881196000853387    steps: 242    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2583   score: 8.0   memory length: 580824   epsilon: 0.047966500008534446    steps: 427    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2584   score: 7.0   memory length: 581209   epsilon: 0.047204200008534966    steps: 385    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2585   score: 6.0   memory length: 581552   epsilon: 0.04652506000853543    steps: 343    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2586   score: 6.0   memory length: 581875   epsilon: 0.045885520008535866    steps: 323    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2587   score: 5.0   memory length: 582165   epsilon: 0.04531132000853626    steps: 290    lr: 0.0001     evaluation reward: 5.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2588   score: 4.0   memory length: 582428   epsilon: 0.04479058000853661    steps: 263    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 2589   score: 5.0   memory length: 582739   epsilon: 0.04417480000853703    steps: 311    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2590   score: 7.0   memory length: 583145   epsilon: 0.04337092000853758    steps: 406    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2591   score: 4.0   memory length: 583401   epsilon: 0.042864040008537926    steps: 256    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2592   score: 4.0   memory length: 583676   epsilon: 0.0423195400085383    steps: 275    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2593   score: 5.0   memory length: 583984   epsilon: 0.041709700008538714    steps: 308    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2594   score: 3.0   memory length: 584234   epsilon: 0.04121470000853905    steps: 250    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2595   score: 7.0   memory length: 584609   epsilon: 0.04047220000853956    steps: 375    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2596   score: 3.0   memory length: 584822   epsilon: 0.040050460008539845    steps: 213    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2597   score: 4.0   memory length: 585064   epsilon: 0.03957130000854017    steps: 242    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2598   score: 9.0   memory length: 585534   epsilon: 0.03864070000854081    steps: 470    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2599   score: 13.0   memory length: 586058   epsilon: 0.037603180008541515    steps: 524    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2600   score: 3.0   memory length: 586268   epsilon: 0.0371873800085418    steps: 210    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2601   score: 10.0   memory length: 586745   epsilon: 0.03624292000854244    steps: 477    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 2602   score: 3.0   memory length: 586957   epsilon: 0.03582316000854273    steps: 212    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 2603   score: 6.0   memory length: 587300   epsilon: 0.03514402000854319    steps: 343    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 2604   score: 9.0   memory length: 587787   epsilon: 0.03417976000854385    steps: 487    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 2605   score: 4.0   memory length: 588069   epsilon: 0.03362140000854423    steps: 282    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 2606   score: 7.0   memory length: 588403   epsilon: 0.03296008000854468    steps: 334    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 2607   score: 5.0   memory length: 588691   epsilon: 0.03238984000854507    steps: 288    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 2608   score: 21.0   memory length: 589203   epsilon: 0.03137608000854576    steps: 512    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 2609   score: 9.0   memory length: 589657   epsilon: 0.030477160008546375    steps: 454    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2610   score: 4.0   memory length: 589899   epsilon: 0.029998000008546702    steps: 242    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2611   score: 8.0   memory length: 590330   epsilon: 0.029144620008547284    steps: 431    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2612   score: 7.0   memory length: 590736   epsilon: 0.028340740008547832    steps: 406    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2613   score: 8.0   memory length: 591173   epsilon: 0.027475480008548422    steps: 437    lr: 0.0001     evaluation reward: 5.56\n",
      "episode: 2614   score: 3.0   memory length: 591403   epsilon: 0.027020080008548733    steps: 230    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2615   score: 4.0   memory length: 591645   epsilon: 0.02654092000854906    steps: 242    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2616   score: 6.0   memory length: 592003   epsilon: 0.025832080008549543    steps: 358    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2617   score: 6.0   memory length: 592362   epsilon: 0.025121260008550028    steps: 359    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2618   score: 7.0   memory length: 592733   epsilon: 0.02438668000855053    steps: 371    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2619   score: 7.0   memory length: 593057   epsilon: 0.023745160008550967    steps: 324    lr: 0.0001     evaluation reward: 5.56\n",
      "episode: 2620   score: 4.0   memory length: 593298   epsilon: 0.023267980008551292    steps: 241    lr: 0.0001     evaluation reward: 5.55\n",
      "episode: 2621   score: 4.0   memory length: 593540   epsilon: 0.02278882000855162    steps: 242    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 2622   score: 4.0   memory length: 593799   epsilon: 0.02227600000855197    steps: 259    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2623   score: 7.0   memory length: 594167   epsilon: 0.021547360008552466    steps: 368    lr: 0.0001     evaluation reward: 5.55\n",
      "episode: 2624   score: 4.0   memory length: 594429   epsilon: 0.02102860000855282    steps: 262    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 2625   score: 4.0   memory length: 594671   epsilon: 0.020549440008553146    steps: 242    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 2626   score: 9.0   memory length: 595012   epsilon: 0.019874260008553607    steps: 341    lr: 0.0001     evaluation reward: 5.57\n",
      "episode: 2627   score: 3.0   memory length: 595225   epsilon: 0.019452520008553895    steps: 213    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2628   score: 5.0   memory length: 595517   epsilon: 0.01887436000855429    steps: 292    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2629   score: 3.0   memory length: 595730   epsilon: 0.018452620008554577    steps: 213    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2630   score: 4.0   memory length: 596010   epsilon: 0.017898220008554955    steps: 280    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2631   score: 4.0   memory length: 596252   epsilon: 0.01741906000855528    steps: 242    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2632   score: 4.0   memory length: 596494   epsilon: 0.01693990000855561    steps: 242    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2633   score: 6.0   memory length: 596849   epsilon: 0.016237000008556088    steps: 355    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2634   score: 5.0   memory length: 597137   epsilon: 0.015666760008556477    steps: 288    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2635   score: 6.0   memory length: 597474   epsilon: 0.014999500008556384    steps: 337    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2636   score: 4.0   memory length: 597751   epsilon: 0.014451040008556277    steps: 277    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2637   score: 4.0   memory length: 598027   epsilon: 0.013904560008556171    steps: 276    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2638   score: 7.0   memory length: 598388   epsilon: 0.013189780008556033    steps: 361    lr: 0.0001     evaluation reward: 5.55\n",
      "episode: 2639   score: 4.0   memory length: 598630   epsilon: 0.01271062000855594    steps: 242    lr: 0.0001     evaluation reward: 5.55\n",
      "episode: 2640   score: 3.0   memory length: 598858   epsilon: 0.012259180008555852    steps: 228    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2641   score: 3.0   memory length: 599071   epsilon: 0.01183744000855577    steps: 213    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2642   score: 6.0   memory length: 599425   epsilon: 0.011136520008555634    steps: 354    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2643   score: 4.0   memory length: 599681   epsilon: 0.010629640008555536    steps: 256    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2644   score: 6.0   memory length: 600031   epsilon: 0.009998020008555413    steps: 350    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2645   score: 3.0   memory length: 600258   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2646   score: 6.0   memory length: 600594   epsilon: 0.009998020008555413    steps: 336    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2647   score: 6.0   memory length: 600917   epsilon: 0.009998020008555413    steps: 323    lr: 0.0001     evaluation reward: 5.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2648   score: 7.0   memory length: 601302   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 5.55\n",
      "episode: 2649   score: 3.0   memory length: 601515   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2650   score: 6.0   memory length: 601874   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2651   score: 3.0   memory length: 602086   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2652   score: 3.0   memory length: 602299   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2653   score: 4.0   memory length: 602574   epsilon: 0.009998020008555413    steps: 275    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 2654   score: 3.0   memory length: 602803   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 2655   score: 6.0   memory length: 603159   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2656   score: 8.0   memory length: 603546   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2657   score: 5.0   memory length: 603836   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2658   score: 6.0   memory length: 604160   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 2659   score: 4.0   memory length: 604402   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.42\n",
      "episode: 2660   score: 3.0   memory length: 604614   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2661   score: 4.0   memory length: 604856   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 2662   score: 7.0   memory length: 605231   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2663   score: 4.0   memory length: 605473   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 2664   score: 7.0   memory length: 605859   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 2665   score: 6.0   memory length: 606214   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2666   score: 6.0   memory length: 606520   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 2667   score: 6.0   memory length: 606878   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 2668   score: 7.0   memory length: 607261   epsilon: 0.009998020008555413    steps: 383    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2669   score: 3.0   memory length: 607473   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2670   score: 9.0   memory length: 607910   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2671   score: 7.0   memory length: 608303   epsilon: 0.009998020008555413    steps: 393    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2672   score: 4.0   memory length: 608560   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2673   score: 6.0   memory length: 608912   epsilon: 0.009998020008555413    steps: 352    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2674   score: 4.0   memory length: 609153   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2675   score: 3.0   memory length: 609366   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2676   score: 3.0   memory length: 609578   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2677   score: 5.0   memory length: 609863   epsilon: 0.009998020008555413    steps: 285    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2678   score: 3.0   memory length: 610075   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2679   score: 6.0   memory length: 610394   epsilon: 0.009998020008555413    steps: 319    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2680   score: 3.0   memory length: 610623   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2681   score: 5.0   memory length: 610915   epsilon: 0.009998020008555413    steps: 292    lr: 0.0001     evaluation reward: 5.42\n",
      "episode: 2682   score: 6.0   memory length: 611220   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2683   score: 4.0   memory length: 611460   epsilon: 0.009998020008555413    steps: 240    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 2684   score: 5.0   memory length: 611735   epsilon: 0.009998020008555413    steps: 275    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2685   score: 11.0   memory length: 612121   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 2686   score: 8.0   memory length: 612527   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2687   score: 9.0   memory length: 612852   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2688   score: 3.0   memory length: 613065   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 2689   score: 6.0   memory length: 613418   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2690   score: 4.0   memory length: 613657   epsilon: 0.009998020008555413    steps: 239    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 2691   score: 9.0   memory length: 614127   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2692   score: 11.0   memory length: 614557   epsilon: 0.009998020008555413    steps: 430    lr: 0.0001     evaluation reward: 5.58\n",
      "episode: 2693   score: 3.0   memory length: 614770   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.56\n",
      "episode: 2694   score: 6.0   memory length: 615126   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 5.59\n",
      "episode: 2695   score: 6.0   memory length: 615482   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 5.58\n",
      "episode: 2696   score: 3.0   memory length: 615695   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.58\n",
      "episode: 2697   score: 5.0   memory length: 615999   epsilon: 0.009998020008555413    steps: 304    lr: 0.0001     evaluation reward: 5.59\n",
      "episode: 2698   score: 11.0   memory length: 616440   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 5.61\n",
      "episode: 2699   score: 4.0   memory length: 616715   epsilon: 0.009998020008555413    steps: 275    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2700   score: 5.0   memory length: 617003   epsilon: 0.009998020008555413    steps: 288    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 2701   score: 3.0   memory length: 617216   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2702   score: 3.0   memory length: 617426   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2703   score: 3.0   memory length: 617639   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2704   score: 3.0   memory length: 617851   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2705   score: 6.0   memory length: 618210   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 2706   score: 3.0   memory length: 618422   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2707   score: 8.0   memory length: 618843   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 2708   score: 4.0   memory length: 619084   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 2709   score: 5.0   memory length: 619374   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2710   score: 6.0   memory length: 619752   epsilon: 0.009998020008555413    steps: 378    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2711   score: 4.0   memory length: 619994   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2712   score: 5.0   memory length: 620268   epsilon: 0.009998020008555413    steps: 274    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2713   score: 5.0   memory length: 620575   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2714   score: 6.0   memory length: 620896   epsilon: 0.009998020008555413    steps: 321    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2715   score: 4.0   memory length: 621173   epsilon: 0.009998020008555413    steps: 277    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2716   score: 6.0   memory length: 621528   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2717   score: 9.0   memory length: 622011   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2718   score: 7.0   memory length: 622406   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2719   score: 4.0   memory length: 622663   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2720   score: 4.0   memory length: 622919   epsilon: 0.009998020008555413    steps: 256    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2721   score: 7.0   memory length: 623309   epsilon: 0.009998020008555413    steps: 390    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2722   score: 6.0   memory length: 623664   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2723   score: 3.0   memory length: 623874   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2724   score: 5.0   memory length: 624163   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2725   score: 3.0   memory length: 624373   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2726   score: 4.0   memory length: 624633   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2727   score: 5.0   memory length: 624941   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2728   score: 6.0   memory length: 625282   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2729   score: 4.0   memory length: 625558   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2730   score: 2.0   memory length: 625776   epsilon: 0.009998020008555413    steps: 218    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2731   score: 6.0   memory length: 626132   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2732   score: 6.0   memory length: 626452   epsilon: 0.009998020008555413    steps: 320    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2733   score: 6.0   memory length: 626828   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2734   score: 8.0   memory length: 627283   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2735   score: 6.0   memory length: 627626   epsilon: 0.009998020008555413    steps: 343    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2736   score: 7.0   memory length: 627986   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 2737   score: 5.0   memory length: 628293   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2738   score: 3.0   memory length: 628525   epsilon: 0.009998020008555413    steps: 232    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2739   score: 3.0   memory length: 628755   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2740   score: 4.0   memory length: 629031   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2741   score: 4.0   memory length: 629308   epsilon: 0.009998020008555413    steps: 277    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2742   score: 6.0   memory length: 629642   epsilon: 0.009998020008555413    steps: 334    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2743   score: 5.0   memory length: 629931   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2744   score: 4.0   memory length: 630172   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2745   score: 7.0   memory length: 630577   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2746   score: 3.0   memory length: 630789   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2747   score: 3.0   memory length: 631002   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2748   score: 5.0   memory length: 631326   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2749   score: 3.0   memory length: 631538   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2750   score: 4.0   memory length: 631798   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2751   score: 3.0   memory length: 632010   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2752   score: 6.0   memory length: 632316   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2753   score: 5.0   memory length: 632625   epsilon: 0.009998020008555413    steps: 309    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2754   score: 4.0   memory length: 632885   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2755   score: 5.0   memory length: 633190   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2756   score: 4.0   memory length: 633451   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2757   score: 4.0   memory length: 633691   epsilon: 0.009998020008555413    steps: 240    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2758   score: 4.0   memory length: 633933   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2759   score: 4.0   memory length: 634189   epsilon: 0.009998020008555413    steps: 256    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2760   score: 6.0   memory length: 634527   epsilon: 0.009998020008555413    steps: 338    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2761   score: 3.0   memory length: 634754   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2762   score: 9.0   memory length: 635098   epsilon: 0.009998020008555413    steps: 344    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2763   score: 9.0   memory length: 635591   epsilon: 0.009998020008555413    steps: 493    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2764   score: 3.0   memory length: 635837   epsilon: 0.009998020008555413    steps: 246    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2765   score: 7.0   memory length: 636249   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 5.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2766   score: 4.0   memory length: 636509   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2767   score: 8.0   memory length: 636923   epsilon: 0.009998020008555413    steps: 414    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2768   score: 4.0   memory length: 637181   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2769   score: 4.0   memory length: 637447   epsilon: 0.009998020008555413    steps: 266    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2770   score: 6.0   memory length: 637784   epsilon: 0.009998020008555413    steps: 337    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 2771   score: 3.0   memory length: 638012   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2772   score: 6.0   memory length: 638316   epsilon: 0.009998020008555413    steps: 304    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2773   score: 4.0   memory length: 638558   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2774   score: 4.0   memory length: 638834   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2775   score: 11.0   memory length: 639251   epsilon: 0.009998020008555413    steps: 417    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2776   score: 5.0   memory length: 639543   epsilon: 0.009998020008555413    steps: 292    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2777   score: 4.0   memory length: 639787   epsilon: 0.009998020008555413    steps: 244    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2778   score: 5.0   memory length: 640064   epsilon: 0.009998020008555413    steps: 277    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 2779   score: 3.0   memory length: 640276   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2780   score: 4.0   memory length: 640536   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2781   score: 4.0   memory length: 640814   epsilon: 0.009998020008555413    steps: 278    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2782   score: 4.0   memory length: 641074   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2783   score: 6.0   memory length: 641430   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 2784   score: 3.0   memory length: 641642   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2785   score: 3.0   memory length: 641854   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2786   score: 5.0   memory length: 642143   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2787   score: 3.0   memory length: 642353   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2788   score: 3.0   memory length: 642566   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2789   score: 3.0   memory length: 642778   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 2790   score: 6.0   memory length: 643114   epsilon: 0.009998020008555413    steps: 336    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2791   score: 6.0   memory length: 643490   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 2792   score: 4.0   memory length: 643750   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2793   score: 4.0   memory length: 644010   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 2794   score: 4.0   memory length: 644268   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2795   score: 8.0   memory length: 644724   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 2796   score: 4.0   memory length: 644966   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 2797   score: 3.0   memory length: 645194   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 2798   score: 4.0   memory length: 645470   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 2799   score: 3.0   memory length: 645683   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 2800   score: 7.0   memory length: 646072   epsilon: 0.009998020008555413    steps: 389    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2801   score: 3.0   memory length: 646299   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 2802   score: 7.0   memory length: 646668   epsilon: 0.009998020008555413    steps: 369    lr: 0.0001     evaluation reward: 4.83\n",
      "episode: 2803   score: 4.0   memory length: 646909   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2804   score: 7.0   memory length: 647316   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 2805   score: 6.0   memory length: 647670   epsilon: 0.009998020008555413    steps: 354    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 2806   score: 4.0   memory length: 647911   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2807   score: 3.0   memory length: 648141   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 4.84\n",
      "episode: 2808   score: 9.0   memory length: 648597   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2809   score: 6.0   memory length: 648918   epsilon: 0.009998020008555413    steps: 321    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 2810   score: 7.0   memory length: 649330   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2811   score: 9.0   memory length: 649804   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2812   score: 3.0   memory length: 650016   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2813   score: 5.0   memory length: 650322   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2814   score: 7.0   memory length: 650700   epsilon: 0.009998020008555413    steps: 378    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2815   score: 4.0   memory length: 650959   epsilon: 0.009998020008555413    steps: 259    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2816   score: 4.0   memory length: 651220   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 2817   score: 3.0   memory length: 651448   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 2818   score: 9.0   memory length: 651902   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 4.89\n",
      "episode: 2819   score: 6.0   memory length: 652245   epsilon: 0.009998020008555413    steps: 343    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2820   score: 4.0   memory length: 652526   epsilon: 0.009998020008555413    steps: 281    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2821   score: 7.0   memory length: 652920   epsilon: 0.009998020008555413    steps: 394    lr: 0.0001     evaluation reward: 4.91\n",
      "episode: 2822   score: 8.0   memory length: 653359   epsilon: 0.009998020008555413    steps: 439    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 2823   score: 5.0   memory length: 653666   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2824   score: 5.0   memory length: 653955   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 4.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2825   score: 3.0   memory length: 654182   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2826   score: 7.0   memory length: 654556   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2827   score: 4.0   memory length: 654816   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2828   score: 6.0   memory length: 655174   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2829   score: 6.0   memory length: 655573   epsilon: 0.009998020008555413    steps: 399    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2830   score: 8.0   memory length: 656017   epsilon: 0.009998020008555413    steps: 444    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2831   score: 6.0   memory length: 656354   epsilon: 0.009998020008555413    steps: 337    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2832   score: 3.0   memory length: 656581   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2833   score: 3.0   memory length: 656794   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2834   score: 6.0   memory length: 657153   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2835   score: 6.0   memory length: 657471   epsilon: 0.009998020008555413    steps: 318    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2836   score: 6.0   memory length: 657798   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2837   score: 10.0   memory length: 658275   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2838   score: 10.0   memory length: 658758   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2839   score: 9.0   memory length: 659200   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2840   score: 12.0   memory length: 659672   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 2841   score: 4.0   memory length: 659928   epsilon: 0.009998020008555413    steps: 256    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 2842   score: 4.0   memory length: 660170   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2843   score: 4.0   memory length: 660431   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2844   score: 5.0   memory length: 660739   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2845   score: 8.0   memory length: 661162   epsilon: 0.009998020008555413    steps: 423    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2846   score: 3.0   memory length: 661373   epsilon: 0.009998020008555413    steps: 211    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2847   score: 3.0   memory length: 661586   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2848   score: 4.0   memory length: 661828   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2849   score: 7.0   memory length: 662219   epsilon: 0.009998020008555413    steps: 391    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2850   score: 4.0   memory length: 662479   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2851   score: 9.0   memory length: 662822   epsilon: 0.009998020008555413    steps: 343    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 2852   score: 4.0   memory length: 663102   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 2853   score: 3.0   memory length: 663329   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 2854   score: 3.0   memory length: 663557   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 2855   score: 4.0   memory length: 663814   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2856   score: 3.0   memory length: 664026   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2857   score: 4.0   memory length: 664286   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2858   score: 8.0   memory length: 664718   epsilon: 0.009998020008555413    steps: 432    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 2859   score: 12.0   memory length: 665182   epsilon: 0.009998020008555413    steps: 464    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 2860   score: 3.0   memory length: 665412   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.32\n",
      "episode: 2861   score: 3.0   memory length: 665624   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.32\n",
      "episode: 2862   score: 3.0   memory length: 665836   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 2863   score: 3.0   memory length: 666065   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2864   score: 6.0   memory length: 666421   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2865   score: 4.0   memory length: 666678   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2866   score: 3.0   memory length: 666907   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.19\n",
      "episode: 2867   score: 3.0   memory length: 667157   epsilon: 0.009998020008555413    steps: 250    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2868   score: 7.0   memory length: 667523   epsilon: 0.009998020008555413    steps: 366    lr: 0.0001     evaluation reward: 5.17\n",
      "episode: 2869   score: 8.0   memory length: 667944   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2870   score: 6.0   memory length: 668267   epsilon: 0.009998020008555413    steps: 323    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 2871   score: 6.0   memory length: 668605   epsilon: 0.009998020008555413    steps: 338    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2872   score: 8.0   memory length: 669035   epsilon: 0.009998020008555413    steps: 430    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 2873   score: 3.0   memory length: 669248   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 2874   score: 5.0   memory length: 669556   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 2875   score: 8.0   memory length: 669976   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 2876   score: 6.0   memory length: 670355   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 2877   score: 7.0   memory length: 670724   epsilon: 0.009998020008555413    steps: 369    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 2878   score: 8.0   memory length: 671129   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 2879   score: 10.0   memory length: 671603   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 2880   score: 5.0   memory length: 671913   epsilon: 0.009998020008555413    steps: 310    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2881   score: 4.0   memory length: 672154   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2882   score: 4.0   memory length: 672414   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2883   score: 8.0   memory length: 672836   epsilon: 0.009998020008555413    steps: 422    lr: 0.0001     evaluation reward: 5.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2884   score: 4.0   memory length: 673112   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 2885   score: 6.0   memory length: 673425   epsilon: 0.009998020008555413    steps: 313    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2886   score: 5.0   memory length: 673732   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2887   score: 6.0   memory length: 674050   epsilon: 0.009998020008555413    steps: 318    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2888   score: 9.0   memory length: 674499   epsilon: 0.009998020008555413    steps: 449    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2889   score: 4.0   memory length: 674780   epsilon: 0.009998020008555413    steps: 281    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 2890   score: 5.0   memory length: 675067   epsilon: 0.009998020008555413    steps: 287    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2891   score: 4.0   memory length: 675326   epsilon: 0.009998020008555413    steps: 259    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2892   score: 4.0   memory length: 675584   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2893   score: 3.0   memory length: 675796   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2894   score: 2.0   memory length: 675994   epsilon: 0.009998020008555413    steps: 198    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 2895   score: 4.0   memory length: 676234   epsilon: 0.009998020008555413    steps: 240    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2896   score: 3.0   memory length: 676464   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 2897   score: 9.0   memory length: 676898   epsilon: 0.009998020008555413    steps: 434    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2898   score: 6.0   memory length: 677209   epsilon: 0.009998020008555413    steps: 311    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 2899   score: 4.0   memory length: 677509   epsilon: 0.009998020008555413    steps: 300    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2900   score: 7.0   memory length: 677897   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 2901   score: 4.0   memory length: 678157   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 2902   score: 3.0   memory length: 678387   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2903   score: 4.0   memory length: 678649   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2904   score: 3.0   memory length: 678879   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2905   score: 3.0   memory length: 679109   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.42\n",
      "episode: 2906   score: 3.0   memory length: 679322   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 2907   score: 7.0   memory length: 679724   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2908   score: 4.0   memory length: 679966   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 2909   score: 9.0   memory length: 680437   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 2910   score: 14.0   memory length: 680804   epsilon: 0.009998020008555413    steps: 367    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 2911   score: 4.0   memory length: 681045   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2912   score: 4.0   memory length: 681286   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 2913   score: 6.0   memory length: 681641   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 2914   score: 6.0   memory length: 681961   epsilon: 0.009998020008555413    steps: 320    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 2915   score: 3.0   memory length: 682191   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2916   score: 8.0   memory length: 682651   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2917   score: 3.0   memory length: 682879   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2918   score: 4.0   memory length: 683120   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2919   score: 6.0   memory length: 683474   epsilon: 0.009998020008555413    steps: 354    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 2920   score: 9.0   memory length: 683780   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 2921   score: 3.0   memory length: 684008   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 2922   score: 5.0   memory length: 684295   epsilon: 0.009998020008555413    steps: 287    lr: 0.0001     evaluation reward: 5.42\n",
      "episode: 2923   score: 4.0   memory length: 684574   epsilon: 0.009998020008555413    steps: 279    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 2924   score: 4.0   memory length: 684817   epsilon: 0.009998020008555413    steps: 243    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 2925   score: 3.0   memory length: 685029   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 2926   score: 3.0   memory length: 685241   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 2927   score: 3.0   memory length: 685454   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 2928   score: 9.0   memory length: 685878   epsilon: 0.009998020008555413    steps: 424    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 2929   score: 5.0   memory length: 686186   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 2930   score: 3.0   memory length: 686416   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.32\n",
      "episode: 2931   score: 3.0   memory length: 686644   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 2932   score: 3.0   memory length: 686856   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 2933   score: 3.0   memory length: 687086   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 2934   score: 6.0   memory length: 687428   epsilon: 0.009998020008555413    steps: 342    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 2935   score: 3.0   memory length: 687655   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 2936   score: 7.0   memory length: 688012   epsilon: 0.009998020008555413    steps: 357    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 2937   score: 3.0   memory length: 688224   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 2938   score: 6.0   memory length: 688582   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 2939   score: 3.0   memory length: 688795   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 2940   score: 4.0   memory length: 689053   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2941   score: 4.0   memory length: 689315   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2942   score: 3.0   memory length: 689528   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2943   score: 3.0   memory length: 689739   epsilon: 0.009998020008555413    steps: 211    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 2944   score: 4.0   memory length: 689999   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2945   score: 6.0   memory length: 690360   epsilon: 0.009998020008555413    steps: 361    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2946   score: 4.0   memory length: 690599   epsilon: 0.009998020008555413    steps: 239    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2947   score: 3.0   memory length: 690827   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2948   score: 8.0   memory length: 691258   epsilon: 0.009998020008555413    steps: 431    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2949   score: 4.0   memory length: 691518   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2950   score: 7.0   memory length: 691869   epsilon: 0.009998020008555413    steps: 351    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2951   score: 4.0   memory length: 692128   epsilon: 0.009998020008555413    steps: 259    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2952   score: 3.0   memory length: 692341   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2953   score: 4.0   memory length: 692585   epsilon: 0.009998020008555413    steps: 244    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2954   score: 4.0   memory length: 692861   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2955   score: 4.0   memory length: 693138   epsilon: 0.009998020008555413    steps: 277    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2956   score: 4.0   memory length: 693380   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2957   score: 3.0   memory length: 693589   epsilon: 0.009998020008555413    steps: 209    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2958   score: 11.0   memory length: 693967   epsilon: 0.009998020008555413    steps: 378    lr: 0.0001     evaluation reward: 5.01\n",
      "episode: 2959   score: 6.0   memory length: 694321   epsilon: 0.009998020008555413    steps: 354    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2960   score: 3.0   memory length: 694549   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2961   score: 4.0   memory length: 694829   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2962   score: 6.0   memory length: 695152   epsilon: 0.009998020008555413    steps: 323    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2963   score: 6.0   memory length: 695495   epsilon: 0.009998020008555413    steps: 343    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2964   score: 6.0   memory length: 695832   epsilon: 0.009998020008555413    steps: 337    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2965   score: 6.0   memory length: 696136   epsilon: 0.009998020008555413    steps: 304    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2966   score: 8.0   memory length: 696559   epsilon: 0.009998020008555413    steps: 423    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2967   score: 7.0   memory length: 696929   epsilon: 0.009998020008555413    steps: 370    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2968   score: 7.0   memory length: 697319   epsilon: 0.009998020008555413    steps: 390    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 2969   score: 3.0   memory length: 697532   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 2970   score: 7.0   memory length: 697892   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2971   score: 4.0   memory length: 698134   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2972   score: 3.0   memory length: 698346   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2973   score: 6.0   memory length: 698666   epsilon: 0.009998020008555413    steps: 320    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 2974   score: 3.0   memory length: 698878   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.03\n",
      "episode: 2975   score: 3.0   memory length: 699090   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2976   score: 4.0   memory length: 699350   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2977   score: 9.0   memory length: 699820   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2978   score: 4.0   memory length: 700080   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2979   score: 3.0   memory length: 700293   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 2980   score: 6.0   memory length: 700653   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 2981   score: 9.0   memory length: 701135   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 2982   score: 6.0   memory length: 701455   epsilon: 0.009998020008555413    steps: 320    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2983   score: 7.0   memory length: 701845   epsilon: 0.009998020008555413    steps: 390    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2984   score: 7.0   memory length: 702207   epsilon: 0.009998020008555413    steps: 362    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 2985   score: 3.0   memory length: 702420   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 2986   score: 9.0   memory length: 702783   epsilon: 0.009998020008555413    steps: 363    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2987   score: 7.0   memory length: 703149   epsilon: 0.009998020008555413    steps: 366    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 2988   score: 5.0   memory length: 703455   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 2989   score: 5.0   memory length: 703780   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 2990   score: 7.0   memory length: 704184   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2991   score: 4.0   memory length: 704426   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 2992   score: 8.0   memory length: 704883   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 2993   score: 5.0   memory length: 705210   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 2994   score: 5.0   memory length: 705501   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 2995   score: 9.0   memory length: 705983   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 2996   score: 5.0   memory length: 706256   epsilon: 0.009998020008555413    steps: 273    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 2997   score: 4.0   memory length: 706497   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 2998   score: 3.0   memory length: 706710   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 2999   score: 5.0   memory length: 706999   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 5.07\n",
      "episode: 3000   score: 5.0   memory length: 707306   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 5.05\n",
      "episode: 3001   score: 5.0   memory length: 707595   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 5.06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3002   score: 7.0   memory length: 707984   epsilon: 0.009998020008555413    steps: 389    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 3003   score: 5.0   memory length: 708275   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 3004   score: 3.0   memory length: 708488   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 3005   score: 7.0   memory length: 708856   epsilon: 0.009998020008555413    steps: 368    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 3006   score: 3.0   memory length: 709069   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 3007   score: 3.0   memory length: 709282   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 3008   score: 3.0   memory length: 709495   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 3009   score: 3.0   memory length: 709708   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.04\n",
      "episode: 3010   score: 3.0   memory length: 709937   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 3011   score: 8.0   memory length: 710358   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 3012   score: 3.0   memory length: 710569   epsilon: 0.009998020008555413    steps: 211    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 3013   score: 6.0   memory length: 710943   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 3014   score: 4.0   memory length: 711185   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 4.94\n",
      "episode: 3015   score: 4.0   memory length: 711446   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 4.95\n",
      "episode: 3016   score: 5.0   memory length: 711737   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 4.92\n",
      "episode: 3017   score: 10.0   memory length: 712218   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 3018   score: 15.0   memory length: 712769   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 3019   score: 9.0   memory length: 713198   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 5.13\n",
      "episode: 3020   score: 4.0   memory length: 713459   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 5.08\n",
      "episode: 3021   score: 7.0   memory length: 713861   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 5.12\n",
      "episode: 3022   score: 3.0   memory length: 714074   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 3023   score: 4.0   memory length: 714334   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.1\n",
      "episode: 3024   score: 5.0   memory length: 714661   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 5.11\n",
      "episode: 3025   score: 12.0   memory length: 715137   epsilon: 0.009998020008555413    steps: 476    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 3026   score: 4.0   memory length: 715379   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 3027   score: 4.0   memory length: 715636   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.22\n",
      "episode: 3028   score: 3.0   memory length: 715846   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 3029   score: 3.0   memory length: 716076   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.14\n",
      "episode: 3030   score: 5.0   memory length: 716381   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 5.16\n",
      "episode: 3031   score: 7.0   memory length: 716773   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 3032   score: 4.0   memory length: 717068   epsilon: 0.009998020008555413    steps: 295    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 3033   score: 6.0   memory length: 717389   epsilon: 0.009998020008555413    steps: 321    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 3034   score: 7.0   memory length: 717765   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 3035   score: 5.0   memory length: 718058   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 3036   score: 6.0   memory length: 718399   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 3037   score: 6.0   memory length: 718718   epsilon: 0.009998020008555413    steps: 319    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3038   score: 5.0   memory length: 719025   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3039   score: 4.0   memory length: 719267   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3040   score: 8.0   memory length: 719705   epsilon: 0.009998020008555413    steps: 438    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 3041   score: 6.0   memory length: 720080   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 3042   score: 5.0   memory length: 720371   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3043   score: 6.0   memory length: 720728   epsilon: 0.009998020008555413    steps: 357    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 3044   score: 4.0   memory length: 720969   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 3045   score: 3.0   memory length: 721182   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3046   score: 4.0   memory length: 721424   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3047   score: 3.0   memory length: 721653   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3048   score: 5.0   memory length: 721964   epsilon: 0.009998020008555413    steps: 311    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 3049   score: 7.0   memory length: 722341   epsilon: 0.009998020008555413    steps: 377    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3050   score: 4.0   memory length: 722619   epsilon: 0.009998020008555413    steps: 278    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 3051   score: 6.0   memory length: 722959   epsilon: 0.009998020008555413    steps: 340    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 3052   score: 3.0   memory length: 723172   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 3053   score: 4.0   memory length: 723450   epsilon: 0.009998020008555413    steps: 278    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 3054   score: 4.0   memory length: 723726   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 3055   score: 5.0   memory length: 724018   epsilon: 0.009998020008555413    steps: 292    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3056   score: 5.0   memory length: 724325   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 3057   score: 8.0   memory length: 724746   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 3058   score: 7.0   memory length: 725139   epsilon: 0.009998020008555413    steps: 393    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 3059   score: 4.0   memory length: 725417   epsilon: 0.009998020008555413    steps: 278    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3060   score: 4.0   memory length: 725656   epsilon: 0.009998020008555413    steps: 239    lr: 0.0001     evaluation reward: 5.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3061   score: 4.0   memory length: 725914   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 3062   score: 3.0   memory length: 726144   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 3063   score: 4.0   memory length: 726403   epsilon: 0.009998020008555413    steps: 259    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 3064   score: 5.0   memory length: 726694   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 5.32\n",
      "episode: 3065   score: 15.0   memory length: 727278   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3066   score: 4.0   memory length: 727519   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3067   score: 5.0   memory length: 727844   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 3068   score: 6.0   memory length: 728185   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 3069   score: 10.0   memory length: 728666   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3070   score: 3.0   memory length: 728894   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3071   score: 6.0   memory length: 729240   epsilon: 0.009998020008555413    steps: 346    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 3072   score: 9.0   memory length: 729742   epsilon: 0.009998020008555413    steps: 502    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3073   score: 7.0   memory length: 730095   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 3074   score: 8.0   memory length: 730515   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 3075   score: 6.0   memory length: 730833   epsilon: 0.009998020008555413    steps: 318    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 3076   score: 3.0   memory length: 731046   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 3077   score: 3.0   memory length: 731258   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 3078   score: 3.0   memory length: 731471   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 3079   score: 4.0   memory length: 731732   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 3080   score: 8.0   memory length: 732189   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 3081   score: 4.0   memory length: 732452   epsilon: 0.009998020008555413    steps: 263    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 3082   score: 7.0   memory length: 732829   epsilon: 0.009998020008555413    steps: 377    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3083   score: 3.0   memory length: 733059   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3084   score: 3.0   memory length: 733287   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3085   score: 4.0   memory length: 733549   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 3086   score: 4.0   memory length: 733825   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 3087   score: 3.0   memory length: 734037   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3088   score: 4.0   memory length: 734319   epsilon: 0.009998020008555413    steps: 282    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3089   score: 5.0   memory length: 734644   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3090   score: 3.0   memory length: 734857   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 3091   score: 8.0   memory length: 735291   epsilon: 0.009998020008555413    steps: 434    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3092   score: 4.0   memory length: 735584   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 3093   score: 4.0   memory length: 735843   epsilon: 0.009998020008555413    steps: 259    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 3094   score: 8.0   memory length: 736238   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 3095   score: 10.0   memory length: 736691   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 3096   score: 4.0   memory length: 736934   epsilon: 0.009998020008555413    steps: 243    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 3097   score: 3.0   memory length: 737147   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 3098   score: 7.0   memory length: 737523   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3099   score: 4.0   memory length: 737785   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3100   score: 4.0   memory length: 738047   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 3101   score: 6.0   memory length: 738397   epsilon: 0.009998020008555413    steps: 350    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3102   score: 9.0   memory length: 738705   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 3103   score: 7.0   memory length: 739093   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 5.32\n",
      "episode: 3104   score: 4.0   memory length: 739353   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 3105   score: 7.0   memory length: 739708   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 3106   score: 7.0   memory length: 740083   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3107   score: 6.0   memory length: 740422   epsilon: 0.009998020008555413    steps: 339    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 3108   score: 4.0   memory length: 740664   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3109   score: 3.0   memory length: 740893   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3110   score: 3.0   memory length: 741123   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3111   score: 5.0   memory length: 741450   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 3112   score: 3.0   memory length: 741663   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 3113   score: 11.0   memory length: 742179   epsilon: 0.009998020008555413    steps: 516    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 3114   score: 12.0   memory length: 742715   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 3115   score: 5.0   memory length: 743027   epsilon: 0.009998020008555413    steps: 312    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 3116   score: 6.0   memory length: 743386   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 5.53\n",
      "episode: 3117   score: 4.0   memory length: 743648   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 3118   score: 9.0   memory length: 743994   epsilon: 0.009998020008555413    steps: 346    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3119   score: 6.0   memory length: 744335   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 5.38\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3120   score: 4.0   memory length: 744610   epsilon: 0.009998020008555413    steps: 275    lr: 0.0001     evaluation reward: 5.38\n",
      "episode: 3121   score: 5.0   memory length: 744920   epsilon: 0.009998020008555413    steps: 310    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 3122   score: 8.0   memory length: 745356   epsilon: 0.009998020008555413    steps: 436    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3123   score: 6.0   memory length: 745668   epsilon: 0.009998020008555413    steps: 312    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 3124   score: 8.0   memory length: 746076   epsilon: 0.009998020008555413    steps: 408    lr: 0.0001     evaluation reward: 5.46\n",
      "episode: 3125   score: 3.0   memory length: 746289   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3126   score: 4.0   memory length: 746549   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3127   score: 8.0   memory length: 746969   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3128   score: 3.0   memory length: 747182   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3129   score: 3.0   memory length: 747412   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3130   score: 5.0   memory length: 747721   epsilon: 0.009998020008555413    steps: 309    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3131   score: 3.0   memory length: 747934   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3132   score: 4.0   memory length: 748215   epsilon: 0.009998020008555413    steps: 281    lr: 0.0001     evaluation reward: 5.37\n",
      "episode: 3133   score: 3.0   memory length: 748427   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 3134   score: 3.0   memory length: 748657   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 3135   score: 6.0   memory length: 748996   epsilon: 0.009998020008555413    steps: 339    lr: 0.0001     evaluation reward: 5.31\n",
      "episode: 3136   score: 3.0   memory length: 749208   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3137   score: 4.0   memory length: 749450   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 3138   score: 3.0   memory length: 749660   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 3139   score: 3.0   memory length: 749873   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 3140   score: 3.0   memory length: 750085   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 3141   score: 6.0   memory length: 750420   epsilon: 0.009998020008555413    steps: 335    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 3142   score: 8.0   memory length: 750860   epsilon: 0.009998020008555413    steps: 440    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 3143   score: 6.0   memory length: 751196   epsilon: 0.009998020008555413    steps: 336    lr: 0.0001     evaluation reward: 5.21\n",
      "episode: 3144   score: 3.0   memory length: 751425   epsilon: 0.009998020008555413    steps: 229    lr: 0.0001     evaluation reward: 5.2\n",
      "episode: 3145   score: 7.0   memory length: 751805   epsilon: 0.009998020008555413    steps: 380    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 3146   score: 4.0   memory length: 752085   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 3147   score: 4.0   memory length: 752345   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 3148   score: 5.0   memory length: 752640   epsilon: 0.009998020008555413    steps: 295    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 3149   score: 7.0   memory length: 753038   epsilon: 0.009998020008555413    steps: 398    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 3150   score: 9.0   memory length: 753491   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 3151   score: 5.0   memory length: 753780   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3152   score: 4.0   memory length: 754037   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 3153   score: 3.0   memory length: 754250   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3154   score: 9.0   memory length: 754720   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 5.34\n",
      "episode: 3155   score: 3.0   memory length: 754932   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.32\n",
      "episode: 3156   score: 6.0   memory length: 755328   epsilon: 0.009998020008555413    steps: 396    lr: 0.0001     evaluation reward: 5.33\n",
      "episode: 3157   score: 3.0   memory length: 755541   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 5.28\n",
      "episode: 3158   score: 8.0   memory length: 755970   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 3159   score: 5.0   memory length: 756260   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 3160   score: 9.0   memory length: 756744   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 3161   score: 5.0   memory length: 757032   epsilon: 0.009998020008555413    steps: 288    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 3162   score: 6.0   memory length: 757335   epsilon: 0.009998020008555413    steps: 303    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 3163   score: 5.0   memory length: 757623   epsilon: 0.009998020008555413    steps: 288    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 3164   score: 10.0   memory length: 758175   epsilon: 0.009998020008555413    steps: 552    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3165   score: 5.0   memory length: 758465   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 5.35\n",
      "episode: 3166   score: 10.0   memory length: 758952   epsilon: 0.009998020008555413    steps: 487    lr: 0.0001     evaluation reward: 5.41\n",
      "episode: 3167   score: 8.0   memory length: 759364   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 3168   score: 7.0   memory length: 759755   epsilon: 0.009998020008555413    steps: 391    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3169   score: 7.0   memory length: 760138   epsilon: 0.009998020008555413    steps: 383    lr: 0.0001     evaluation reward: 5.42\n",
      "episode: 3170   score: 6.0   memory length: 760491   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3171   score: 11.0   memory length: 761056   epsilon: 0.009998020008555413    steps: 565    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 3172   score: 6.0   memory length: 761409   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 5.47\n",
      "episode: 3173   score: 3.0   memory length: 761659   epsilon: 0.009998020008555413    steps: 250    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 3174   score: 5.0   memory length: 761931   epsilon: 0.009998020008555413    steps: 272    lr: 0.0001     evaluation reward: 5.4\n",
      "episode: 3175   score: 5.0   memory length: 762239   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 5.39\n",
      "episode: 3176   score: 9.0   memory length: 762675   epsilon: 0.009998020008555413    steps: 436    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3177   score: 3.0   memory length: 762887   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 3178   score: 7.0   memory length: 763252   epsilon: 0.009998020008555413    steps: 365    lr: 0.0001     evaluation reward: 5.49\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3179   score: 5.0   memory length: 763542   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 3180   score: 10.0   memory length: 764068   epsilon: 0.009998020008555413    steps: 526    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 3181   score: 7.0   memory length: 764470   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 5.55\n",
      "episode: 3182   score: 8.0   memory length: 764891   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 5.56\n",
      "episode: 3183   score: 9.0   memory length: 765345   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 5.62\n",
      "episode: 3184   score: 11.0   memory length: 765863   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 5.7\n",
      "episode: 3185   score: 8.0   memory length: 766277   epsilon: 0.009998020008555413    steps: 414    lr: 0.0001     evaluation reward: 5.74\n",
      "episode: 3186   score: 4.0   memory length: 766534   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 5.74\n",
      "episode: 3187   score: 10.0   memory length: 767042   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 5.81\n",
      "episode: 3188   score: 4.0   memory length: 767322   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 5.81\n",
      "episode: 3189   score: 4.0   memory length: 767617   epsilon: 0.009998020008555413    steps: 295    lr: 0.0001     evaluation reward: 5.8\n",
      "episode: 3190   score: 5.0   memory length: 767910   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 5.82\n",
      "episode: 3191   score: 8.0   memory length: 768382   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 5.82\n",
      "episode: 3192   score: 5.0   memory length: 768675   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 5.83\n",
      "episode: 3193   score: 12.0   memory length: 769239   epsilon: 0.009998020008555413    steps: 564    lr: 0.0001     evaluation reward: 5.91\n",
      "episode: 3194   score: 7.0   memory length: 769641   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 3195   score: 6.0   memory length: 770020   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 5.86\n",
      "episode: 3196   score: 7.0   memory length: 770426   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 3197   score: 9.0   memory length: 770878   epsilon: 0.009998020008555413    steps: 452    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 3198   score: 7.0   memory length: 771258   epsilon: 0.009998020008555413    steps: 380    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 3199   score: 10.0   memory length: 771599   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 3200   score: 5.0   memory length: 771912   epsilon: 0.009998020008555413    steps: 313    lr: 0.0001     evaluation reward: 6.02\n",
      "episode: 3201   score: 4.0   memory length: 772174   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 6.0\n",
      "episode: 3202   score: 7.0   memory length: 772559   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 5.98\n",
      "episode: 3203   score: 5.0   memory length: 772887   epsilon: 0.009998020008555413    steps: 328    lr: 0.0001     evaluation reward: 5.96\n",
      "episode: 3204   score: 4.0   memory length: 773148   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 5.96\n",
      "episode: 3205   score: 12.0   memory length: 773691   epsilon: 0.009998020008555413    steps: 543    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 3206   score: 9.0   memory length: 774104   epsilon: 0.009998020008555413    steps: 413    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 3207   score: 4.0   memory length: 774364   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 3208   score: 5.0   memory length: 774636   epsilon: 0.009998020008555413    steps: 272    lr: 0.0001     evaluation reward: 6.02\n",
      "episode: 3209   score: 4.0   memory length: 774916   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 3210   score: 11.0   memory length: 775359   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 3211   score: 8.0   memory length: 775757   epsilon: 0.009998020008555413    steps: 398    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 3212   score: 5.0   memory length: 776064   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 3213   score: 4.0   memory length: 776306   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 3214   score: 4.0   memory length: 776548   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 3215   score: 10.0   memory length: 777086   epsilon: 0.009998020008555413    steps: 538    lr: 0.0001     evaluation reward: 6.06\n",
      "episode: 3216   score: 7.0   memory length: 777445   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 3217   score: 5.0   memory length: 777754   epsilon: 0.009998020008555413    steps: 309    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 3218   score: 10.0   memory length: 778146   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 3219   score: 6.0   memory length: 778522   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 3220   score: 5.0   memory length: 778831   epsilon: 0.009998020008555413    steps: 309    lr: 0.0001     evaluation reward: 6.1\n",
      "episode: 3221   score: 12.0   memory length: 779304   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 6.17\n",
      "episode: 3222   score: 7.0   memory length: 779691   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 3223   score: 3.0   memory length: 779921   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 6.13\n",
      "episode: 3224   score: 7.0   memory length: 780321   epsilon: 0.009998020008555413    steps: 400    lr: 0.0001     evaluation reward: 6.12\n",
      "episode: 3225   score: 5.0   memory length: 780595   epsilon: 0.009998020008555413    steps: 274    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 3226   score: 9.0   memory length: 780919   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 6.19\n",
      "episode: 3227   score: 6.0   memory length: 781237   epsilon: 0.009998020008555413    steps: 318    lr: 0.0001     evaluation reward: 6.17\n",
      "episode: 3228   score: 7.0   memory length: 781603   epsilon: 0.009998020008555413    steps: 366    lr: 0.0001     evaluation reward: 6.21\n",
      "episode: 3229   score: 5.0   memory length: 781929   epsilon: 0.009998020008555413    steps: 326    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 3230   score: 14.0   memory length: 782484   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 6.32\n",
      "episode: 3231   score: 6.0   memory length: 782840   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 6.35\n",
      "episode: 3232   score: 4.0   memory length: 783082   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 6.35\n",
      "episode: 3233   score: 8.0   memory length: 783502   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 6.4\n",
      "episode: 3234   score: 9.0   memory length: 783900   epsilon: 0.009998020008555413    steps: 398    lr: 0.0001     evaluation reward: 6.46\n",
      "episode: 3235   score: 11.0   memory length: 784304   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 6.51\n",
      "episode: 3236   score: 10.0   memory length: 784807   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 6.58\n",
      "episode: 3237   score: 5.0   memory length: 785101   epsilon: 0.009998020008555413    steps: 294    lr: 0.0001     evaluation reward: 6.59\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3238   score: 4.0   memory length: 785360   epsilon: 0.009998020008555413    steps: 259    lr: 0.0001     evaluation reward: 6.6\n",
      "episode: 3239   score: 5.0   memory length: 785666   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 6.62\n",
      "episode: 3240   score: 5.0   memory length: 785957   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 3241   score: 9.0   memory length: 786396   epsilon: 0.009998020008555413    steps: 439    lr: 0.0001     evaluation reward: 6.67\n",
      "episode: 3242   score: 5.0   memory length: 786690   epsilon: 0.009998020008555413    steps: 294    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 3243   score: 12.0   memory length: 787283   epsilon: 0.009998020008555413    steps: 593    lr: 0.0001     evaluation reward: 6.7\n",
      "episode: 3244   score: 7.0   memory length: 787687   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 6.74\n",
      "episode: 3245   score: 10.0   memory length: 788216   epsilon: 0.009998020008555413    steps: 529    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 3246   score: 10.0   memory length: 788719   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 6.83\n",
      "episode: 3247   score: 14.0   memory length: 789239   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 6.93\n",
      "episode: 3248   score: 9.0   memory length: 789659   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 6.97\n",
      "episode: 3249   score: 7.0   memory length: 790066   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 6.97\n",
      "episode: 3250   score: 6.0   memory length: 790395   epsilon: 0.009998020008555413    steps: 329    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 3251   score: 4.0   memory length: 790634   epsilon: 0.009998020008555413    steps: 239    lr: 0.0001     evaluation reward: 6.93\n",
      "episode: 3252   score: 11.0   memory length: 791202   epsilon: 0.009998020008555413    steps: 568    lr: 0.0001     evaluation reward: 7.0\n",
      "episode: 3253   score: 8.0   memory length: 791623   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 3254   score: 8.0   memory length: 792010   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 7.04\n",
      "episode: 3255   score: 10.0   memory length: 792384   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 3256   score: 7.0   memory length: 792760   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 7.12\n",
      "episode: 3257   score: 13.0   memory length: 793364   epsilon: 0.009998020008555413    steps: 604    lr: 0.0001     evaluation reward: 7.22\n",
      "episode: 3258   score: 8.0   memory length: 793793   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 7.22\n",
      "episode: 3259   score: 9.0   memory length: 794246   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 7.26\n",
      "episode: 3260   score: 14.0   memory length: 794752   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 7.31\n",
      "episode: 3261   score: 7.0   memory length: 795142   epsilon: 0.009998020008555413    steps: 390    lr: 0.0001     evaluation reward: 7.33\n",
      "episode: 3262   score: 8.0   memory length: 795598   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3263   score: 9.0   memory length: 795926   epsilon: 0.009998020008555413    steps: 328    lr: 0.0001     evaluation reward: 7.39\n",
      "episode: 3264   score: 9.0   memory length: 796398   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 7.38\n",
      "episode: 3265   score: 5.0   memory length: 796706   epsilon: 0.009998020008555413    steps: 308    lr: 0.0001     evaluation reward: 7.38\n",
      "episode: 3266   score: 7.0   memory length: 797108   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3267   score: 6.0   memory length: 797447   epsilon: 0.009998020008555413    steps: 339    lr: 0.0001     evaluation reward: 7.33\n",
      "episode: 3268   score: 4.0   memory length: 797705   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 7.3\n",
      "episode: 3269   score: 14.0   memory length: 798314   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 7.37\n",
      "episode: 3270   score: 4.0   memory length: 798607   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3271   score: 5.0   memory length: 798896   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 3272   score: 4.0   memory length: 799156   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3273   score: 6.0   memory length: 799484   epsilon: 0.009998020008555413    steps: 328    lr: 0.0001     evaluation reward: 7.3\n",
      "episode: 3274   score: 6.0   memory length: 799819   epsilon: 0.009998020008555413    steps: 335    lr: 0.0001     evaluation reward: 7.31\n",
      "episode: 3275   score: 9.0   memory length: 800288   epsilon: 0.009998020008555413    steps: 469    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3276   score: 7.0   memory length: 800671   epsilon: 0.009998020008555413    steps: 383    lr: 0.0001     evaluation reward: 7.33\n",
      "episode: 3277   score: 6.0   memory length: 801002   epsilon: 0.009998020008555413    steps: 331    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 3278   score: 7.0   memory length: 801445   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 3279   score: 4.0   memory length: 801721   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3280   score: 3.0   memory length: 801949   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 7.28\n",
      "episode: 3281   score: 8.0   memory length: 802362   epsilon: 0.009998020008555413    steps: 413    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 3282   score: 5.0   memory length: 802653   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 7.26\n",
      "episode: 3283   score: 6.0   memory length: 803026   epsilon: 0.009998020008555413    steps: 373    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 3284   score: 6.0   memory length: 803399   epsilon: 0.009998020008555413    steps: 373    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 3285   score: 11.0   memory length: 803929   epsilon: 0.009998020008555413    steps: 530    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3286   score: 4.0   memory length: 804171   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3287   score: 3.0   memory length: 804398   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3288   score: 11.0   memory length: 804827   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3289   score: 4.0   memory length: 805085   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3290   score: 7.0   memory length: 805460   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 3291   score: 7.0   memory length: 805884   epsilon: 0.009998020008555413    steps: 424    lr: 0.0001     evaluation reward: 7.22\n",
      "episode: 3292   score: 3.0   memory length: 806097   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 3293   score: 7.0   memory length: 806457   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 7.15\n",
      "episode: 3294   score: 10.0   memory length: 806934   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 3295   score: 3.0   memory length: 807161   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 7.15\n",
      "episode: 3296   score: 4.0   memory length: 807403   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 7.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3297   score: 6.0   memory length: 807759   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 7.09\n",
      "episode: 3298   score: 5.0   memory length: 808048   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3299   score: 9.0   memory length: 808503   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3300   score: 12.0   memory length: 809066   epsilon: 0.009998020008555413    steps: 563    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 3301   score: 9.0   memory length: 809538   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 3302   score: 5.0   memory length: 809840   epsilon: 0.009998020008555413    steps: 302    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 3303   score: 3.0   memory length: 810053   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3304   score: 4.0   memory length: 810315   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3305   score: 6.0   memory length: 810679   epsilon: 0.009998020008555413    steps: 364    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 3306   score: 7.0   memory length: 811022   epsilon: 0.009998020008555413    steps: 343    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3307   score: 3.0   memory length: 811268   epsilon: 0.009998020008555413    steps: 246    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 3308   score: 11.0   memory length: 811792   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 3309   score: 3.0   memory length: 812005   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 3310   score: 8.0   memory length: 812437   epsilon: 0.009998020008555413    steps: 432    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3311   score: 5.0   memory length: 812753   epsilon: 0.009998020008555413    steps: 316    lr: 0.0001     evaluation reward: 7.04\n",
      "episode: 3312   score: 7.0   memory length: 813182   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3313   score: 10.0   memory length: 813699   epsilon: 0.009998020008555413    steps: 517    lr: 0.0001     evaluation reward: 7.12\n",
      "episode: 3314   score: 6.0   memory length: 814059   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3315   score: 9.0   memory length: 814512   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 3316   score: 4.0   memory length: 814788   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 3317   score: 6.0   memory length: 815144   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 3318   score: 6.0   memory length: 815498   epsilon: 0.009998020008555413    steps: 354    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3319   score: 6.0   memory length: 815828   epsilon: 0.009998020008555413    steps: 330    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3320   score: 4.0   memory length: 816085   epsilon: 0.009998020008555413    steps: 257    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3321   score: 7.0   memory length: 816462   epsilon: 0.009998020008555413    steps: 377    lr: 0.0001     evaluation reward: 7.01\n",
      "episode: 3322   score: 7.0   memory length: 816806   epsilon: 0.009998020008555413    steps: 344    lr: 0.0001     evaluation reward: 7.01\n",
      "episode: 3323   score: 8.0   memory length: 817198   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3324   score: 12.0   memory length: 817648   epsilon: 0.009998020008555413    steps: 450    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 3325   score: 4.0   memory length: 817928   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 3326   score: 6.0   memory length: 818304   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3327   score: 9.0   memory length: 818808   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 3328   score: 4.0   memory length: 819070   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3329   score: 3.0   memory length: 819282   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 3330   score: 7.0   memory length: 819689   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 6.98\n",
      "episode: 3331   score: 11.0   memory length: 820239   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 7.03\n",
      "episode: 3332   score: 4.0   memory length: 820481   epsilon: 0.009998020008555413    steps: 242    lr: 0.0001     evaluation reward: 7.03\n",
      "episode: 3333   score: 6.0   memory length: 820836   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 7.01\n",
      "episode: 3334   score: 6.0   memory length: 821140   epsilon: 0.009998020008555413    steps: 304    lr: 0.0001     evaluation reward: 6.98\n",
      "episode: 3335   score: 8.0   memory length: 821577   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 6.95\n",
      "episode: 3336   score: 16.0   memory length: 822200   epsilon: 0.009998020008555413    steps: 623    lr: 0.0001     evaluation reward: 7.01\n",
      "episode: 3337   score: 8.0   memory length: 822605   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 7.04\n",
      "episode: 3338   score: 10.0   memory length: 823013   epsilon: 0.009998020008555413    steps: 408    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 3339   score: 9.0   memory length: 823523   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3340   score: 6.0   memory length: 823843   epsilon: 0.009998020008555413    steps: 320    lr: 0.0001     evaluation reward: 7.15\n",
      "episode: 3341   score: 7.0   memory length: 824226   epsilon: 0.009998020008555413    steps: 383    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 3342   score: 7.0   memory length: 824589   epsilon: 0.009998020008555413    steps: 363    lr: 0.0001     evaluation reward: 7.15\n",
      "episode: 3343   score: 10.0   memory length: 825098   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 3344   score: 5.0   memory length: 825425   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 3345   score: 5.0   memory length: 825730   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3346   score: 7.0   memory length: 826136   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 7.03\n",
      "episode: 3347   score: 7.0   memory length: 826517   epsilon: 0.009998020008555413    steps: 381    lr: 0.0001     evaluation reward: 6.96\n",
      "episode: 3348   score: 4.0   memory length: 826795   epsilon: 0.009998020008555413    steps: 278    lr: 0.0001     evaluation reward: 6.91\n",
      "episode: 3349   score: 7.0   memory length: 827168   epsilon: 0.009998020008555413    steps: 373    lr: 0.0001     evaluation reward: 6.91\n",
      "episode: 3350   score: 7.0   memory length: 827547   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 6.92\n",
      "episode: 3351   score: 11.0   memory length: 828138   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 6.99\n",
      "episode: 3352   score: 3.0   memory length: 828351   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 6.91\n",
      "episode: 3353   score: 3.0   memory length: 828564   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 6.86\n",
      "episode: 3354   score: 5.0   memory length: 828888   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 6.83\n",
      "episode: 3355   score: 3.0   memory length: 829118   epsilon: 0.009998020008555413    steps: 230    lr: 0.0001     evaluation reward: 6.76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3356   score: 3.0   memory length: 829331   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 6.72\n",
      "episode: 3357   score: 9.0   memory length: 829817   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 3358   score: 4.0   memory length: 830058   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 3359   score: 15.0   memory length: 830648   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 6.7\n",
      "episode: 3360   score: 3.0   memory length: 830860   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 6.59\n",
      "episode: 3361   score: 4.0   memory length: 831116   epsilon: 0.009998020008555413    steps: 256    lr: 0.0001     evaluation reward: 6.56\n",
      "episode: 3362   score: 6.0   memory length: 831457   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 6.54\n",
      "episode: 3363   score: 12.0   memory length: 831908   epsilon: 0.009998020008555413    steps: 451    lr: 0.0001     evaluation reward: 6.57\n",
      "episode: 3364   score: 8.0   memory length: 832345   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 6.56\n",
      "episode: 3365   score: 9.0   memory length: 832802   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 6.6\n",
      "episode: 3366   score: 12.0   memory length: 833256   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 6.65\n",
      "episode: 3367   score: 7.0   memory length: 833661   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 6.66\n",
      "episode: 3368   score: 9.0   memory length: 834094   epsilon: 0.009998020008555413    steps: 433    lr: 0.0001     evaluation reward: 6.71\n",
      "episode: 3369   score: 9.0   memory length: 834548   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 6.66\n",
      "episode: 3370   score: 6.0   memory length: 834925   epsilon: 0.009998020008555413    steps: 377    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 3371   score: 5.0   memory length: 835216   epsilon: 0.009998020008555413    steps: 291    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 3372   score: 5.0   memory length: 835505   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 6.69\n",
      "episode: 3373   score: 10.0   memory length: 836008   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 6.73\n",
      "episode: 3374   score: 8.0   memory length: 836303   epsilon: 0.009998020008555413    steps: 295    lr: 0.0001     evaluation reward: 6.75\n",
      "episode: 3375   score: 7.0   memory length: 836689   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 6.73\n",
      "episode: 3376   score: 6.0   memory length: 837044   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 6.72\n",
      "episode: 3377   score: 5.0   memory length: 837331   epsilon: 0.009998020008555413    steps: 287    lr: 0.0001     evaluation reward: 6.71\n",
      "episode: 3378   score: 5.0   memory length: 837619   epsilon: 0.009998020008555413    steps: 288    lr: 0.0001     evaluation reward: 6.69\n",
      "episode: 3379   score: 6.0   memory length: 837974   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 6.71\n",
      "episode: 3380   score: 9.0   memory length: 838418   epsilon: 0.009998020008555413    steps: 444    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 3381   score: 8.0   memory length: 838876   epsilon: 0.009998020008555413    steps: 458    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 3382   score: 4.0   memory length: 839152   epsilon: 0.009998020008555413    steps: 276    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 3383   score: 6.0   memory length: 839528   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 3384   score: 6.0   memory length: 839864   epsilon: 0.009998020008555413    steps: 336    lr: 0.0001     evaluation reward: 6.76\n",
      "episode: 3385   score: 3.0   memory length: 840074   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 3386   score: 3.0   memory length: 840302   epsilon: 0.009998020008555413    steps: 228    lr: 0.0001     evaluation reward: 6.67\n",
      "episode: 3387   score: 5.0   memory length: 840605   epsilon: 0.009998020008555413    steps: 303    lr: 0.0001     evaluation reward: 6.69\n",
      "episode: 3388   score: 6.0   memory length: 840946   epsilon: 0.009998020008555413    steps: 341    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 3389   score: 10.0   memory length: 841455   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 6.7\n",
      "episode: 3390   score: 5.0   memory length: 841742   epsilon: 0.009998020008555413    steps: 287    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 3391   score: 7.0   memory length: 842146   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 3392   score: 9.0   memory length: 842512   epsilon: 0.009998020008555413    steps: 366    lr: 0.0001     evaluation reward: 6.74\n",
      "episode: 3393   score: 8.0   memory length: 842968   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 6.75\n",
      "episode: 3394   score: 8.0   memory length: 843383   epsilon: 0.009998020008555413    steps: 415    lr: 0.0001     evaluation reward: 6.73\n",
      "episode: 3395   score: 9.0   memory length: 843811   epsilon: 0.009998020008555413    steps: 428    lr: 0.0001     evaluation reward: 6.79\n",
      "episode: 3396   score: 7.0   memory length: 844216   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 6.82\n",
      "episode: 3397   score: 12.0   memory length: 844719   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 6.88\n",
      "episode: 3398   score: 4.0   memory length: 844981   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 6.87\n",
      "episode: 3399   score: 7.0   memory length: 845364   epsilon: 0.009998020008555413    steps: 383    lr: 0.0001     evaluation reward: 6.85\n",
      "episode: 3400   score: 6.0   memory length: 845720   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 6.79\n",
      "episode: 3401   score: 11.0   memory length: 846240   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 6.81\n",
      "episode: 3402   score: 4.0   memory length: 846481   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 6.8\n",
      "episode: 3403   score: 10.0   memory length: 846935   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 6.87\n",
      "episode: 3404   score: 12.0   memory length: 847412   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 6.95\n",
      "episode: 3405   score: 6.0   memory length: 847755   epsilon: 0.009998020008555413    steps: 343    lr: 0.0001     evaluation reward: 6.95\n",
      "episode: 3406   score: 6.0   memory length: 848129   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 3407   score: 7.0   memory length: 848526   epsilon: 0.009998020008555413    steps: 397    lr: 0.0001     evaluation reward: 6.98\n",
      "episode: 3408   score: 6.0   memory length: 848864   epsilon: 0.009998020008555413    steps: 338    lr: 0.0001     evaluation reward: 6.93\n",
      "episode: 3409   score: 7.0   memory length: 849267   epsilon: 0.009998020008555413    steps: 403    lr: 0.0001     evaluation reward: 6.97\n",
      "episode: 3410   score: 5.0   memory length: 849591   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 3411   score: 7.0   memory length: 849962   epsilon: 0.009998020008555413    steps: 371    lr: 0.0001     evaluation reward: 6.96\n",
      "episode: 3412   score: 7.0   memory length: 850368   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 6.96\n",
      "episode: 3413   score: 8.0   memory length: 850772   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 6.94\n",
      "episode: 3414   score: 6.0   memory length: 851127   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 6.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3415   score: 15.0   memory length: 851698   epsilon: 0.009998020008555413    steps: 571    lr: 0.0001     evaluation reward: 7.0\n",
      "episode: 3416   score: 9.0   memory length: 852151   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 3417   score: 9.0   memory length: 852619   epsilon: 0.009998020008555413    steps: 468    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 3418   score: 6.0   memory length: 852971   epsilon: 0.009998020008555413    steps: 352    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 3419   score: 7.0   memory length: 853356   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 7.09\n",
      "episode: 3420   score: 11.0   memory length: 853885   epsilon: 0.009998020008555413    steps: 529    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 3421   score: 12.0   memory length: 854467   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3422   score: 12.0   memory length: 854912   epsilon: 0.009998020008555413    steps: 445    lr: 0.0001     evaluation reward: 7.26\n",
      "episode: 3423   score: 9.0   memory length: 855350   epsilon: 0.009998020008555413    steps: 438    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3424   score: 9.0   memory length: 855844   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 7.24\n",
      "episode: 3425   score: 11.0   memory length: 856339   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 7.31\n",
      "episode: 3426   score: 10.0   memory length: 856793   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3427   score: 8.0   memory length: 857194   epsilon: 0.009998020008555413    steps: 401    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 3428   score: 6.0   memory length: 857569   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 3429   score: 9.0   memory length: 858061   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 7.42\n",
      "episode: 3430   score: 3.0   memory length: 858306   epsilon: 0.009998020008555413    steps: 245    lr: 0.0001     evaluation reward: 7.38\n",
      "episode: 3431   score: 4.0   memory length: 858547   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 7.31\n",
      "episode: 3432   score: 7.0   memory length: 858917   epsilon: 0.009998020008555413    steps: 370    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 3433   score: 6.0   memory length: 859251   epsilon: 0.009998020008555413    steps: 334    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 3434   score: 7.0   memory length: 859656   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3435   score: 7.0   memory length: 860030   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 3436   score: 7.0   memory length: 860435   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 7.25\n",
      "episode: 3437   score: 10.0   memory length: 860808   epsilon: 0.009998020008555413    steps: 373    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3438   score: 6.0   memory length: 861166   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 3439   score: 5.0   memory length: 861453   epsilon: 0.009998020008555413    steps: 287    lr: 0.0001     evaluation reward: 7.19\n",
      "episode: 3440   score: 4.0   memory length: 861711   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 3441   score: 9.0   memory length: 862168   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 7.19\n",
      "episode: 3442   score: 6.0   memory length: 862525   epsilon: 0.009998020008555413    steps: 357    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 3443   score: 6.0   memory length: 862863   epsilon: 0.009998020008555413    steps: 338    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3444   score: 12.0   memory length: 863484   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3445   score: 9.0   memory length: 863926   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 7.25\n",
      "episode: 3446   score: 6.0   memory length: 864301   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 7.24\n",
      "episode: 3447   score: 12.0   memory length: 864794   epsilon: 0.009998020008555413    steps: 493    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 3448   score: 11.0   memory length: 865273   epsilon: 0.009998020008555413    steps: 479    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 3449   score: 5.0   memory length: 865566   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 3450   score: 3.0   memory length: 865778   epsilon: 0.009998020008555413    steps: 212    lr: 0.0001     evaluation reward: 7.3\n",
      "episode: 3451   score: 8.0   memory length: 866214   epsilon: 0.009998020008555413    steps: 436    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3452   score: 4.0   memory length: 866491   epsilon: 0.009998020008555413    steps: 277    lr: 0.0001     evaluation reward: 7.28\n",
      "episode: 3453   score: 6.0   memory length: 866819   epsilon: 0.009998020008555413    steps: 328    lr: 0.0001     evaluation reward: 7.31\n",
      "episode: 3454   score: 6.0   memory length: 867124   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 7.32\n",
      "episode: 3455   score: 5.0   memory length: 867413   epsilon: 0.009998020008555413    steps: 289    lr: 0.0001     evaluation reward: 7.34\n",
      "episode: 3456   score: 6.0   memory length: 867766   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 7.37\n",
      "episode: 3457   score: 8.0   memory length: 868043   epsilon: 0.009998020008555413    steps: 277    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 3458   score: 8.0   memory length: 868450   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 7.4\n",
      "episode: 3459   score: 8.0   memory length: 868890   epsilon: 0.009998020008555413    steps: 440    lr: 0.0001     evaluation reward: 7.33\n",
      "episode: 3460   score: 7.0   memory length: 869243   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 7.37\n",
      "episode: 3461   score: 6.0   memory length: 869599   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 7.39\n",
      "episode: 3462   score: 9.0   memory length: 869924   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 7.42\n",
      "episode: 3463   score: 5.0   memory length: 870249   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 7.35\n",
      "episode: 3464   score: 9.0   memory length: 870738   epsilon: 0.009998020008555413    steps: 489    lr: 0.0001     evaluation reward: 7.36\n",
      "episode: 3465   score: 6.0   memory length: 871076   epsilon: 0.009998020008555413    steps: 338    lr: 0.0001     evaluation reward: 7.33\n",
      "episode: 3466   score: 5.0   memory length: 871381   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 7.26\n",
      "episode: 3467   score: 4.0   memory length: 871622   epsilon: 0.009998020008555413    steps: 241    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 3468   score: 6.0   memory length: 871980   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 3469   score: 6.0   memory length: 872338   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 3470   score: 7.0   memory length: 872727   epsilon: 0.009998020008555413    steps: 389    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 3471   score: 5.0   memory length: 873047   epsilon: 0.009998020008555413    steps: 320    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 3472   score: 3.0   memory length: 873257   epsilon: 0.009998020008555413    steps: 210    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 3473   score: 4.0   memory length: 873515   epsilon: 0.009998020008555413    steps: 258    lr: 0.0001     evaluation reward: 7.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3474   score: 4.0   memory length: 873777   epsilon: 0.009998020008555413    steps: 262    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3475   score: 7.0   memory length: 874164   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 7.06\n",
      "episode: 3476   score: 7.0   memory length: 874552   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 7.07\n",
      "episode: 3477   score: 8.0   memory length: 874993   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 3478   score: 8.0   memory length: 875407   epsilon: 0.009998020008555413    steps: 414    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 3479   score: 5.0   memory length: 875713   epsilon: 0.009998020008555413    steps: 306    lr: 0.0001     evaluation reward: 7.12\n",
      "episode: 3480   score: 5.0   memory length: 876020   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 3481   score: 13.0   memory length: 876663   epsilon: 0.009998020008555413    steps: 643    lr: 0.0001     evaluation reward: 7.13\n",
      "episode: 3482   score: 5.0   memory length: 876955   epsilon: 0.009998020008555413    steps: 292    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3483   score: 6.0   memory length: 877331   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 7.14\n",
      "episode: 3484   score: 12.0   memory length: 877805   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 3485   score: 6.0   memory length: 878143   epsilon: 0.009998020008555413    steps: 338    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 3486   score: 7.0   memory length: 878532   epsilon: 0.009998020008555413    steps: 389    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3487   score: 8.0   memory length: 878964   epsilon: 0.009998020008555413    steps: 432    lr: 0.0001     evaluation reward: 7.3\n",
      "episode: 3488   score: 6.0   memory length: 879267   epsilon: 0.009998020008555413    steps: 303    lr: 0.0001     evaluation reward: 7.3\n",
      "episode: 3489   score: 7.0   memory length: 879633   epsilon: 0.009998020008555413    steps: 366    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3490   score: 5.0   memory length: 879938   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 7.27\n",
      "episode: 3491   score: 9.0   memory length: 880409   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 3492   score: 3.0   memory length: 880622   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 3493   score: 6.0   memory length: 880966   epsilon: 0.009998020008555413    steps: 344    lr: 0.0001     evaluation reward: 7.21\n",
      "episode: 3494   score: 3.0   memory length: 881193   epsilon: 0.009998020008555413    steps: 227    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 3495   score: 4.0   memory length: 881449   epsilon: 0.009998020008555413    steps: 256    lr: 0.0001     evaluation reward: 7.11\n",
      "episode: 3496   score: 4.0   memory length: 881712   epsilon: 0.009998020008555413    steps: 263    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 3497   score: 7.0   memory length: 882066   epsilon: 0.009998020008555413    steps: 354    lr: 0.0001     evaluation reward: 7.03\n",
      "episode: 3498   score: 6.0   memory length: 882402   epsilon: 0.009998020008555413    steps: 336    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 3499   score: 3.0   memory length: 882615   epsilon: 0.009998020008555413    steps: 213    lr: 0.0001     evaluation reward: 7.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    HISTORY_SIZE = len(history)\n",
    "    get_init_state(history, state[0], HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video', force=True)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(300, 200))\n",
    "display.start()\n",
    "\n",
    "# Load agent\n",
    "agent.load_policy_net(\"./save_model/breakout_dqn_latest.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state)\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Render breakout\n",
    "    env.render()\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "    life = info['ale.lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n",
    "display.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = True # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_4404\\932857171.py:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_4404\\932857171.py:21: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 1.0   memory length: 169   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 1   score: 0.0   memory length: 292   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 0.5\n",
      "episode: 2   score: 1.0   memory length: 443   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 0.6666666666666666\n",
      "episode: 3   score: 1.0   memory length: 612   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 0.75\n",
      "episode: 4   score: 3.0   memory length: 879   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 5   score: 1.0   memory length: 1030   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 6   score: 0.0   memory length: 1153   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0\n",
      "episode: 7   score: 2.0   memory length: 1333   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.125\n",
      "episode: 8   score: 4.0   memory length: 1590   epsilon: 1.0    steps: 257    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 9   score: 2.0   memory length: 1788   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 10   score: 0.0   memory length: 1911   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3636363636363635\n",
      "episode: 11   score: 0.0   memory length: 2033   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 12   score: 4.0   memory length: 2320   epsilon: 1.0    steps: 287    lr: 0.0001     evaluation reward: 1.4615384615384615\n",
      "episode: 13   score: 1.0   memory length: 2471   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 14   score: 1.0   memory length: 2639   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 15   score: 0.0   memory length: 2762   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3125\n",
      "episode: 16   score: 2.0   memory length: 2962   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.3529411764705883\n",
      "episode: 17   score: 6.0   memory length: 3376   epsilon: 1.0    steps: 414    lr: 0.0001     evaluation reward: 1.6111111111111112\n",
      "episode: 18   score: 3.0   memory length: 3602   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.6842105263157894\n",
      "episode: 19   score: 0.0   memory length: 3725   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 20   score: 1.0   memory length: 3876   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5714285714285714\n",
      "episode: 21   score: 2.0   memory length: 4076   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5909090909090908\n",
      "episode: 22   score: 2.0   memory length: 4274   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.608695652173913\n",
      "episode: 23   score: 2.0   memory length: 4471   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 24   score: 0.0   memory length: 4594   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 25   score: 1.0   memory length: 4763   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5384615384615385\n",
      "episode: 26   score: 0.0   memory length: 4885   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.4814814814814814\n",
      "episode: 27   score: 5.0   memory length: 5213   epsilon: 1.0    steps: 328    lr: 0.0001     evaluation reward: 1.6071428571428572\n",
      "episode: 28   score: 3.0   memory length: 5458   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.6551724137931034\n",
      "episode: 29   score: 2.0   memory length: 5656   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 30   score: 3.0   memory length: 5902   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.7096774193548387\n",
      "episode: 31   score: 3.0   memory length: 6148   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 32   score: 0.0   memory length: 6271   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.696969696969697\n",
      "episode: 33   score: 0.0   memory length: 6394   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6470588235294117\n",
      "episode: 34   score: 1.0   memory length: 6566   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.6285714285714286\n",
      "episode: 35   score: 2.0   memory length: 6766   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.6388888888888888\n",
      "episode: 36   score: 1.0   memory length: 6935   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6216216216216217\n",
      "episode: 37   score: 0.0   memory length: 7057   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5789473684210527\n",
      "episode: 38   score: 0.0   memory length: 7180   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5384615384615385\n",
      "episode: 39   score: 0.0   memory length: 7303   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 40   score: 3.0   memory length: 7551   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.5365853658536586\n",
      "episode: 41   score: 0.0   memory length: 7674   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 42   score: 0.0   memory length: 7797   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4651162790697674\n",
      "episode: 43   score: 2.0   memory length: 7995   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.4772727272727273\n",
      "episode: 44   score: 2.0   memory length: 8192   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.488888888888889\n",
      "episode: 45   score: 2.0   memory length: 8392   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 46   score: 0.0   memory length: 8515   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4680851063829787\n",
      "episode: 47   score: 0.0   memory length: 8638   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4375\n",
      "episode: 48   score: 1.0   memory length: 8807   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 49   score: 0.0   memory length: 8930   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 50   score: 0.0   memory length: 9053   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3725490196078431\n",
      "episode: 51   score: 0.0   memory length: 9175   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3461538461538463\n",
      "episode: 52   score: 0.0   memory length: 9298   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.320754716981132\n",
      "episode: 53   score: 0.0   memory length: 9420   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2962962962962963\n",
      "episode: 54   score: 1.0   memory length: 9589   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.290909090909091\n",
      "episode: 55   score: 0.0   memory length: 9711   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2678571428571428\n",
      "episode: 56   score: 2.0   memory length: 9910   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.280701754385965\n",
      "episode: 57   score: 3.0   memory length: 10162   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.3103448275862069\n",
      "episode: 58   score: 0.0   memory length: 10284   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2881355932203389\n",
      "episode: 59   score: 2.0   memory length: 10481   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 60   score: 0.0   memory length: 10604   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.278688524590164\n",
      "episode: 61   score: 3.0   memory length: 10850   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.3064516129032258\n",
      "episode: 62   score: 0.0   memory length: 10973   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 63   score: 2.0   memory length: 11171   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.296875\n",
      "episode: 64   score: 0.0   memory length: 11293   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2769230769230768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 65   score: 2.0   memory length: 11493   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.2878787878787878\n",
      "episode: 66   score: 2.0   memory length: 11710   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.2985074626865671\n",
      "episode: 67   score: 3.0   memory length: 11937   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.3235294117647058\n",
      "episode: 68   score: 0.0   memory length: 12060   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3043478260869565\n",
      "episode: 69   score: 3.0   memory length: 12307   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.3285714285714285\n",
      "episode: 70   score: 0.0   memory length: 12429   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3098591549295775\n",
      "episode: 71   score: 1.0   memory length: 12597   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.3055555555555556\n",
      "episode: 72   score: 0.0   memory length: 12720   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2876712328767124\n",
      "episode: 73   score: 0.0   memory length: 12843   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2702702702702702\n",
      "episode: 74   score: 1.0   memory length: 12994   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2666666666666666\n",
      "episode: 75   score: 0.0   memory length: 13116   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 76   score: 4.0   memory length: 13433   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.2857142857142858\n",
      "episode: 77   score: 0.0   memory length: 13556   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2692307692307692\n",
      "episode: 78   score: 2.0   memory length: 13754   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2784810126582278\n",
      "episode: 79   score: 0.0   memory length: 13877   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2625\n",
      "episode: 80   score: 5.0   memory length: 14234   epsilon: 1.0    steps: 357    lr: 0.0001     evaluation reward: 1.308641975308642\n",
      "episode: 81   score: 1.0   memory length: 14403   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3048780487804879\n",
      "episode: 82   score: 0.0   memory length: 14526   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2891566265060241\n",
      "episode: 83   score: 2.0   memory length: 14724   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2976190476190477\n",
      "episode: 84   score: 1.0   memory length: 14875   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2941176470588236\n",
      "episode: 85   score: 0.0   memory length: 14997   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2790697674418605\n",
      "episode: 86   score: 1.0   memory length: 15165   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.2758620689655173\n",
      "episode: 87   score: 1.0   memory length: 15315   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.2727272727272727\n",
      "episode: 88   score: 0.0   memory length: 15438   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2584269662921348\n",
      "episode: 89   score: 0.0   memory length: 15560   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2444444444444445\n",
      "episode: 90   score: 1.0   memory length: 15731   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.2417582417582418\n",
      "episode: 91   score: 0.0   memory length: 15854   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2282608695652173\n",
      "episode: 92   score: 2.0   memory length: 16052   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2365591397849462\n",
      "episode: 93   score: 1.0   memory length: 16221   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.2340425531914894\n",
      "episode: 94   score: 0.0   memory length: 16344   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2210526315789474\n",
      "episode: 95   score: 0.0   memory length: 16467   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2083333333333333\n",
      "episode: 96   score: 2.0   memory length: 16664   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.2164948453608246\n",
      "episode: 97   score: 0.0   memory length: 16787   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2040816326530612\n",
      "episode: 98   score: 1.0   memory length: 16958   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.202020202020202\n",
      "episode: 99   score: 2.0   memory length: 17179   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 100   score: 1.0   memory length: 17347   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 101   score: 2.0   memory length: 17544   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 102   score: 0.0   memory length: 17667   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 103   score: 0.0   memory length: 17789   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 104   score: 1.0   memory length: 17940   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 105   score: 0.0   memory length: 18062   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 106   score: 1.0   memory length: 18213   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 107   score: 1.0   memory length: 18382   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 108   score: 2.0   memory length: 18600   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 109   score: 1.0   memory length: 18771   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 110   score: 1.0   memory length: 18941   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 111   score: 2.0   memory length: 19139   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 112   score: 2.0   memory length: 19337   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 113   score: 0.0   memory length: 19459   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 114   score: 4.0   memory length: 19756   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 115   score: 1.0   memory length: 19925   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 116   score: 1.0   memory length: 20093   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 117   score: 3.0   memory length: 20342   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 118   score: 1.0   memory length: 20510   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 119   score: 0.0   memory length: 20632   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 120   score: 1.0   memory length: 20801   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.13\n",
      "episode: 121   score: 0.0   memory length: 20923   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.11\n",
      "episode: 122   score: 5.0   memory length: 21248   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.14\n",
      "episode: 123   score: 3.0   memory length: 21495   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.15\n",
      "episode: 124   score: 4.0   memory length: 21770   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 125   score: 2.0   memory length: 21969   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 126   score: 0.0   memory length: 22091   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 127   score: 5.0   memory length: 22369   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 128   score: 1.0   memory length: 22520   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 129   score: 2.0   memory length: 22717   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 130   score: 3.0   memory length: 22964   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 131   score: 3.0   memory length: 23231   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 132   score: 1.0   memory length: 23400   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 133   score: 3.0   memory length: 23625   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 134   score: 0.0   memory length: 23748   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 135   score: 0.0   memory length: 23871   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 136   score: 1.0   memory length: 24022   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 137   score: 0.0   memory length: 24145   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 138   score: 0.0   memory length: 24268   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 139   score: 3.0   memory length: 24532   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 140   score: 2.0   memory length: 24749   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 141   score: 0.0   memory length: 24872   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 142   score: 0.0   memory length: 24995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 143   score: 1.0   memory length: 25164   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 144   score: 3.0   memory length: 25390   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 145   score: 1.0   memory length: 25540   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 146   score: 0.0   memory length: 25662   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 147   score: 1.0   memory length: 25813   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 148   score: 2.0   memory length: 26032   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 149   score: 2.0   memory length: 26230   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 150   score: 3.0   memory length: 26478   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 151   score: 0.0   memory length: 26601   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 152   score: 0.0   memory length: 26724   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 153   score: 1.0   memory length: 26895   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 154   score: 1.0   memory length: 27046   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 155   score: 2.0   memory length: 27261   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 156   score: 1.0   memory length: 27430   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 157   score: 3.0   memory length: 27678   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 158   score: 0.0   memory length: 27801   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 159   score: 0.0   memory length: 27924   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 160   score: 0.0   memory length: 28047   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 161   score: 0.0   memory length: 28169   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 162   score: 2.0   memory length: 28388   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 163   score: 0.0   memory length: 28511   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 164   score: 1.0   memory length: 28662   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 165   score: 4.0   memory length: 28918   epsilon: 1.0    steps: 256    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 166   score: 3.0   memory length: 29144   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 167   score: 1.0   memory length: 29315   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 168   score: 2.0   memory length: 29517   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 169   score: 3.0   memory length: 29763   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 170   score: 2.0   memory length: 29979   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 171   score: 5.0   memory length: 30300   epsilon: 1.0    steps: 321    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 172   score: 0.0   memory length: 30422   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 173   score: 1.0   memory length: 30590   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 174   score: 0.0   memory length: 30713   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 175   score: 2.0   memory length: 30914   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 176   score: 0.0   memory length: 31036   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 177   score: 0.0   memory length: 31158   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 178   score: 2.0   memory length: 31374   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 179   score: 1.0   memory length: 31543   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 180   score: 2.0   memory length: 31741   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 181   score: 0.0   memory length: 31864   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 182   score: 1.0   memory length: 32015   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 183   score: 1.0   memory length: 32167   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 184   score: 0.0   memory length: 32290   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 185   score: 3.0   memory length: 32533   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 186   score: 1.0   memory length: 32684   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 187   score: 3.0   memory length: 32909   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 188   score: 0.0   memory length: 33031   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 189   score: 1.0   memory length: 33200   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 190   score: 2.0   memory length: 33418   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 191   score: 0.0   memory length: 33540   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 192   score: 0.0   memory length: 33663   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 193   score: 4.0   memory length: 33938   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 194   score: 0.0   memory length: 34061   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 195   score: 3.0   memory length: 34309   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 196   score: 2.0   memory length: 34506   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 197   score: 0.0   memory length: 34628   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 198   score: 5.0   memory length: 34951   epsilon: 1.0    steps: 323    lr: 0.0001     evaluation reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 199   score: 1.0   memory length: 35121   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 200   score: 1.0   memory length: 35272   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 201   score: 0.0   memory length: 35395   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 202   score: 2.0   memory length: 35593   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 203   score: 2.0   memory length: 35791   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 204   score: 1.0   memory length: 35959   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 205   score: 0.0   memory length: 36082   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 206   score: 2.0   memory length: 36280   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 207   score: 2.0   memory length: 36479   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 208   score: 1.0   memory length: 36630   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 209   score: 2.0   memory length: 36849   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 210   score: 4.0   memory length: 37123   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 211   score: 1.0   memory length: 37293   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 212   score: 1.0   memory length: 37462   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 213   score: 2.0   memory length: 37660   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 214   score: 3.0   memory length: 37906   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 215   score: 2.0   memory length: 38104   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 216   score: 5.0   memory length: 38470   epsilon: 1.0    steps: 366    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 217   score: 3.0   memory length: 38699   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 218   score: 2.0   memory length: 38897   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 219   score: 0.0   memory length: 39020   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 220   score: 0.0   memory length: 39143   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 221   score: 0.0   memory length: 39266   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 222   score: 3.0   memory length: 39533   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 223   score: 4.0   memory length: 39831   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 224   score: 1.0   memory length: 40000   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 225   score: 1.0   memory length: 40169   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 226   score: 3.0   memory length: 40382   epsilon: 1.0    steps: 213    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 227   score: 1.0   memory length: 40554   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 228   score: 1.0   memory length: 40704   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 229   score: 0.0   memory length: 40826   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 230   score: 2.0   memory length: 41044   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 231   score: 1.0   memory length: 41195   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 232   score: 0.0   memory length: 41318   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 233   score: 5.0   memory length: 41681   epsilon: 1.0    steps: 363    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 234   score: 2.0   memory length: 41879   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 235   score: 0.0   memory length: 42001   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 236   score: 2.0   memory length: 42219   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 237   score: 0.0   memory length: 42342   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 238   score: 1.0   memory length: 42513   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 239   score: 0.0   memory length: 42636   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 240   score: 1.0   memory length: 42804   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 241   score: 2.0   memory length: 43022   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 242   score: 1.0   memory length: 43192   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 243   score: 1.0   memory length: 43361   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 244   score: 2.0   memory length: 43580   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 245   score: 2.0   memory length: 43797   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 246   score: 2.0   memory length: 44015   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 247   score: 2.0   memory length: 44212   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 248   score: 0.0   memory length: 44335   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 249   score: 4.0   memory length: 44611   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 250   score: 2.0   memory length: 44828   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 251   score: 1.0   memory length: 44979   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 252   score: 3.0   memory length: 45208   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 253   score: 4.0   memory length: 45506   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 254   score: 0.0   memory length: 45628   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 255   score: 3.0   memory length: 45854   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 256   score: 2.0   memory length: 46051   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 257   score: 0.0   memory length: 46173   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 258   score: 0.0   memory length: 46295   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 259   score: 0.0   memory length: 46418   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 260   score: 2.0   memory length: 46620   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 261   score: 4.0   memory length: 46915   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 262   score: 2.0   memory length: 47112   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 263   score: 2.0   memory length: 47310   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 264   score: 1.0   memory length: 47463   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 265   score: 1.0   memory length: 47633   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 266   score: 3.0   memory length: 47903   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 267   score: 0.0   memory length: 48025   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 268   score: 2.0   memory length: 48226   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 269   score: 1.0   memory length: 48398   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 270   score: 0.0   memory length: 48521   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 271   score: 3.0   memory length: 48768   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 272   score: 0.0   memory length: 48891   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 273   score: 2.0   memory length: 49109   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 274   score: 2.0   memory length: 49311   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 275   score: 1.0   memory length: 49480   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 276   score: 2.0   memory length: 49698   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 277   score: 2.0   memory length: 49898   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 278   score: 0.0   memory length: 50021   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 279   score: 1.0   memory length: 50190   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 280   score: 0.0   memory length: 50312   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 281   score: 0.0   memory length: 50434   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 282   score: 0.0   memory length: 50556   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 283   score: 1.0   memory length: 50726   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 284   score: 4.0   memory length: 51040   epsilon: 1.0    steps: 314    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 285   score: 2.0   memory length: 51238   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 286   score: 0.0   memory length: 51361   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 287   score: 1.0   memory length: 51533   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 288   score: 0.0   memory length: 51655   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 289   score: 2.0   memory length: 51854   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 290   score: 0.0   memory length: 51977   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 291   score: 2.0   memory length: 52174   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 292   score: 2.0   memory length: 52393   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 293   score: 2.0   memory length: 52610   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 294   score: 1.0   memory length: 52778   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 295   score: 1.0   memory length: 52947   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 296   score: 1.0   memory length: 53115   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 297   score: 2.0   memory length: 53312   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 298   score: 1.0   memory length: 53481   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 299   score: 0.0   memory length: 53604   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 300   score: 0.0   memory length: 53727   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 301   score: 0.0   memory length: 53849   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 302   score: 1.0   memory length: 54000   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 303   score: 0.0   memory length: 54122   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 304   score: 0.0   memory length: 54244   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 305   score: 1.0   memory length: 54413   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 306   score: 3.0   memory length: 54678   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 307   score: 1.0   memory length: 54846   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 308   score: 3.0   memory length: 55072   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 309   score: 1.0   memory length: 55223   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 310   score: 0.0   memory length: 55346   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 311   score: 1.0   memory length: 55514   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 312   score: 0.0   memory length: 55637   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 313   score: 3.0   memory length: 55902   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 314   score: 0.0   memory length: 56025   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 315   score: 0.0   memory length: 56147   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 316   score: 2.0   memory length: 56364   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 317   score: 1.0   memory length: 56514   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 318   score: 1.0   memory length: 56684   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 319   score: 3.0   memory length: 56950   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 320   score: 1.0   memory length: 57101   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 321   score: 3.0   memory length: 57353   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 322   score: 2.0   memory length: 57552   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 323   score: 2.0   memory length: 57750   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 324   score: 0.0   memory length: 57872   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 325   score: 0.0   memory length: 57995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 326   score: 0.0   memory length: 58118   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 327   score: 0.0   memory length: 58240   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 328   score: 0.0   memory length: 58362   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 329   score: 3.0   memory length: 58590   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 330   score: 0.0   memory length: 58712   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 331   score: 0.0   memory length: 58834   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 332   score: 1.0   memory length: 58984   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 333   score: 4.0   memory length: 59278   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 334   score: 1.0   memory length: 59447   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 335   score: 4.0   memory length: 59720   epsilon: 1.0    steps: 273    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 336   score: 0.0   memory length: 59843   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 337   score: 4.0   memory length: 60136   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 338   score: 3.0   memory length: 60382   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 339   score: 0.0   memory length: 60504   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 340   score: 3.0   memory length: 60751   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 341   score: 3.0   memory length: 60984   epsilon: 1.0    steps: 233    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 342   score: 2.0   memory length: 61199   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 343   score: 4.0   memory length: 61475   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 344   score: 2.0   memory length: 61696   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 345   score: 2.0   memory length: 61894   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 346   score: 0.0   memory length: 62017   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 347   score: 1.0   memory length: 62186   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 348   score: 2.0   memory length: 62405   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 349   score: 1.0   memory length: 62556   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 350   score: 0.0   memory length: 62679   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 351   score: 2.0   memory length: 62898   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 352   score: 2.0   memory length: 63096   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 353   score: 0.0   memory length: 63219   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 354   score: 0.0   memory length: 63341   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 355   score: 4.0   memory length: 63635   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 356   score: 2.0   memory length: 63835   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 357   score: 0.0   memory length: 63958   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 358   score: 3.0   memory length: 64183   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 359   score: 0.0   memory length: 64306   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 360   score: 0.0   memory length: 64429   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 361   score: 2.0   memory length: 64627   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 362   score: 0.0   memory length: 64749   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 363   score: 0.0   memory length: 64872   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 364   score: 0.0   memory length: 64995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 365   score: 1.0   memory length: 65145   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 366   score: 4.0   memory length: 65443   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 367   score: 2.0   memory length: 65663   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 368   score: 1.0   memory length: 65834   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 369   score: 0.0   memory length: 65957   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 370   score: 0.0   memory length: 66079   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 371   score: 1.0   memory length: 66248   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 372   score: 0.0   memory length: 66371   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 373   score: 0.0   memory length: 66494   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 374   score: 2.0   memory length: 66692   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 375   score: 1.0   memory length: 66843   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 376   score: 2.0   memory length: 67059   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 377   score: 1.0   memory length: 67228   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 378   score: 2.0   memory length: 67426   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 379   score: 0.0   memory length: 67549   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 380   score: 3.0   memory length: 67797   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 381   score: 2.0   memory length: 67979   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 382   score: 2.0   memory length: 68198   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 383   score: 2.0   memory length: 68397   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 384   score: 2.0   memory length: 68594   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 385   score: 0.0   memory length: 68717   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 386   score: 1.0   memory length: 68886   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 387   score: 0.0   memory length: 69009   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 388   score: 4.0   memory length: 69305   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 389   score: 2.0   memory length: 69503   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 390   score: 0.0   memory length: 69625   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 391   score: 1.0   memory length: 69794   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 392   score: 2.0   memory length: 70009   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 393   score: 4.0   memory length: 70322   epsilon: 1.0    steps: 313    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 394   score: 4.0   memory length: 70573   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 395   score: 0.0   memory length: 70695   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 396   score: 1.0   memory length: 70865   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 397   score: 1.0   memory length: 71035   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 398   score: 1.0   memory length: 71187   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 399   score: 3.0   memory length: 71436   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 400   score: 0.0   memory length: 71558   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 401   score: 1.0   memory length: 71709   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 402   score: 0.0   memory length: 71832   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 403   score: 6.0   memory length: 72157   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 404   score: 1.0   memory length: 72326   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 405   score: 1.0   memory length: 72477   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 406   score: 3.0   memory length: 72724   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 407   score: 2.0   memory length: 72922   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 408   score: 2.0   memory length: 73142   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 409   score: 1.0   memory length: 73311   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 410   score: 0.0   memory length: 73433   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 411   score: 2.0   memory length: 73652   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 412   score: 4.0   memory length: 73968   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 413   score: 4.0   memory length: 74266   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 414   score: 3.0   memory length: 74515   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 415   score: 1.0   memory length: 74666   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 416   score: 0.0   memory length: 74788   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 417   score: 0.0   memory length: 74911   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 418   score: 2.0   memory length: 75132   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 419   score: 2.0   memory length: 75348   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 420   score: 2.0   memory length: 75565   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 421   score: 3.0   memory length: 75835   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 422   score: 0.0   memory length: 75958   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 423   score: 1.0   memory length: 76126   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 424   score: 0.0   memory length: 76248   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 425   score: 3.0   memory length: 76493   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 426   score: 2.0   memory length: 76675   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 427   score: 2.0   memory length: 76872   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 428   score: 0.0   memory length: 76995   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 429   score: 1.0   memory length: 77145   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 430   score: 2.0   memory length: 77343   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 431   score: 0.0   memory length: 77465   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 432   score: 0.0   memory length: 77587   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 433   score: 4.0   memory length: 77878   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 434   score: 2.0   memory length: 78076   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 435   score: 2.0   memory length: 78274   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 436   score: 5.0   memory length: 78618   epsilon: 1.0    steps: 344    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 437   score: 2.0   memory length: 78799   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 438   score: 1.0   memory length: 78950   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 439   score: 3.0   memory length: 79194   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 440   score: 0.0   memory length: 79316   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 441   score: 1.0   memory length: 79467   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 442   score: 2.0   memory length: 79665   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 443   score: 1.0   memory length: 79816   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 444   score: 2.0   memory length: 80014   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 445   score: 1.0   memory length: 80186   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 446   score: 0.0   memory length: 80309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 447   score: 1.0   memory length: 80459   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 448   score: 0.0   memory length: 80581   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 449   score: 2.0   memory length: 80778   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 450   score: 2.0   memory length: 80975   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 451   score: 0.0   memory length: 81097   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 452   score: 0.0   memory length: 81219   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 453   score: 1.0   memory length: 81388   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 454   score: 2.0   memory length: 81586   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 455   score: 2.0   memory length: 81805   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 456   score: 2.0   memory length: 82003   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 457   score: 1.0   memory length: 82173   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 458   score: 0.0   memory length: 82296   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 459   score: 0.0   memory length: 82419   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 460   score: 0.0   memory length: 82541   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 461   score: 4.0   memory length: 82835   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 462   score: 4.0   memory length: 83094   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 463   score: 2.0   memory length: 83316   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 464   score: 0.0   memory length: 83438   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 465   score: 2.0   memory length: 83636   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 466   score: 5.0   memory length: 83956   epsilon: 1.0    steps: 320    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 467   score: 1.0   memory length: 84124   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 468   score: 0.0   memory length: 84246   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 469   score: 3.0   memory length: 84472   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 470   score: 1.0   memory length: 84642   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 471   score: 2.0   memory length: 84863   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 472   score: 1.0   memory length: 85031   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 473   score: 1.0   memory length: 85199   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 474   score: 3.0   memory length: 85469   epsilon: 1.0    steps: 270    lr: 0.0001     evaluation reward: 1.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 475   score: 0.0   memory length: 85591   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 476   score: 3.0   memory length: 85816   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 477   score: 1.0   memory length: 85985   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 478   score: 0.0   memory length: 86108   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 479   score: 2.0   memory length: 86306   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 480   score: 2.0   memory length: 86524   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 481   score: 0.0   memory length: 86647   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 482   score: 4.0   memory length: 86913   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 483   score: 3.0   memory length: 87158   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 484   score: 1.0   memory length: 87327   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 485   score: 1.0   memory length: 87499   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 486   score: 1.0   memory length: 87650   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 487   score: 0.0   memory length: 87773   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 488   score: 2.0   memory length: 87959   epsilon: 1.0    steps: 186    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 489   score: 0.0   memory length: 88082   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 490   score: 1.0   memory length: 88252   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 491   score: 1.0   memory length: 88423   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 492   score: 3.0   memory length: 88675   epsilon: 1.0    steps: 252    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 493   score: 3.0   memory length: 88901   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 494   score: 5.0   memory length: 89264   epsilon: 1.0    steps: 363    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 495   score: 2.0   memory length: 89461   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 496   score: 3.0   memory length: 89712   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 497   score: 3.0   memory length: 89939   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 498   score: 4.0   memory length: 90231   epsilon: 1.0    steps: 292    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 499   score: 1.0   memory length: 90400   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 500   score: 1.0   memory length: 90569   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 501   score: 2.0   memory length: 90749   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 502   score: 2.0   memory length: 90947   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 503   score: 0.0   memory length: 91070   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 504   score: 2.0   memory length: 91268   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 505   score: 1.0   memory length: 91419   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 506   score: 1.0   memory length: 91571   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 507   score: 1.0   memory length: 91742   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 508   score: 0.0   memory length: 91865   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 509   score: 0.0   memory length: 91987   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 510   score: 0.0   memory length: 92109   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 511   score: 1.0   memory length: 92262   epsilon: 1.0    steps: 153    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 512   score: 2.0   memory length: 92464   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 513   score: 2.0   memory length: 92661   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 514   score: 0.0   memory length: 92784   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 515   score: 2.0   memory length: 92982   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 516   score: 0.0   memory length: 93105   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 517   score: 4.0   memory length: 93380   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 518   score: 2.0   memory length: 93563   epsilon: 1.0    steps: 183    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 519   score: 0.0   memory length: 93686   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 520   score: 1.0   memory length: 93857   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 521   score: 5.0   memory length: 94182   epsilon: 1.0    steps: 325    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 522   score: 1.0   memory length: 94353   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 523   score: 2.0   memory length: 94551   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 524   score: 2.0   memory length: 94748   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 525   score: 1.0   memory length: 94917   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 526   score: 2.0   memory length: 95114   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 527   score: 1.0   memory length: 95283   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 528   score: 3.0   memory length: 95547   epsilon: 1.0    steps: 264    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 529   score: 1.0   memory length: 95717   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 530   score: 1.0   memory length: 95885   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 531   score: 0.0   memory length: 96008   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 532   score: 2.0   memory length: 96205   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 533   score: 5.0   memory length: 96521   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 534   score: 0.0   memory length: 96644   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 535   score: 0.0   memory length: 96767   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 536   score: 1.0   memory length: 96918   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 537   score: 0.0   memory length: 97040   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 538   score: 0.0   memory length: 97162   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 539   score: 2.0   memory length: 97360   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 540   score: 2.0   memory length: 97558   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 541   score: 1.0   memory length: 97727   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 542   score: 2.0   memory length: 97945   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 543   score: 2.0   memory length: 98142   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 544   score: 3.0   memory length: 98388   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 545   score: 2.0   memory length: 98604   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 546   score: 1.0   memory length: 98775   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 547   score: 3.0   memory length: 99020   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 548   score: 2.0   memory length: 99237   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 549   score: 2.0   memory length: 99435   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 550   score: 3.0   memory length: 99662   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 551   score: 2.0   memory length: 99860   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 552   score: 6.0   memory length: 100225   epsilon: 0.9995525200000097    steps: 365    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 553   score: 4.0   memory length: 100485   epsilon: 0.9990377200000209    steps: 260    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 554   score: 0.0   memory length: 100608   epsilon: 0.9987941800000262    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 555   score: 1.0   memory length: 100758   epsilon: 0.9984971800000326    steps: 150    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 556   score: 0.0   memory length: 100880   epsilon: 0.9982556200000379    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 557   score: 0.0   memory length: 101002   epsilon: 0.9980140600000431    steps: 122    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 558   score: 3.0   memory length: 101248   epsilon: 0.9975269800000537    steps: 246    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 559   score: 1.0   memory length: 101417   epsilon: 0.997192360000061    steps: 169    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 560   score: 2.0   memory length: 101635   epsilon: 0.9967607200000703    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 561   score: 0.0   memory length: 101758   epsilon: 0.9965171800000756    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 562   score: 2.0   memory length: 101955   epsilon: 0.9961271200000841    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 563   score: 0.0   memory length: 102078   epsilon: 0.9958835800000894    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 564   score: 0.0   memory length: 102201   epsilon: 0.9956400400000947    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 565   score: 2.0   memory length: 102381   epsilon: 0.9952836400001024    steps: 180    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 566   score: 2.0   memory length: 102579   epsilon: 0.9948916000001109    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 567   score: 5.0   memory length: 102924   epsilon: 0.9942085000001257    steps: 345    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 568   score: 3.0   memory length: 103189   epsilon: 0.9936838000001371    steps: 265    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 569   score: 0.0   memory length: 103312   epsilon: 0.9934402600001424    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 570   score: 1.0   memory length: 103480   epsilon: 0.9931076200001496    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 571   score: 0.0   memory length: 103602   epsilon: 0.9928660600001549    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 572   score: 1.0   memory length: 103753   epsilon: 0.9925670800001614    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 573   score: 0.0   memory length: 103876   epsilon: 0.9923235400001666    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 574   score: 0.0   memory length: 103999   epsilon: 0.9920800000001719    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 575   score: 2.0   memory length: 104196   epsilon: 0.9916899400001804    steps: 197    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 576   score: 1.0   memory length: 104365   epsilon: 0.9913553200001877    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 577   score: 3.0   memory length: 104590   epsilon: 0.9909098200001973    steps: 225    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 578   score: 0.0   memory length: 104712   epsilon: 0.9906682600002026    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 579   score: 0.0   memory length: 104835   epsilon: 0.9904247200002079    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 580   score: 1.0   memory length: 104985   epsilon: 0.9901277200002143    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 581   score: 0.0   memory length: 105107   epsilon: 0.9898861600002196    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 582   score: 0.0   memory length: 105230   epsilon: 0.9896426200002248    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 583   score: 4.0   memory length: 105525   epsilon: 0.9890585200002375    steps: 295    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 584   score: 1.0   memory length: 105695   epsilon: 0.9887219200002448    steps: 170    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 585   score: 0.0   memory length: 105818   epsilon: 0.9884783800002501    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 586   score: 4.0   memory length: 106093   epsilon: 0.9879338800002619    steps: 275    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 587   score: 1.0   memory length: 106265   epsilon: 0.9875933200002693    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 588   score: 1.0   memory length: 106433   epsilon: 0.9872606800002766    steps: 168    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 589   score: 0.0   memory length: 106556   epsilon: 0.9870171400002818    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 590   score: 2.0   memory length: 106774   epsilon: 0.9865855000002912    steps: 218    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 591   score: 1.0   memory length: 106943   epsilon: 0.9862508800002985    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 592   score: 0.0   memory length: 107066   epsilon: 0.9860073400003038    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 593   score: 2.0   memory length: 107283   epsilon: 0.9855776800003131    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 594   score: 0.0   memory length: 107405   epsilon: 0.9853361200003183    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 595   score: 4.0   memory length: 107701   epsilon: 0.9847500400003311    steps: 296    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 596   score: 4.0   memory length: 107997   epsilon: 0.9841639600003438    steps: 296    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 597   score: 1.0   memory length: 108166   epsilon: 0.983829340000351    steps: 169    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 598   score: 2.0   memory length: 108364   epsilon: 0.9834373000003596    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 599   score: 0.0   memory length: 108487   epsilon: 0.9831937600003648    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 600   score: 3.0   memory length: 108713   epsilon: 0.9827462800003746    steps: 226    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 601   score: 2.0   memory length: 108931   epsilon: 0.9823146400003839    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 602   score: 0.0   memory length: 109054   epsilon: 0.9820711000003892    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 603   score: 2.0   memory length: 109275   epsilon: 0.9816335200003987    steps: 221    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 604   score: 2.0   memory length: 109490   epsilon: 0.981207820000408    steps: 215    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 605   score: 0.0   memory length: 109612   epsilon: 0.9809662600004132    steps: 122    lr: 0.0001     evaluation reward: 1.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 606   score: 3.0   memory length: 109856   epsilon: 0.9804831400004237    steps: 244    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 607   score: 1.0   memory length: 110007   epsilon: 0.9801841600004302    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 608   score: 2.0   memory length: 110205   epsilon: 0.9797921200004387    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 609   score: 2.0   memory length: 110403   epsilon: 0.9794000800004472    steps: 198    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 610   score: 1.0   memory length: 110554   epsilon: 0.9791011000004537    steps: 151    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 611   score: 1.0   memory length: 110722   epsilon: 0.9787684600004609    steps: 168    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 612   score: 0.0   memory length: 110845   epsilon: 0.9785249200004662    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 613   score: 2.0   memory length: 111043   epsilon: 0.9781328800004747    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 614   score: 1.0   memory length: 111193   epsilon: 0.9778358800004812    steps: 150    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 615   score: 0.0   memory length: 111316   epsilon: 0.9775923400004864    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 616   score: 1.0   memory length: 111466   epsilon: 0.9772953400004929    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 617   score: 0.0   memory length: 111589   epsilon: 0.9770518000004982    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 618   score: 0.0   memory length: 111712   epsilon: 0.9768082600005035    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 619   score: 2.0   memory length: 111931   epsilon: 0.9763746400005129    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 620   score: 4.0   memory length: 112224   epsilon: 0.9757945000005255    steps: 293    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 621   score: 1.0   memory length: 112375   epsilon: 0.975495520000532    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 622   score: 4.0   memory length: 112675   epsilon: 0.9749015200005449    steps: 300    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 623   score: 3.0   memory length: 112888   epsilon: 0.974479780000554    steps: 213    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 624   score: 3.0   memory length: 113135   epsilon: 0.9739907200005646    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 625   score: 0.0   memory length: 113258   epsilon: 0.9737471800005699    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 626   score: 0.0   memory length: 113381   epsilon: 0.9735036400005752    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 627   score: 0.0   memory length: 113504   epsilon: 0.9732601000005805    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 628   score: 1.0   memory length: 113655   epsilon: 0.972961120000587    steps: 151    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 629   score: 2.0   memory length: 113870   epsilon: 0.9725354200005962    steps: 215    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 630   score: 0.0   memory length: 113993   epsilon: 0.9722918800006015    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 631   score: 2.0   memory length: 114190   epsilon: 0.97190182000061    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 632   score: 3.0   memory length: 114457   epsilon: 0.9713731600006215    steps: 267    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 633   score: 1.0   memory length: 114608   epsilon: 0.971074180000628    steps: 151    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 634   score: 2.0   memory length: 114805   epsilon: 0.9706841200006364    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 635   score: 0.0   memory length: 114928   epsilon: 0.9704405800006417    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 636   score: 2.0   memory length: 115147   epsilon: 0.9700069600006511    steps: 219    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 637   score: 5.0   memory length: 115492   epsilon: 0.969323860000666    steps: 345    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 638   score: 2.0   memory length: 115689   epsilon: 0.9689338000006744    steps: 197    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 639   score: 0.0   memory length: 115812   epsilon: 0.9686902600006797    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 640   score: 1.0   memory length: 115963   epsilon: 0.9683912800006862    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 641   score: 4.0   memory length: 116260   epsilon: 0.967803220000699    steps: 297    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 642   score: 0.0   memory length: 116383   epsilon: 0.9675596800007042    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 643   score: 2.0   memory length: 116582   epsilon: 0.9671656600007128    steps: 199    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 644   score: 0.0   memory length: 116704   epsilon: 0.966924100000718    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 645   score: 2.0   memory length: 116902   epsilon: 0.9665320600007266    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 646   score: 0.0   memory length: 117025   epsilon: 0.9662885200007318    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 647   score: 0.0   memory length: 117148   epsilon: 0.9660449800007371    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 648   score: 2.0   memory length: 117346   epsilon: 0.9656529400007456    steps: 198    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 649   score: 0.0   memory length: 117469   epsilon: 0.9654094000007509    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 650   score: 2.0   memory length: 117684   epsilon: 0.9649837000007602    steps: 215    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 651   score: 4.0   memory length: 117950   epsilon: 0.9644570200007716    steps: 266    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 652   score: 0.0   memory length: 118072   epsilon: 0.9642154600007768    steps: 122    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 653   score: 1.0   memory length: 118244   epsilon: 0.9638749000007842    steps: 172    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 654   score: 4.0   memory length: 118520   epsilon: 0.9633284200007961    steps: 276    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 655   score: 4.0   memory length: 118818   epsilon: 0.9627383800008089    steps: 298    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 656   score: 2.0   memory length: 119036   epsilon: 0.9623067400008183    steps: 218    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 657   score: 0.0   memory length: 119159   epsilon: 0.9620632000008236    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 658   score: 1.0   memory length: 119309   epsilon: 0.96176620000083    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 659   score: 4.0   memory length: 119587   epsilon: 0.961215760000842    steps: 278    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 660   score: 0.0   memory length: 119710   epsilon: 0.9609722200008473    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 661   score: 2.0   memory length: 119907   epsilon: 0.9605821600008557    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 662   score: 2.0   memory length: 120106   epsilon: 0.9601881400008643    steps: 199    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 663   score: 3.0   memory length: 120375   epsilon: 0.9596555200008758    steps: 269    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 664   score: 0.0   memory length: 120497   epsilon: 0.9594139600008811    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 665   score: 1.0   memory length: 120668   epsilon: 0.9590753800008884    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 666   score: 1.0   memory length: 120819   epsilon: 0.9587764000008949    steps: 151    lr: 0.0001     evaluation reward: 1.46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 667   score: 1.0   memory length: 120987   epsilon: 0.9584437600009021    steps: 168    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 668   score: 1.0   memory length: 121156   epsilon: 0.9581091400009094    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 669   score: 1.0   memory length: 121325   epsilon: 0.9577745200009167    steps: 169    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 670   score: 2.0   memory length: 121507   epsilon: 0.9574141600009245    steps: 182    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 671   score: 0.0   memory length: 121630   epsilon: 0.9571706200009298    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 672   score: 3.0   memory length: 121861   epsilon: 0.9567132400009397    steps: 231    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 673   score: 1.0   memory length: 122032   epsilon: 0.9563746600009471    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 674   score: 1.0   memory length: 122200   epsilon: 0.9560420200009543    steps: 168    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 675   score: 2.0   memory length: 122397   epsilon: 0.9556519600009628    steps: 197    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 676   score: 1.0   memory length: 122568   epsilon: 0.9553133800009701    steps: 171    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 677   score: 1.0   memory length: 122738   epsilon: 0.9549767800009774    steps: 170    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 678   score: 1.0   memory length: 122910   epsilon: 0.9546362200009848    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 679   score: 4.0   memory length: 123222   epsilon: 0.9540184600009982    steps: 312    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 680   score: 0.0   memory length: 123345   epsilon: 0.9537749200010035    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 681   score: 1.0   memory length: 123515   epsilon: 0.9534383200010108    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 682   score: 1.0   memory length: 123665   epsilon: 0.9531413200010173    steps: 150    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 683   score: 4.0   memory length: 123959   epsilon: 0.9525592000010299    steps: 294    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 684   score: 3.0   memory length: 124209   epsilon: 0.9520642000010406    steps: 250    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 685   score: 1.0   memory length: 124378   epsilon: 0.9517295800010479    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 686   score: 1.0   memory length: 124549   epsilon: 0.9513910000010553    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 687   score: 2.0   memory length: 124769   epsilon: 0.9509554000010647    steps: 220    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 688   score: 2.0   memory length: 124986   epsilon: 0.950525740001074    steps: 217    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 689   score: 1.0   memory length: 125155   epsilon: 0.9501911200010813    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 690   score: 0.0   memory length: 125278   epsilon: 0.9499475800010866    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 691   score: 0.0   memory length: 125401   epsilon: 0.9497040400010919    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 692   score: 1.0   memory length: 125551   epsilon: 0.9494070400010983    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 693   score: 2.0   memory length: 125751   epsilon: 0.9490110400011069    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 694   score: 2.0   memory length: 125949   epsilon: 0.9486190000011154    steps: 198    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 695   score: 3.0   memory length: 126195   epsilon: 0.948131920001126    steps: 246    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 696   score: 0.0   memory length: 126317   epsilon: 0.9478903600011312    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 697   score: 2.0   memory length: 126497   epsilon: 0.947533960001139    steps: 180    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 698   score: 1.0   memory length: 126648   epsilon: 0.9472349800011455    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 699   score: 3.0   memory length: 126895   epsilon: 0.9467459200011561    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 700   score: 1.0   memory length: 127064   epsilon: 0.9464113000011634    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 701   score: 0.0   memory length: 127187   epsilon: 0.9461677600011686    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 702   score: 2.0   memory length: 127403   epsilon: 0.9457400800011779    steps: 216    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 703   score: 4.0   memory length: 127701   epsilon: 0.9451500400011907    steps: 298    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 704   score: 2.0   memory length: 127919   epsilon: 0.9447184000012001    steps: 218    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 705   score: 1.0   memory length: 128087   epsilon: 0.9443857600012073    steps: 168    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 706   score: 2.0   memory length: 128284   epsilon: 0.9439957000012158    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 707   score: 0.0   memory length: 128407   epsilon: 0.9437521600012211    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 708   score: 0.0   memory length: 128530   epsilon: 0.9435086200012264    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 709   score: 2.0   memory length: 128728   epsilon: 0.9431165800012349    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 710   score: 2.0   memory length: 128946   epsilon: 0.9426849400012443    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 711   score: 0.0   memory length: 129069   epsilon: 0.9424414000012495    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 712   score: 0.0   memory length: 129192   epsilon: 0.9421978600012548    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 713   score: 4.0   memory length: 129470   epsilon: 0.9416474200012668    steps: 278    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 714   score: 0.0   memory length: 129593   epsilon: 0.9414038800012721    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 715   score: 3.0   memory length: 129820   epsilon: 0.9409544200012818    steps: 227    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 716   score: 2.0   memory length: 130039   epsilon: 0.9405208000012912    steps: 219    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 717   score: 2.0   memory length: 130255   epsilon: 0.9400931200013005    steps: 216    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 718   score: 2.0   memory length: 130453   epsilon: 0.939701080001309    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 719   score: 2.0   memory length: 130651   epsilon: 0.9393090400013175    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 720   score: 1.0   memory length: 130821   epsilon: 0.9389724400013248    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 721   score: 0.0   memory length: 130944   epsilon: 0.9387289000013301    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 722   score: 2.0   memory length: 131142   epsilon: 0.9383368600013386    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 723   score: 1.0   memory length: 131293   epsilon: 0.9380378800013451    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 724   score: 3.0   memory length: 131506   epsilon: 0.9376161400013543    steps: 213    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 725   score: 2.0   memory length: 131686   epsilon: 0.937259740001362    steps: 180    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 726   score: 4.0   memory length: 131981   epsilon: 0.9366756400013747    steps: 295    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 727   score: 3.0   memory length: 132207   epsilon: 0.9362281600013844    steps: 226    lr: 0.0001     evaluation reward: 1.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 728   score: 3.0   memory length: 132419   epsilon: 0.9358084000013935    steps: 212    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 729   score: 2.0   memory length: 132618   epsilon: 0.9354143800014021    steps: 199    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 730   score: 0.0   memory length: 132740   epsilon: 0.9351728200014073    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 731   score: 1.0   memory length: 132908   epsilon: 0.9348401800014146    steps: 168    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 732   score: 3.0   memory length: 133136   epsilon: 0.9343887400014244    steps: 228    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 733   score: 0.0   memory length: 133259   epsilon: 0.9341452000014296    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 734   score: 1.0   memory length: 133431   epsilon: 0.933804640001437    steps: 172    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 735   score: 0.0   memory length: 133554   epsilon: 0.9335611000014423    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 736   score: 0.0   memory length: 133677   epsilon: 0.9333175600014476    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 737   score: 0.0   memory length: 133800   epsilon: 0.9330740200014529    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 738   score: 0.0   memory length: 133923   epsilon: 0.9328304800014582    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 739   score: 1.0   memory length: 134074   epsilon: 0.9325315000014647    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 740   score: 4.0   memory length: 134376   epsilon: 0.9319335400014777    steps: 302    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 741   score: 2.0   memory length: 134593   epsilon: 0.931503880001487    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 742   score: 1.0   memory length: 134764   epsilon: 0.9311653000014943    steps: 171    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 743   score: 0.0   memory length: 134887   epsilon: 0.9309217600014996    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 744   score: 1.0   memory length: 135038   epsilon: 0.9306227800015061    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 745   score: 2.0   memory length: 135254   epsilon: 0.9301951000015154    steps: 216    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 746   score: 0.0   memory length: 135376   epsilon: 0.9299535400015206    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 747   score: 1.0   memory length: 135527   epsilon: 0.9296545600015271    steps: 151    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 748   score: 1.0   memory length: 135678   epsilon: 0.9293555800015336    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 749   score: 4.0   memory length: 135991   epsilon: 0.9287358400015471    steps: 313    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 750   score: 3.0   memory length: 136235   epsilon: 0.9282527200015576    steps: 244    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 751   score: 3.0   memory length: 136460   epsilon: 0.9278072200015672    steps: 225    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 752   score: 2.0   memory length: 136661   epsilon: 0.9274092400015759    steps: 201    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 753   score: 0.0   memory length: 136784   epsilon: 0.9271657000015812    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 754   score: 6.0   memory length: 137154   epsilon: 0.9264331000015971    steps: 370    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 755   score: 5.0   memory length: 137482   epsilon: 0.9257836600016112    steps: 328    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 756   score: 2.0   memory length: 137701   epsilon: 0.9253500400016206    steps: 219    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 757   score: 2.0   memory length: 137898   epsilon: 0.924959980001629    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 758   score: 3.0   memory length: 138162   epsilon: 0.9244372600016404    steps: 264    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 759   score: 2.0   memory length: 138359   epsilon: 0.9240472000016489    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 760   score: 2.0   memory length: 138558   epsilon: 0.9236531800016574    steps: 199    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 761   score: 1.0   memory length: 138708   epsilon: 0.9233561800016639    steps: 150    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 762   score: 1.0   memory length: 138859   epsilon: 0.9230572000016704    steps: 151    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 763   score: 1.0   memory length: 139009   epsilon: 0.9227602000016768    steps: 150    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 764   score: 3.0   memory length: 139256   epsilon: 0.9222711400016874    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 765   score: 1.0   memory length: 139425   epsilon: 0.9219365200016947    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 766   score: 0.0   memory length: 139548   epsilon: 0.9216929800017    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 767   score: 2.0   memory length: 139746   epsilon: 0.9213009400017085    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 768   score: 4.0   memory length: 140005   epsilon: 0.9207881200017196    steps: 259    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 769   score: 0.0   memory length: 140127   epsilon: 0.9205465600017249    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 770   score: 2.0   memory length: 140325   epsilon: 0.9201545200017334    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 771   score: 3.0   memory length: 140553   epsilon: 0.9197030800017432    steps: 228    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 772   score: 4.0   memory length: 140829   epsilon: 0.919156600001755    steps: 276    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 773   score: 4.0   memory length: 141106   epsilon: 0.9186081400017669    steps: 277    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 774   score: 4.0   memory length: 141402   epsilon: 0.9180220600017797    steps: 296    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 775   score: 1.0   memory length: 141553   epsilon: 0.9177230800017862    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 776   score: 2.0   memory length: 141733   epsilon: 0.9173666800017939    steps: 180    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 777   score: 1.0   memory length: 141902   epsilon: 0.9170320600018012    steps: 169    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 778   score: 2.0   memory length: 142084   epsilon: 0.916671700001809    steps: 182    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 779   score: 1.0   memory length: 142254   epsilon: 0.9163351000018163    steps: 170    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 780   score: 2.0   memory length: 142471   epsilon: 0.9159054400018256    steps: 217    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 781   score: 4.0   memory length: 142737   epsilon: 0.915378760001837    steps: 266    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 782   score: 2.0   memory length: 142935   epsilon: 0.9149867200018456    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 783   score: 4.0   memory length: 143252   epsilon: 0.9143590600018592    steps: 317    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 784   score: 2.0   memory length: 143449   epsilon: 0.9139690000018676    steps: 197    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 785   score: 0.0   memory length: 143571   epsilon: 0.9137274400018729    steps: 122    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 786   score: 0.0   memory length: 143693   epsilon: 0.9134858800018781    steps: 122    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 787   score: 3.0   memory length: 143962   epsilon: 0.9129532600018897    steps: 269    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 788   score: 0.0   memory length: 144085   epsilon: 0.912709720001895    steps: 123    lr: 0.0001     evaluation reward: 1.73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 789   score: 1.0   memory length: 144257   epsilon: 0.9123691600019024    steps: 172    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 790   score: 3.0   memory length: 144503   epsilon: 0.911882080001913    steps: 246    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 791   score: 0.0   memory length: 144625   epsilon: 0.9116405200019182    steps: 122    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 792   score: 2.0   memory length: 144845   epsilon: 0.9112049200019277    steps: 220    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 793   score: 1.0   memory length: 144996   epsilon: 0.9109059400019341    steps: 151    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 794   score: 2.0   memory length: 145195   epsilon: 0.9105119200019427    steps: 199    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 795   score: 2.0   memory length: 145393   epsilon: 0.9101198800019512    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 796   score: 2.0   memory length: 145591   epsilon: 0.9097278400019597    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 797   score: 3.0   memory length: 145838   epsilon: 0.9092387800019703    steps: 247    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 798   score: 3.0   memory length: 146104   epsilon: 0.9087121000019818    steps: 266    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 799   score: 0.0   memory length: 146227   epsilon: 0.9084685600019871    steps: 123    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 800   score: 1.0   memory length: 146377   epsilon: 0.9081715600019935    steps: 150    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 801   score: 2.0   memory length: 146595   epsilon: 0.9077399200020029    steps: 218    lr: 0.0001     evaluation reward: 1.79\n",
      "episode: 802   score: 0.0   memory length: 146717   epsilon: 0.9074983600020081    steps: 122    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 803   score: 1.0   memory length: 146885   epsilon: 0.9071657200020153    steps: 168    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 804   score: 3.0   memory length: 147111   epsilon: 0.906718240002025    steps: 226    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 805   score: 2.0   memory length: 147309   epsilon: 0.9063262000020336    steps: 198    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 806   score: 1.0   memory length: 147479   epsilon: 0.9059896000020409    steps: 170    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 807   score: 1.0   memory length: 147650   epsilon: 0.9056510200020482    steps: 171    lr: 0.0001     evaluation reward: 1.76\n",
      "episode: 808   score: 1.0   memory length: 147820   epsilon: 0.9053144200020555    steps: 170    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 809   score: 2.0   memory length: 148018   epsilon: 0.904922380002064    steps: 198    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 810   score: 2.0   memory length: 148234   epsilon: 0.9044947000020733    steps: 216    lr: 0.0001     evaluation reward: 1.77\n",
      "episode: 811   score: 1.0   memory length: 148404   epsilon: 0.9041581000020806    steps: 170    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 812   score: 2.0   memory length: 148602   epsilon: 0.9037660600020891    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 813   score: 2.0   memory length: 148821   epsilon: 0.9033324400020986    steps: 219    lr: 0.0001     evaluation reward: 1.78\n",
      "episode: 814   score: 2.0   memory length: 149019   epsilon: 0.9029404000021071    steps: 198    lr: 0.0001     evaluation reward: 1.8\n",
      "episode: 815   score: 6.0   memory length: 149347   epsilon: 0.9022909600021212    steps: 328    lr: 0.0001     evaluation reward: 1.83\n",
      "episode: 816   score: 3.0   memory length: 149594   epsilon: 0.9018019000021318    steps: 247    lr: 0.0001     evaluation reward: 1.84\n",
      "episode: 817   score: 3.0   memory length: 149824   epsilon: 0.9013465000021417    steps: 230    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 818   score: 4.0   memory length: 150105   epsilon: 0.9007901200021537    steps: 281    lr: 0.0001     evaluation reward: 1.87\n",
      "episode: 819   score: 4.0   memory length: 150403   epsilon: 0.9002000800021666    steps: 298    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 820   score: 2.0   memory length: 150624   epsilon: 0.8997625000021761    steps: 221    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 821   score: 0.0   memory length: 150747   epsilon: 0.8995189600021813    steps: 123    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 822   score: 1.0   memory length: 150916   epsilon: 0.8991843400021886    steps: 169    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 823   score: 2.0   memory length: 151113   epsilon: 0.8987942800021971    steps: 197    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 824   score: 2.0   memory length: 151332   epsilon: 0.8983606600022065    steps: 219    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 825   score: 6.0   memory length: 151704   epsilon: 0.8976241000022225    steps: 372    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 826   score: 1.0   memory length: 151873   epsilon: 0.8972894800022297    steps: 169    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 827   score: 2.0   memory length: 152096   epsilon: 0.8968479400022393    steps: 223    lr: 0.0001     evaluation reward: 1.89\n",
      "episode: 828   score: 0.0   memory length: 152218   epsilon: 0.8966063800022446    steps: 122    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 829   score: 1.0   memory length: 152387   epsilon: 0.8962717600022518    steps: 169    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 830   score: 0.0   memory length: 152509   epsilon: 0.8960302000022571    steps: 122    lr: 0.0001     evaluation reward: 1.85\n",
      "episode: 831   score: 4.0   memory length: 152767   epsilon: 0.8955193600022682    steps: 258    lr: 0.0001     evaluation reward: 1.88\n",
      "episode: 832   score: 1.0   memory length: 152918   epsilon: 0.8952203800022747    steps: 151    lr: 0.0001     evaluation reward: 1.86\n",
      "episode: 833   score: 4.0   memory length: 153193   epsilon: 0.8946758800022865    steps: 275    lr: 0.0001     evaluation reward: 1.9\n",
      "episode: 834   score: 4.0   memory length: 153469   epsilon: 0.8941294000022983    steps: 276    lr: 0.0001     evaluation reward: 1.93\n",
      "episode: 835   score: 1.0   memory length: 153638   epsilon: 0.8937947800023056    steps: 169    lr: 0.0001     evaluation reward: 1.94\n",
      "episode: 836   score: 2.0   memory length: 153836   epsilon: 0.8934027400023141    steps: 198    lr: 0.0001     evaluation reward: 1.96\n",
      "episode: 837   score: 3.0   memory length: 154045   epsilon: 0.8929889200023231    steps: 209    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 838   score: 1.0   memory length: 154215   epsilon: 0.8926523200023304    steps: 170    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 839   score: 1.0   memory length: 154366   epsilon: 0.8923533400023369    steps: 151    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 840   score: 3.0   memory length: 154592   epsilon: 0.8919058600023466    steps: 226    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 841   score: 2.0   memory length: 154793   epsilon: 0.8915078800023553    steps: 201    lr: 0.0001     evaluation reward: 1.99\n",
      "episode: 842   score: 0.0   memory length: 154916   epsilon: 0.8912643400023605    steps: 123    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 843   score: 5.0   memory length: 155230   epsilon: 0.890642620002374    steps: 314    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 844   score: 2.0   memory length: 155447   epsilon: 0.8902129600023834    steps: 217    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 845   score: 0.0   memory length: 155569   epsilon: 0.8899714000023886    steps: 122    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 846   score: 2.0   memory length: 155748   epsilon: 0.8896169800023963    steps: 179    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 847   score: 1.0   memory length: 155898   epsilon: 0.8893199800024028    steps: 150    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 848   score: 1.0   memory length: 156067   epsilon: 0.88898536000241    steps: 169    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 849   score: 3.0   memory length: 156293   epsilon: 0.8885378800024197    steps: 226    lr: 0.0001     evaluation reward: 2.03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 850   score: 1.0   memory length: 156444   epsilon: 0.8882389000024262    steps: 151    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 851   score: 2.0   memory length: 156663   epsilon: 0.8878052800024356    steps: 219    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 852   score: 0.0   memory length: 156786   epsilon: 0.8875617400024409    steps: 123    lr: 0.0001     evaluation reward: 1.98\n",
      "episode: 853   score: 7.0   memory length: 157210   epsilon: 0.8867222200024591    steps: 424    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 854   score: 5.0   memory length: 157476   epsilon: 0.8861955400024706    steps: 266    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 855   score: 3.0   memory length: 157720   epsilon: 0.8857124200024811    steps: 244    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 856   score: 3.0   memory length: 157968   epsilon: 0.8852213800024917    steps: 248    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 857   score: 3.0   memory length: 158196   epsilon: 0.8847699400025015    steps: 228    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 858   score: 2.0   memory length: 158414   epsilon: 0.8843383000025109    steps: 218    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 859   score: 0.0   memory length: 158536   epsilon: 0.8840967400025161    steps: 122    lr: 0.0001     evaluation reward: 2.01\n",
      "episode: 860   score: 3.0   memory length: 158763   epsilon: 0.8836472800025259    steps: 227    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 861   score: 3.0   memory length: 158988   epsilon: 0.8832017800025356    steps: 225    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 862   score: 2.0   memory length: 159186   epsilon: 0.8828097400025441    steps: 198    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 863   score: 2.0   memory length: 159404   epsilon: 0.8823781000025535    steps: 218    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 864   score: 2.0   memory length: 159622   epsilon: 0.8819464600025628    steps: 218    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 865   score: 7.0   memory length: 159979   epsilon: 0.8812396000025782    steps: 357    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 866   score: 2.0   memory length: 160161   epsilon: 0.880879240002586    steps: 182    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 867   score: 2.0   memory length: 160377   epsilon: 0.8804515600025953    steps: 216    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 868   score: 2.0   memory length: 160557   epsilon: 0.880095160002603    steps: 180    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 869   score: 3.0   memory length: 160823   epsilon: 0.8795684800026145    steps: 266    lr: 0.0001     evaluation reward: 2.14\n",
      "episode: 870   score: 1.0   memory length: 160991   epsilon: 0.8792358400026217    steps: 168    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 871   score: 0.0   memory length: 161114   epsilon: 0.878992300002627    steps: 123    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 872   score: 0.0   memory length: 161237   epsilon: 0.8787487600026322    steps: 123    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 873   score: 4.0   memory length: 161534   epsilon: 0.878160700002645    steps: 297    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 874   score: 4.0   memory length: 161831   epsilon: 0.8775726400026578    steps: 297    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 875   score: 1.0   memory length: 161982   epsilon: 0.8772736600026643    steps: 151    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 876   score: 3.0   memory length: 162227   epsilon: 0.8767885600026748    steps: 245    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 877   score: 3.0   memory length: 162456   epsilon: 0.8763351400026846    steps: 229    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 878   score: 0.0   memory length: 162578   epsilon: 0.8760935800026899    steps: 122    lr: 0.0001     evaluation reward: 2.07\n",
      "episode: 879   score: 0.0   memory length: 162701   epsilon: 0.8758500400026952    steps: 123    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 880   score: 1.0   memory length: 162870   epsilon: 0.8755154200027024    steps: 169    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 881   score: 2.0   memory length: 163089   epsilon: 0.8750818000027119    steps: 219    lr: 0.0001     evaluation reward: 2.03\n",
      "episode: 882   score: 3.0   memory length: 163316   epsilon: 0.8746323400027216    steps: 227    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 883   score: 2.0   memory length: 163533   epsilon: 0.8742026800027309    steps: 217    lr: 0.0001     evaluation reward: 2.02\n",
      "episode: 884   score: 0.0   memory length: 163656   epsilon: 0.8739591400027362    steps: 123    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 885   score: 4.0   memory length: 163945   epsilon: 0.8733869200027486    steps: 289    lr: 0.0001     evaluation reward: 2.04\n",
      "episode: 886   score: 2.0   memory length: 164143   epsilon: 0.8729948800027572    steps: 198    lr: 0.0001     evaluation reward: 2.06\n",
      "episode: 887   score: 2.0   memory length: 164343   epsilon: 0.8725988800027658    steps: 200    lr: 0.0001     evaluation reward: 2.05\n",
      "episode: 888   score: 5.0   memory length: 164646   epsilon: 0.8719989400027788    steps: 303    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 889   score: 2.0   memory length: 164844   epsilon: 0.8716069000027873    steps: 198    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 890   score: 1.0   memory length: 164995   epsilon: 0.8713079200027938    steps: 151    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 891   score: 0.0   memory length: 165118   epsilon: 0.8710643800027991    steps: 123    lr: 0.0001     evaluation reward: 2.09\n",
      "episode: 892   score: 1.0   memory length: 165269   epsilon: 0.8707654000028056    steps: 151    lr: 0.0001     evaluation reward: 2.08\n",
      "episode: 893   score: 4.0   memory length: 165544   epsilon: 0.8702209000028174    steps: 275    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 894   score: 3.0   memory length: 165772   epsilon: 0.8697694600028272    steps: 228    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 895   score: 3.0   memory length: 166018   epsilon: 0.8692823800028378    steps: 246    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 896   score: 1.0   memory length: 166169   epsilon: 0.8689834000028442    steps: 151    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 897   score: 2.0   memory length: 166367   epsilon: 0.8685913600028528    steps: 198    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 898   score: 2.0   memory length: 166588   epsilon: 0.8681537800028623    steps: 221    lr: 0.0001     evaluation reward: 2.1\n",
      "episode: 899   score: 1.0   memory length: 166759   epsilon: 0.8678152000028696    steps: 171    lr: 0.0001     evaluation reward: 2.11\n",
      "episode: 900   score: 2.0   memory length: 166957   epsilon: 0.8674231600028781    steps: 198    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 901   score: 2.0   memory length: 167155   epsilon: 0.8670311200028866    steps: 198    lr: 0.0001     evaluation reward: 2.12\n",
      "episode: 902   score: 1.0   memory length: 167324   epsilon: 0.8666965000028939    steps: 169    lr: 0.0001     evaluation reward: 2.13\n",
      "episode: 903   score: 4.0   memory length: 167621   epsilon: 0.8661084400029067    steps: 297    lr: 0.0001     evaluation reward: 2.16\n",
      "episode: 904   score: 2.0   memory length: 167819   epsilon: 0.8657164000029152    steps: 198    lr: 0.0001     evaluation reward: 2.15\n",
      "episode: 905   score: 8.0   memory length: 168269   epsilon: 0.8648254000029345    steps: 450    lr: 0.0001     evaluation reward: 2.21\n",
      "episode: 906   score: 5.0   memory length: 168596   epsilon: 0.8641779400029486    steps: 327    lr: 0.0001     evaluation reward: 2.25\n",
      "episode: 907   score: 5.0   memory length: 168934   epsilon: 0.8635087000029631    steps: 338    lr: 0.0001     evaluation reward: 2.29\n",
      "episode: 908   score: 5.0   memory length: 169298   epsilon: 0.8627879800029787    steps: 364    lr: 0.0001     evaluation reward: 2.33\n",
      "episode: 909   score: 5.0   memory length: 169622   epsilon: 0.8621464600029927    steps: 324    lr: 0.0001     evaluation reward: 2.36\n",
      "episode: 910   score: 5.0   memory length: 169951   epsilon: 0.8614950400030068    steps: 329    lr: 0.0001     evaluation reward: 2.39\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 911   score: 2.0   memory length: 170169   epsilon: 0.8610634000030162    steps: 218    lr: 0.0001     evaluation reward: 2.4\n",
      "episode: 912   score: 3.0   memory length: 170398   epsilon: 0.860609980003026    steps: 229    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 913   score: 3.0   memory length: 170629   epsilon: 0.860152600003036    steps: 231    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 914   score: 3.0   memory length: 170855   epsilon: 0.8597051200030457    steps: 226    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 915   score: 1.0   memory length: 171026   epsilon: 0.859366540003053    steps: 171    lr: 0.0001     evaluation reward: 2.38\n",
      "episode: 916   score: 0.0   memory length: 171149   epsilon: 0.8591230000030583    steps: 123    lr: 0.0001     evaluation reward: 2.35\n",
      "episode: 917   score: 5.0   memory length: 171491   epsilon: 0.858445840003073    steps: 342    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 918   score: 4.0   memory length: 171766   epsilon: 0.8579013400030848    steps: 275    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 919   score: 4.0   memory length: 172044   epsilon: 0.8573509000030968    steps: 278    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 920   score: 2.0   memory length: 172262   epsilon: 0.8569192600031061    steps: 218    lr: 0.0001     evaluation reward: 2.37\n",
      "episode: 921   score: 4.0   memory length: 172521   epsilon: 0.8564064400031173    steps: 259    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 922   score: 3.0   memory length: 172766   epsilon: 0.8559213400031278    steps: 245    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 923   score: 1.0   memory length: 172937   epsilon: 0.8555827600031352    steps: 171    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 924   score: 5.0   memory length: 173280   epsilon: 0.8549036200031499    steps: 343    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 925   score: 3.0   memory length: 173510   epsilon: 0.8544482200031598    steps: 230    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 926   score: 1.0   memory length: 173682   epsilon: 0.8541076600031672    steps: 172    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 927   score: 1.0   memory length: 173851   epsilon: 0.8537730400031744    steps: 169    lr: 0.0001     evaluation reward: 2.41\n",
      "episode: 928   score: 3.0   memory length: 174098   epsilon: 0.8532839800031851    steps: 247    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 929   score: 2.0   memory length: 174297   epsilon: 0.8528899600031936    steps: 199    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 930   score: 2.0   memory length: 174515   epsilon: 0.852458320003203    steps: 218    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 931   score: 2.0   memory length: 174730   epsilon: 0.8520326200032122    steps: 215    lr: 0.0001     evaluation reward: 2.45\n",
      "episode: 932   score: 0.0   memory length: 174853   epsilon: 0.8517890800032175    steps: 123    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 933   score: 3.0   memory length: 175084   epsilon: 0.8513317000032274    steps: 231    lr: 0.0001     evaluation reward: 2.43\n",
      "episode: 934   score: 3.0   memory length: 175329   epsilon: 0.850846600003238    steps: 245    lr: 0.0001     evaluation reward: 2.42\n",
      "episode: 935   score: 5.0   memory length: 175623   epsilon: 0.8502644800032506    steps: 294    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 936   score: 6.0   memory length: 175993   epsilon: 0.8495318800032665    steps: 370    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 937   score: 0.0   memory length: 176115   epsilon: 0.8492903200032718    steps: 122    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 938   score: 1.0   memory length: 176284   epsilon: 0.848955700003279    steps: 169    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 939   score: 3.0   memory length: 176509   epsilon: 0.8485102000032887    steps: 225    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 940   score: 0.0   memory length: 176631   epsilon: 0.8482686400032939    steps: 122    lr: 0.0001     evaluation reward: 2.46\n",
      "episode: 941   score: 3.0   memory length: 176857   epsilon: 0.8478211600033037    steps: 226    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 942   score: 2.0   memory length: 177054   epsilon: 0.8474311000033121    steps: 197    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 943   score: 0.0   memory length: 177177   epsilon: 0.8471875600033174    steps: 123    lr: 0.0001     evaluation reward: 2.44\n",
      "episode: 944   score: 5.0   memory length: 177480   epsilon: 0.8465876200033304    steps: 303    lr: 0.0001     evaluation reward: 2.47\n",
      "episode: 945   score: 4.0   memory length: 177759   epsilon: 0.8460352000033424    steps: 279    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 946   score: 0.0   memory length: 177882   epsilon: 0.8457916600033477    steps: 123    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 947   score: 1.0   memory length: 178032   epsilon: 0.8454946600033542    steps: 150    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 948   score: 7.0   memory length: 178332   epsilon: 0.844900660003367    steps: 300    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 949   score: 3.0   memory length: 178600   epsilon: 0.8443700200033786    steps: 268    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 950   score: 3.0   memory length: 178865   epsilon: 0.84384532000339    steps: 265    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 951   score: 2.0   memory length: 179063   epsilon: 0.8434532800033985    steps: 198    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 952   score: 2.0   memory length: 179284   epsilon: 0.843015700003408    steps: 221    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 953   score: 4.0   memory length: 179561   epsilon: 0.8424672400034199    steps: 277    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 954   score: 1.0   memory length: 179711   epsilon: 0.8421702400034263    steps: 150    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 955   score: 0.0   memory length: 179833   epsilon: 0.8419286800034316    steps: 122    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 956   score: 3.0   memory length: 180059   epsilon: 0.8414812000034413    steps: 226    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 957   score: 3.0   memory length: 180285   epsilon: 0.841033720003451    steps: 226    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 958   score: 4.0   memory length: 180562   epsilon: 0.8404852600034629    steps: 277    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 959   score: 5.0   memory length: 180908   epsilon: 0.8398001800034778    steps: 346    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 960   score: 2.0   memory length: 181126   epsilon: 0.8393685400034872    steps: 218    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 961   score: 2.0   memory length: 181344   epsilon: 0.8389369000034965    steps: 218    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 962   score: 2.0   memory length: 181562   epsilon: 0.8385052600035059    steps: 218    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 963   score: 3.0   memory length: 181793   epsilon: 0.8380478800035158    steps: 231    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 964   score: 4.0   memory length: 182071   epsilon: 0.8374974400035278    steps: 278    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 965   score: 3.0   memory length: 182316   epsilon: 0.8370123400035383    steps: 245    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 966   score: 1.0   memory length: 182466   epsilon: 0.8367153400035447    steps: 150    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 967   score: 0.0   memory length: 182588   epsilon: 0.83647378000355    steps: 122    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 968   score: 2.0   memory length: 182805   epsilon: 0.8360441200035593    steps: 217    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 969   score: 1.0   memory length: 182955   epsilon: 0.8357471200035658    steps: 150    lr: 0.0001     evaluation reward: 2.48\n",
      "episode: 970   score: 4.0   memory length: 183238   epsilon: 0.8351867800035779    steps: 283    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 971   score: 4.0   memory length: 183513   epsilon: 0.8346422800035898    steps: 275    lr: 0.0001     evaluation reward: 2.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 972   score: 3.0   memory length: 183722   epsilon: 0.8342284600035987    steps: 209    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 973   score: 2.0   memory length: 183902   epsilon: 0.8338720600036065    steps: 180    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 974   score: 2.0   memory length: 184101   epsilon: 0.833478040003615    steps: 199    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 975   score: 3.0   memory length: 184327   epsilon: 0.8330305600036247    steps: 226    lr: 0.0001     evaluation reward: 2.56\n",
      "episode: 976   score: 5.0   memory length: 184611   epsilon: 0.832468240003637    steps: 284    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 977   score: 2.0   memory length: 184829   epsilon: 0.8320366000036463    steps: 218    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 978   score: 1.0   memory length: 184981   epsilon: 0.8317356400036529    steps: 152    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 979   score: 3.0   memory length: 185231   epsilon: 0.8312406400036636    steps: 250    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 980   score: 1.0   memory length: 185382   epsilon: 0.8309416600036701    steps: 151    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 981   score: 3.0   memory length: 185611   epsilon: 0.8304882400036799    steps: 229    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 982   score: 4.0   memory length: 185888   epsilon: 0.8299397800036918    steps: 277    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 983   score: 5.0   memory length: 186256   epsilon: 0.8292111400037077    steps: 368    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 984   score: 0.0   memory length: 186379   epsilon: 0.828967600003713    steps: 123    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 985   score: 1.0   memory length: 186530   epsilon: 0.8286686200037194    steps: 151    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 986   score: 1.0   memory length: 186681   epsilon: 0.8283696400037259    steps: 151    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 987   score: 1.0   memory length: 186849   epsilon: 0.8280370000037331    steps: 168    lr: 0.0001     evaluation reward: 2.61\n",
      "episode: 988   score: 3.0   memory length: 187078   epsilon: 0.827583580003743    steps: 229    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 989   score: 1.0   memory length: 187246   epsilon: 0.8272509400037502    steps: 168    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 990   score: 5.0   memory length: 187581   epsilon: 0.8265876400037646    steps: 335    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 991   score: 2.0   memory length: 187798   epsilon: 0.8261579800037739    steps: 217    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 992   score: 4.0   memory length: 188090   epsilon: 0.8255798200037865    steps: 292    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 993   score: 0.0   memory length: 188212   epsilon: 0.8253382600037917    steps: 122    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 994   score: 2.0   memory length: 188428   epsilon: 0.824910580003801    steps: 216    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 995   score: 4.0   memory length: 188722   epsilon: 0.8243284600038137    steps: 294    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 996   score: 2.0   memory length: 188939   epsilon: 0.823898800003823    steps: 217    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 997   score: 3.0   memory length: 189183   epsilon: 0.8234156800038335    steps: 244    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 998   score: 3.0   memory length: 189409   epsilon: 0.8229682000038432    steps: 226    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 999   score: 3.0   memory length: 189635   epsilon: 0.8225207200038529    steps: 226    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1000   score: 2.0   memory length: 189853   epsilon: 0.8220890800038623    steps: 218    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1001   score: 1.0   memory length: 190023   epsilon: 0.8217524800038696    steps: 170    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 1002   score: 4.0   memory length: 190324   epsilon: 0.8211565000038825    steps: 301    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1003   score: 3.0   memory length: 190550   epsilon: 0.8207090200038922    steps: 226    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1004   score: 4.0   memory length: 190867   epsilon: 0.8200813600039059    steps: 317    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 1005   score: 4.0   memory length: 191184   epsilon: 0.8194537000039195    steps: 317    lr: 0.0001     evaluation reward: 2.67\n",
      "episode: 1006   score: 3.0   memory length: 191430   epsilon: 0.8189666200039301    steps: 246    lr: 0.0001     evaluation reward: 2.65\n",
      "episode: 1007   score: 4.0   memory length: 191705   epsilon: 0.8184221200039419    steps: 275    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1008   score: 3.0   memory length: 191953   epsilon: 0.8179310800039525    steps: 248    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1009   score: 5.0   memory length: 192280   epsilon: 0.8172836200039666    steps: 327    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1010   score: 0.0   memory length: 192402   epsilon: 0.8170420600039718    steps: 122    lr: 0.0001     evaluation reward: 2.57\n",
      "episode: 1011   score: 3.0   memory length: 192669   epsilon: 0.8165134000039833    steps: 267    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1012   score: 3.0   memory length: 192879   epsilon: 0.8160976000039923    steps: 210    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1013   score: 3.0   memory length: 193105   epsilon: 0.815650120004002    steps: 226    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1014   score: 3.0   memory length: 193351   epsilon: 0.8151630400040126    steps: 246    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1015   score: 3.0   memory length: 193617   epsilon: 0.8146363600040241    steps: 266    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1016   score: 2.0   memory length: 193817   epsilon: 0.8142403600040327    steps: 200    lr: 0.0001     evaluation reward: 2.62\n",
      "episode: 1017   score: 2.0   memory length: 193998   epsilon: 0.8138819800040404    steps: 181    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1018   score: 0.0   memory length: 194121   epsilon: 0.8136384400040457    steps: 123    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1019   score: 2.0   memory length: 194322   epsilon: 0.8132404600040544    steps: 201    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1020   score: 2.0   memory length: 194501   epsilon: 0.8128860400040621    steps: 179    lr: 0.0001     evaluation reward: 2.53\n",
      "episode: 1021   score: 2.0   memory length: 194699   epsilon: 0.8124940000040706    steps: 198    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1022   score: 1.0   memory length: 194850   epsilon: 0.8121950200040771    steps: 151    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1023   score: 3.0   memory length: 195097   epsilon: 0.8117059600040877    steps: 247    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1024   score: 4.0   memory length: 195390   epsilon: 0.8111258200041003    steps: 293    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1025   score: 3.0   memory length: 195656   epsilon: 0.8105991400041117    steps: 266    lr: 0.0001     evaluation reward: 2.5\n",
      "episode: 1026   score: 0.0   memory length: 195778   epsilon: 0.810357580004117    steps: 122    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1027   score: 1.0   memory length: 195947   epsilon: 0.8100229600041242    steps: 169    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1028   score: 5.0   memory length: 196272   epsilon: 0.8093794600041382    steps: 325    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1029   score: 3.0   memory length: 196498   epsilon: 0.8089319800041479    steps: 226    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1030   score: 4.0   memory length: 196774   epsilon: 0.8083855000041598    steps: 276    lr: 0.0001     evaluation reward: 2.54\n",
      "episode: 1031   score: 3.0   memory length: 197020   epsilon: 0.8078984200041703    steps: 246    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1032   score: 3.0   memory length: 197267   epsilon: 0.807409360004181    steps: 247    lr: 0.0001     evaluation reward: 2.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1033   score: 3.0   memory length: 197496   epsilon: 0.8069559400041908    steps: 229    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1034   score: 4.0   memory length: 197789   epsilon: 0.8063758000042034    steps: 293    lr: 0.0001     evaluation reward: 2.59\n",
      "episode: 1035   score: 1.0   memory length: 197959   epsilon: 0.8060392000042107    steps: 170    lr: 0.0001     evaluation reward: 2.55\n",
      "episode: 1036   score: 0.0   memory length: 198082   epsilon: 0.805795660004216    steps: 123    lr: 0.0001     evaluation reward: 2.49\n",
      "episode: 1037   score: 3.0   memory length: 198328   epsilon: 0.8053085800042266    steps: 246    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1038   score: 0.0   memory length: 198451   epsilon: 0.8050650400042318    steps: 123    lr: 0.0001     evaluation reward: 2.51\n",
      "episode: 1039   score: 4.0   memory length: 198747   epsilon: 0.8044789600042446    steps: 296    lr: 0.0001     evaluation reward: 2.52\n",
      "episode: 1040   score: 8.0   memory length: 199156   epsilon: 0.8036691400042622    steps: 409    lr: 0.0001     evaluation reward: 2.6\n",
      "episode: 1041   score: 1.0   memory length: 199307   epsilon: 0.8033701600042686    steps: 151    lr: 0.0001     evaluation reward: 2.58\n",
      "episode: 1042   score: 7.0   memory length: 199609   epsilon: 0.8027722000042816    steps: 302    lr: 0.0001     evaluation reward: 2.63\n",
      "episode: 1043   score: 5.0   memory length: 199932   epsilon: 0.8021326600042955    steps: 323    lr: 0.0001     evaluation reward: 2.68\n",
      "episode: 1044   score: 3.0   memory length: 200158   epsilon: 0.8016851800043052    steps: 226    lr: 0.0001     evaluation reward: 2.66\n",
      "episode: 1045   score: 2.0   memory length: 200338   epsilon: 0.801328780004313    steps: 180    lr: 0.0001     evaluation reward: 2.64\n",
      "episode: 1046   score: 5.0   memory length: 200704   epsilon: 0.8006041000043287    steps: 366    lr: 0.0001     evaluation reward: 2.69\n",
      "episode: 1047   score: 4.0   memory length: 201000   epsilon: 0.8000180200043414    steps: 296    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1048   score: 6.0   memory length: 201380   epsilon: 0.7992656200043577    steps: 380    lr: 0.0001     evaluation reward: 2.71\n",
      "episode: 1049   score: 4.0   memory length: 201695   epsilon: 0.7986419200043713    steps: 315    lr: 0.0001     evaluation reward: 2.72\n",
      "episode: 1050   score: 1.0   memory length: 201863   epsilon: 0.7983092800043785    steps: 168    lr: 0.0001     evaluation reward: 2.7\n",
      "episode: 1051   score: 6.0   memory length: 202257   epsilon: 0.7975291600043954    steps: 394    lr: 0.0001     evaluation reward: 2.74\n",
      "episode: 1052   score: 3.0   memory length: 202502   epsilon: 0.797044060004406    steps: 245    lr: 0.0001     evaluation reward: 2.75\n",
      "episode: 1053   score: 4.0   memory length: 202776   epsilon: 0.7965015400044178    steps: 274    lr: 0.0001     evaluation reward: 2.75\n",
      "episode: 1054   score: 9.0   memory length: 203232   epsilon: 0.7955986600044374    steps: 456    lr: 0.0001     evaluation reward: 2.83\n",
      "episode: 1055   score: 3.0   memory length: 203458   epsilon: 0.7951511800044471    steps: 226    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1056   score: 3.0   memory length: 203705   epsilon: 0.7946621200044577    steps: 247    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1057   score: 4.0   memory length: 203981   epsilon: 0.7941156400044695    steps: 276    lr: 0.0001     evaluation reward: 2.87\n",
      "episode: 1058   score: 3.0   memory length: 204206   epsilon: 0.7936701400044792    steps: 225    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1059   score: 5.0   memory length: 204552   epsilon: 0.7929850600044941    steps: 346    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1060   score: 0.0   memory length: 204675   epsilon: 0.7927415200044994    steps: 123    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 1061   score: 2.0   memory length: 204893   epsilon: 0.7923098800045087    steps: 218    lr: 0.0001     evaluation reward: 2.84\n",
      "episode: 1062   score: 3.0   memory length: 205121   epsilon: 0.7918584400045185    steps: 228    lr: 0.0001     evaluation reward: 2.85\n",
      "episode: 1063   score: 4.0   memory length: 205415   epsilon: 0.7912763200045312    steps: 294    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1064   score: 0.0   memory length: 205537   epsilon: 0.7910347600045364    steps: 122    lr: 0.0001     evaluation reward: 2.82\n",
      "episode: 1065   score: 1.0   memory length: 205709   epsilon: 0.7906942000045438    steps: 172    lr: 0.0001     evaluation reward: 2.8\n",
      "episode: 1066   score: 2.0   memory length: 205929   epsilon: 0.7902586000045533    steps: 220    lr: 0.0001     evaluation reward: 2.81\n",
      "episode: 1067   score: 5.0   memory length: 206273   epsilon: 0.7895774800045681    steps: 344    lr: 0.0001     evaluation reward: 2.86\n",
      "episode: 1068   score: 6.0   memory length: 206648   epsilon: 0.7888349800045842    steps: 375    lr: 0.0001     evaluation reward: 2.9\n",
      "episode: 1069   score: 7.0   memory length: 207056   epsilon: 0.7880271400046017    steps: 408    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1070   score: 4.0   memory length: 207310   epsilon: 0.7875242200046126    steps: 254    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1071   score: 6.0   memory length: 207685   epsilon: 0.7867817200046288    steps: 375    lr: 0.0001     evaluation reward: 2.98\n",
      "episode: 1072   score: 1.0   memory length: 207853   epsilon: 0.786449080004636    steps: 168    lr: 0.0001     evaluation reward: 2.96\n",
      "episode: 1073   score: 5.0   memory length: 208196   epsilon: 0.7857699400046507    steps: 343    lr: 0.0001     evaluation reward: 2.99\n",
      "episode: 1074   score: 3.0   memory length: 208423   epsilon: 0.7853204800046605    steps: 227    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1075   score: 3.0   memory length: 208656   epsilon: 0.7848591400046705    steps: 233    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1076   score: 4.0   memory length: 208931   epsilon: 0.7843146400046823    steps: 275    lr: 0.0001     evaluation reward: 2.99\n",
      "episode: 1077   score: 3.0   memory length: 209143   epsilon: 0.7838948800046914    steps: 212    lr: 0.0001     evaluation reward: 3.0\n",
      "episode: 1078   score: 3.0   memory length: 209389   epsilon: 0.783407800004702    steps: 246    lr: 0.0001     evaluation reward: 3.02\n",
      "episode: 1079   score: 3.0   memory length: 209635   epsilon: 0.7829207200047126    steps: 246    lr: 0.0001     evaluation reward: 3.02\n",
      "episode: 1080   score: 2.0   memory length: 209835   epsilon: 0.7825247200047212    steps: 200    lr: 0.0001     evaluation reward: 3.03\n",
      "episode: 1081   score: 2.0   memory length: 210058   epsilon: 0.7820831800047308    steps: 223    lr: 0.0001     evaluation reward: 3.02\n",
      "episode: 1082   score: 3.0   memory length: 210305   epsilon: 0.7815941200047414    steps: 247    lr: 0.0001     evaluation reward: 3.01\n",
      "episode: 1083   score: 5.0   memory length: 210611   epsilon: 0.7809882400047545    steps: 306    lr: 0.0001     evaluation reward: 3.01\n",
      "episode: 1084   score: 6.0   memory length: 210977   epsilon: 0.7802635600047703    steps: 366    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1085   score: 7.0   memory length: 211399   epsilon: 0.7794280000047884    steps: 422    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1086   score: 3.0   memory length: 211630   epsilon: 0.7789706200047983    steps: 231    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1087   score: 0.0   memory length: 211753   epsilon: 0.7787270800048036    steps: 123    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1088   score: 2.0   memory length: 211950   epsilon: 0.7783370200048121    steps: 197    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1089   score: 2.0   memory length: 212132   epsilon: 0.7779766600048199    steps: 182    lr: 0.0001     evaluation reward: 3.14\n",
      "episode: 1090   score: 2.0   memory length: 212312   epsilon: 0.7776202600048276    steps: 180    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1091   score: 3.0   memory length: 212556   epsilon: 0.7771371400048381    steps: 244    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1092   score: 1.0   memory length: 212707   epsilon: 0.7768381600048446    steps: 151    lr: 0.0001     evaluation reward: 3.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1093   score: 1.0   memory length: 212859   epsilon: 0.7765372000048512    steps: 152    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1094   score: 3.0   memory length: 213106   epsilon: 0.7760481400048618    steps: 247    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1095   score: 3.0   memory length: 213351   epsilon: 0.7755630400048723    steps: 245    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1096   score: 4.0   memory length: 213645   epsilon: 0.7749809200048849    steps: 294    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1097   score: 2.0   memory length: 213863   epsilon: 0.7745492800048943    steps: 218    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1098   score: 5.0   memory length: 214207   epsilon: 0.7738681600049091    steps: 344    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1099   score: 5.0   memory length: 214548   epsilon: 0.7731929800049238    steps: 341    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1100   score: 2.0   memory length: 214746   epsilon: 0.7728009400049323    steps: 198    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1101   score: 2.0   memory length: 214945   epsilon: 0.7724069200049408    steps: 199    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1102   score: 0.0   memory length: 215068   epsilon: 0.7721633800049461    steps: 123    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1103   score: 4.0   memory length: 215383   epsilon: 0.7715396800049596    steps: 315    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1104   score: 0.0   memory length: 215506   epsilon: 0.7712961400049649    steps: 123    lr: 0.0001     evaluation reward: 3.09\n",
      "episode: 1105   score: 6.0   memory length: 215844   epsilon: 0.7706269000049795    steps: 338    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1106   score: 0.0   memory length: 215966   epsilon: 0.7703853400049847    steps: 122    lr: 0.0001     evaluation reward: 3.08\n",
      "episode: 1107   score: 1.0   memory length: 216135   epsilon: 0.770050720004992    steps: 169    lr: 0.0001     evaluation reward: 3.05\n",
      "episode: 1108   score: 2.0   memory length: 216314   epsilon: 0.7696963000049997    steps: 179    lr: 0.0001     evaluation reward: 3.04\n",
      "episode: 1109   score: 4.0   memory length: 216576   epsilon: 0.7691775400050109    steps: 262    lr: 0.0001     evaluation reward: 3.03\n",
      "episode: 1110   score: 2.0   memory length: 216774   epsilon: 0.7687855000050194    steps: 198    lr: 0.0001     evaluation reward: 3.05\n",
      "episode: 1111   score: 4.0   memory length: 217066   epsilon: 0.768207340005032    steps: 292    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1112   score: 4.0   memory length: 217340   epsilon: 0.7676648200050438    steps: 274    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1113   score: 6.0   memory length: 217675   epsilon: 0.7670015200050582    steps: 335    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1114   score: 0.0   memory length: 217798   epsilon: 0.7667579800050635    steps: 123    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1115   score: 2.0   memory length: 217996   epsilon: 0.766365940005072    steps: 198    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1116   score: 2.0   memory length: 218214   epsilon: 0.7659343000050813    steps: 218    lr: 0.0001     evaluation reward: 3.06\n",
      "episode: 1117   score: 3.0   memory length: 218444   epsilon: 0.7654789000050912    steps: 230    lr: 0.0001     evaluation reward: 3.07\n",
      "episode: 1118   score: 3.0   memory length: 218708   epsilon: 0.7649561800051026    steps: 264    lr: 0.0001     evaluation reward: 3.1\n",
      "episode: 1119   score: 5.0   memory length: 219014   epsilon: 0.7643503000051157    steps: 306    lr: 0.0001     evaluation reward: 3.13\n",
      "episode: 1120   score: 5.0   memory length: 219320   epsilon: 0.7637444200051289    steps: 306    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1121   score: 2.0   memory length: 219518   epsilon: 0.7633523800051374    steps: 198    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1122   score: 3.0   memory length: 219749   epsilon: 0.7628950000051473    steps: 231    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1123   score: 4.0   memory length: 220041   epsilon: 0.7623168400051599    steps: 292    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1124   score: 8.0   memory length: 220361   epsilon: 0.7616832400051736    steps: 320    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1125   score: 3.0   memory length: 220611   epsilon: 0.7611882400051844    steps: 250    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1126   score: 3.0   memory length: 220836   epsilon: 0.760742740005194    steps: 225    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1127   score: 3.0   memory length: 221106   epsilon: 0.7602081400052056    steps: 270    lr: 0.0001     evaluation reward: 3.28\n",
      "episode: 1128   score: 3.0   memory length: 221352   epsilon: 0.7597210600052162    steps: 246    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1129   score: 2.0   memory length: 221569   epsilon: 0.7592914000052255    steps: 217    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1130   score: 1.0   memory length: 221738   epsilon: 0.7589567800052328    steps: 169    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1131   score: 2.0   memory length: 221937   epsilon: 0.7585627600052414    steps: 199    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1132   score: 2.0   memory length: 222135   epsilon: 0.7581707200052499    steps: 198    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1133   score: 4.0   memory length: 222431   epsilon: 0.7575846400052626    steps: 296    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1134   score: 4.0   memory length: 222709   epsilon: 0.7570342000052745    steps: 278    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1135   score: 2.0   memory length: 222907   epsilon: 0.7566421600052831    steps: 198    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1136   score: 5.0   memory length: 223231   epsilon: 0.756000640005297    steps: 324    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1137   score: 2.0   memory length: 223428   epsilon: 0.7556105800053055    steps: 197    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1138   score: 4.0   memory length: 223671   epsilon: 0.7551294400053159    steps: 243    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1139   score: 4.0   memory length: 223931   epsilon: 0.7546146400053271    steps: 260    lr: 0.0001     evaluation reward: 3.3\n",
      "episode: 1140   score: 3.0   memory length: 224196   epsilon: 0.7540899400053385    steps: 265    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1141   score: 3.0   memory length: 224426   epsilon: 0.7536345400053484    steps: 230    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1142   score: 4.0   memory length: 224685   epsilon: 0.7531217200053595    steps: 259    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1143   score: 2.0   memory length: 224883   epsilon: 0.752729680005368    steps: 198    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1144   score: 6.0   memory length: 225239   epsilon: 0.7520248000053833    steps: 356    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1145   score: 3.0   memory length: 225486   epsilon: 0.7515357400053939    steps: 247    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1146   score: 2.0   memory length: 225684   epsilon: 0.7511437000054024    steps: 198    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1147   score: 3.0   memory length: 225930   epsilon: 0.750656620005413    steps: 246    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1148   score: 6.0   memory length: 226323   epsilon: 0.7498784800054299    steps: 393    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1149   score: 3.0   memory length: 226548   epsilon: 0.7494329800054396    steps: 225    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1150   score: 2.0   memory length: 226727   epsilon: 0.7490785600054473    steps: 179    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1151   score: 3.0   memory length: 226974   epsilon: 0.7485895000054579    steps: 247    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1152   score: 1.0   memory length: 227142   epsilon: 0.7482568600054651    steps: 168    lr: 0.0001     evaluation reward: 3.16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1153   score: 3.0   memory length: 227372   epsilon: 0.747801460005475    steps: 230    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1154   score: 5.0   memory length: 227701   epsilon: 0.7471500400054891    steps: 329    lr: 0.0001     evaluation reward: 3.11\n",
      "episode: 1155   score: 4.0   memory length: 228018   epsilon: 0.7465223800055028    steps: 317    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1156   score: 3.0   memory length: 228249   epsilon: 0.7460650000055127    steps: 231    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1157   score: 4.0   memory length: 228546   epsilon: 0.7454769400055254    steps: 297    lr: 0.0001     evaluation reward: 3.12\n",
      "episode: 1158   score: 7.0   memory length: 228907   epsilon: 0.744762160005541    steps: 361    lr: 0.0001     evaluation reward: 3.16\n",
      "episode: 1159   score: 4.0   memory length: 229200   epsilon: 0.7441820200055536    steps: 293    lr: 0.0001     evaluation reward: 3.15\n",
      "episode: 1160   score: 8.0   memory length: 229638   epsilon: 0.7433147800055724    steps: 438    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1161   score: 2.0   memory length: 229856   epsilon: 0.7428831400055818    steps: 218    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1162   score: 4.0   memory length: 230116   epsilon: 0.7423683400055929    steps: 260    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1163   score: 3.0   memory length: 230362   epsilon: 0.7418812600056035    steps: 246    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1164   score: 1.0   memory length: 230512   epsilon: 0.74158426000561    steps: 150    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1165   score: 3.0   memory length: 230757   epsilon: 0.7410991600056205    steps: 245    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1166   score: 3.0   memory length: 231003   epsilon: 0.7406120800056311    steps: 246    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1167   score: 3.0   memory length: 231214   epsilon: 0.7401943000056401    steps: 211    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1168   score: 6.0   memory length: 231548   epsilon: 0.7395329800056545    steps: 334    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1169   score: 5.0   memory length: 231869   epsilon: 0.7388974000056683    steps: 321    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1170   score: 3.0   memory length: 232115   epsilon: 0.7384103200056789    steps: 246    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1171   score: 2.0   memory length: 232296   epsilon: 0.7380519400056866    steps: 181    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1172   score: 2.0   memory length: 232478   epsilon: 0.7376915800056945    steps: 182    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1173   score: 5.0   memory length: 232802   epsilon: 0.7370500600057084    steps: 324    lr: 0.0001     evaluation reward: 3.19\n",
      "episode: 1174   score: 5.0   memory length: 233126   epsilon: 0.7364085400057223    steps: 324    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1175   score: 2.0   memory length: 233308   epsilon: 0.7360481800057301    steps: 182    lr: 0.0001     evaluation reward: 3.2\n",
      "episode: 1176   score: 7.0   memory length: 233679   epsilon: 0.7353136000057461    steps: 371    lr: 0.0001     evaluation reward: 3.23\n",
      "episode: 1177   score: 1.0   memory length: 233830   epsilon: 0.7350146200057526    steps: 151    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1178   score: 9.0   memory length: 234329   epsilon: 0.734026600005774    steps: 499    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1179   score: 2.0   memory length: 234526   epsilon: 0.7336365400057825    steps: 197    lr: 0.0001     evaluation reward: 3.26\n",
      "episode: 1180   score: 3.0   memory length: 234773   epsilon: 0.7331474800057931    steps: 247    lr: 0.0001     evaluation reward: 3.27\n",
      "episode: 1181   score: 0.0   memory length: 234896   epsilon: 0.7329039400057984    steps: 123    lr: 0.0001     evaluation reward: 3.25\n",
      "episode: 1182   score: 2.0   memory length: 235096   epsilon: 0.732507940005807    steps: 200    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1183   score: 5.0   memory length: 235443   epsilon: 0.7318208800058219    steps: 347    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1184   score: 3.0   memory length: 235710   epsilon: 0.7312922200058334    steps: 267    lr: 0.0001     evaluation reward: 3.21\n",
      "episode: 1185   score: 4.0   memory length: 235985   epsilon: 0.7307477200058452    steps: 275    lr: 0.0001     evaluation reward: 3.18\n",
      "episode: 1186   score: 2.0   memory length: 236167   epsilon: 0.730387360005853    steps: 182    lr: 0.0001     evaluation reward: 3.17\n",
      "episode: 1187   score: 5.0   memory length: 236510   epsilon: 0.7297082200058678    steps: 343    lr: 0.0001     evaluation reward: 3.22\n",
      "episode: 1188   score: 4.0   memory length: 236788   epsilon: 0.7291577800058797    steps: 278    lr: 0.0001     evaluation reward: 3.24\n",
      "episode: 1189   score: 7.0   memory length: 237193   epsilon: 0.7283558800058971    steps: 405    lr: 0.0001     evaluation reward: 3.29\n",
      "episode: 1190   score: 4.0   memory length: 237496   epsilon: 0.7277559400059102    steps: 303    lr: 0.0001     evaluation reward: 3.31\n",
      "episode: 1191   score: 5.0   memory length: 237841   epsilon: 0.727072840005925    steps: 345    lr: 0.0001     evaluation reward: 3.33\n",
      "episode: 1192   score: 6.0   memory length: 238186   epsilon: 0.7263897400059398    steps: 345    lr: 0.0001     evaluation reward: 3.38\n",
      "episode: 1193   score: 7.0   memory length: 238590   epsilon: 0.7255898200059572    steps: 404    lr: 0.0001     evaluation reward: 3.44\n",
      "episode: 1194   score: 5.0   memory length: 238896   epsilon: 0.7249839400059703    steps: 306    lr: 0.0001     evaluation reward: 3.46\n",
      "episode: 1195   score: 7.0   memory length: 239249   epsilon: 0.7242850000059855    steps: 353    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1196   score: 4.0   memory length: 239500   epsilon: 0.7237880200059963    steps: 251    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1197   score: 2.0   memory length: 239698   epsilon: 0.7233959800060048    steps: 198    lr: 0.0001     evaluation reward: 3.5\n",
      "episode: 1198   score: 7.0   memory length: 240048   epsilon: 0.7227029800060198    steps: 350    lr: 0.0001     evaluation reward: 3.52\n",
      "episode: 1199   score: 4.0   memory length: 240343   epsilon: 0.7221188800060325    steps: 295    lr: 0.0001     evaluation reward: 3.51\n",
      "episode: 1200   score: 6.0   memory length: 240712   epsilon: 0.7213882600060484    steps: 369    lr: 0.0001     evaluation reward: 3.55\n",
      "episode: 1201   score: 5.0   memory length: 241038   epsilon: 0.7207427800060624    steps: 326    lr: 0.0001     evaluation reward: 3.58\n",
      "episode: 1202   score: 3.0   memory length: 241283   epsilon: 0.7202576800060729    steps: 245    lr: 0.0001     evaluation reward: 3.61\n",
      "episode: 1203   score: 2.0   memory length: 241483   epsilon: 0.7198616800060815    steps: 200    lr: 0.0001     evaluation reward: 3.59\n",
      "episode: 1204   score: 5.0   memory length: 241829   epsilon: 0.7191766000060964    steps: 346    lr: 0.0001     evaluation reward: 3.64\n",
      "episode: 1205   score: 2.0   memory length: 242011   epsilon: 0.7188162400061042    steps: 182    lr: 0.0001     evaluation reward: 3.6\n",
      "episode: 1206   score: 3.0   memory length: 242257   epsilon: 0.7183291600061148    steps: 246    lr: 0.0001     evaluation reward: 3.63\n",
      "episode: 1207   score: 6.0   memory length: 242632   epsilon: 0.7175866600061309    steps: 375    lr: 0.0001     evaluation reward: 3.68\n",
      "episode: 1208   score: 2.0   memory length: 242830   epsilon: 0.7171946200061394    steps: 198    lr: 0.0001     evaluation reward: 3.68\n",
      "episode: 1209   score: 6.0   memory length: 243199   epsilon: 0.7164640000061553    steps: 369    lr: 0.0001     evaluation reward: 3.7\n",
      "episode: 1210   score: 3.0   memory length: 243426   epsilon: 0.716014540006165    steps: 227    lr: 0.0001     evaluation reward: 3.71\n",
      "episode: 1211   score: 3.0   memory length: 243673   epsilon: 0.7155254800061757    steps: 247    lr: 0.0001     evaluation reward: 3.7\n",
      "episode: 1212   score: 2.0   memory length: 243889   epsilon: 0.715097800006185    steps: 216    lr: 0.0001     evaluation reward: 3.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1213   score: 6.0   memory length: 244242   epsilon: 0.7143988600062001    steps: 353    lr: 0.0001     evaluation reward: 3.68\n",
      "episode: 1214   score: 5.0   memory length: 244546   epsilon: 0.7137969400062132    steps: 304    lr: 0.0001     evaluation reward: 3.73\n",
      "episode: 1215   score: 6.0   memory length: 244872   epsilon: 0.7131514600062272    steps: 326    lr: 0.0001     evaluation reward: 3.77\n",
      "episode: 1216   score: 4.0   memory length: 245149   epsilon: 0.7126030000062391    steps: 277    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1217   score: 5.0   memory length: 245437   epsilon: 0.7120327600062515    steps: 288    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1218   score: 3.0   memory length: 245700   epsilon: 0.7115120200062628    steps: 263    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1219   score: 4.0   memory length: 245975   epsilon: 0.7109675200062746    steps: 275    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1220   score: 3.0   memory length: 246220   epsilon: 0.7104824200062851    steps: 245    lr: 0.0001     evaluation reward: 3.78\n",
      "episode: 1221   score: 1.0   memory length: 246370   epsilon: 0.7101854200062916    steps: 150    lr: 0.0001     evaluation reward: 3.77\n",
      "episode: 1222   score: 6.0   memory length: 246714   epsilon: 0.7095043000063064    steps: 344    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1223   score: 7.0   memory length: 247103   epsilon: 0.7087340800063231    steps: 389    lr: 0.0001     evaluation reward: 3.83\n",
      "episode: 1224   score: 5.0   memory length: 247429   epsilon: 0.7080886000063371    steps: 326    lr: 0.0001     evaluation reward: 3.8\n",
      "episode: 1225   score: 4.0   memory length: 247705   epsilon: 0.707542120006349    steps: 276    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1226   score: 4.0   memory length: 247966   epsilon: 0.7070253400063602    steps: 261    lr: 0.0001     evaluation reward: 3.82\n",
      "episode: 1227   score: 0.0   memory length: 248089   epsilon: 0.7067818000063655    steps: 123    lr: 0.0001     evaluation reward: 3.79\n",
      "episode: 1228   score: 5.0   memory length: 248388   epsilon: 0.7061897800063783    steps: 299    lr: 0.0001     evaluation reward: 3.81\n",
      "episode: 1229   score: 7.0   memory length: 248790   epsilon: 0.7053938200063956    steps: 402    lr: 0.0001     evaluation reward: 3.86\n",
      "episode: 1230   score: 4.0   memory length: 249069   epsilon: 0.7048414000064076    steps: 279    lr: 0.0001     evaluation reward: 3.89\n",
      "episode: 1231   score: 3.0   memory length: 249298   epsilon: 0.7043879800064174    steps: 229    lr: 0.0001     evaluation reward: 3.9\n",
      "episode: 1232   score: 7.0   memory length: 249691   epsilon: 0.7036098400064343    steps: 393    lr: 0.0001     evaluation reward: 3.95\n",
      "episode: 1233   score: 9.0   memory length: 250173   epsilon: 0.7026554800064551    steps: 482    lr: 0.0001     evaluation reward: 4.0\n",
      "episode: 1234   score: 7.0   memory length: 250615   epsilon: 0.7017803200064741    steps: 442    lr: 0.0001     evaluation reward: 4.03\n",
      "episode: 1235   score: 3.0   memory length: 250884   epsilon: 0.7012477000064856    steps: 269    lr: 0.0001     evaluation reward: 4.04\n",
      "episode: 1236   score: 6.0   memory length: 251243   epsilon: 0.700536880006501    steps: 359    lr: 0.0001     evaluation reward: 4.05\n",
      "episode: 1237   score: 5.0   memory length: 251539   epsilon: 0.6999508000065138    steps: 296    lr: 0.0001     evaluation reward: 4.08\n",
      "episode: 1238   score: 7.0   memory length: 251928   epsilon: 0.6991805800065305    steps: 389    lr: 0.0001     evaluation reward: 4.11\n",
      "episode: 1239   score: 5.0   memory length: 252235   epsilon: 0.6985727200065437    steps: 307    lr: 0.0001     evaluation reward: 4.12\n",
      "episode: 1240   score: 6.0   memory length: 252614   epsilon: 0.69782230000656    steps: 379    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 1241   score: 4.0   memory length: 252892   epsilon: 0.6972718600065719    steps: 278    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 1242   score: 5.0   memory length: 253186   epsilon: 0.6966897400065846    steps: 294    lr: 0.0001     evaluation reward: 4.17\n",
      "episode: 1243   score: 3.0   memory length: 253412   epsilon: 0.6962422600065943    steps: 226    lr: 0.0001     evaluation reward: 4.18\n",
      "episode: 1244   score: 2.0   memory length: 253593   epsilon: 0.6958838800066021    steps: 181    lr: 0.0001     evaluation reward: 4.14\n",
      "episode: 1245   score: 4.0   memory length: 253850   epsilon: 0.6953750200066131    steps: 257    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 1246   score: 3.0   memory length: 254079   epsilon: 0.694921600006623    steps: 229    lr: 0.0001     evaluation reward: 4.16\n",
      "episode: 1247   score: 0.0   memory length: 254201   epsilon: 0.6946800400066282    steps: 122    lr: 0.0001     evaluation reward: 4.13\n",
      "episode: 1248   score: 4.0   memory length: 254477   epsilon: 0.6941335600066401    steps: 276    lr: 0.0001     evaluation reward: 4.11\n",
      "episode: 1249   score: 7.0   memory length: 254919   epsilon: 0.6932584000066591    steps: 442    lr: 0.0001     evaluation reward: 4.15\n",
      "episode: 1250   score: 9.0   memory length: 255241   epsilon: 0.6926208400066729    steps: 322    lr: 0.0001     evaluation reward: 4.22\n",
      "episode: 1251   score: 5.0   memory length: 255572   epsilon: 0.6919654600066871    steps: 331    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 1252   score: 6.0   memory length: 255967   epsilon: 0.6911833600067041    steps: 395    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1253   score: 6.0   memory length: 256321   epsilon: 0.6904824400067193    steps: 354    lr: 0.0001     evaluation reward: 4.32\n",
      "episode: 1254   score: 2.0   memory length: 256523   epsilon: 0.690082480006728    steps: 202    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1255   score: 3.0   memory length: 256752   epsilon: 0.6896290600067378    steps: 229    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 1256   score: 3.0   memory length: 256997   epsilon: 0.6891439600067484    steps: 245    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 1257   score: 3.0   memory length: 257243   epsilon: 0.688656880006759    steps: 246    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 1258   score: 4.0   memory length: 257537   epsilon: 0.6880747600067716    steps: 294    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 1259   score: 0.0   memory length: 257660   epsilon: 0.6878312200067769    steps: 123    lr: 0.0001     evaluation reward: 4.2\n",
      "episode: 1260   score: 5.0   memory length: 257985   epsilon: 0.6871877200067908    steps: 325    lr: 0.0001     evaluation reward: 4.17\n",
      "episode: 1261   score: 9.0   memory length: 258398   epsilon: 0.6863699800068086    steps: 413    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 1262   score: 9.0   memory length: 258725   epsilon: 0.6857225200068227    steps: 327    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1263   score: 3.0   memory length: 258969   epsilon: 0.6852394000068331    steps: 244    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1264   score: 2.0   memory length: 259167   epsilon: 0.6848473600068417    steps: 198    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 1265   score: 3.0   memory length: 259432   epsilon: 0.684322660006853    steps: 265    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 1266   score: 5.0   memory length: 259760   epsilon: 0.6836732200068671    steps: 328    lr: 0.0001     evaluation reward: 4.32\n",
      "episode: 1267   score: 1.0   memory length: 259911   epsilon: 0.6833742400068736    steps: 151    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 1268   score: 3.0   memory length: 260157   epsilon: 0.6828871600068842    steps: 246    lr: 0.0001     evaluation reward: 4.27\n",
      "episode: 1269   score: 3.0   memory length: 260404   epsilon: 0.6823981000068948    steps: 247    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1270   score: 7.0   memory length: 260843   epsilon: 0.6815288800069137    steps: 439    lr: 0.0001     evaluation reward: 4.29\n",
      "episode: 1271   score: 4.0   memory length: 261157   epsilon: 0.6809071600069272    steps: 314    lr: 0.0001     evaluation reward: 4.31\n",
      "episode: 1272   score: 1.0   memory length: 261307   epsilon: 0.6806101600069336    steps: 150    lr: 0.0001     evaluation reward: 4.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1273   score: 3.0   memory length: 261554   epsilon: 0.6801211000069443    steps: 247    lr: 0.0001     evaluation reward: 4.28\n",
      "episode: 1274   score: 0.0   memory length: 261676   epsilon: 0.6798795400069495    steps: 122    lr: 0.0001     evaluation reward: 4.23\n",
      "episode: 1275   score: 4.0   memory length: 261971   epsilon: 0.6792954400069622    steps: 295    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1276   score: 7.0   memory length: 262376   epsilon: 0.6784935400069796    steps: 405    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1277   score: 2.0   memory length: 262595   epsilon: 0.678059920006989    steps: 219    lr: 0.0001     evaluation reward: 4.26\n",
      "episode: 1278   score: 7.0   memory length: 263019   epsilon: 0.6772204000070072    steps: 424    lr: 0.0001     evaluation reward: 4.24\n",
      "episode: 1279   score: 3.0   memory length: 263228   epsilon: 0.6768065800070162    steps: 209    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1280   score: 3.0   memory length: 263475   epsilon: 0.6763175200070268    steps: 247    lr: 0.0001     evaluation reward: 4.25\n",
      "episode: 1281   score: 6.0   memory length: 263831   epsilon: 0.6756126400070421    steps: 356    lr: 0.0001     evaluation reward: 4.31\n",
      "episode: 1282   score: 3.0   memory length: 264077   epsilon: 0.6751255600070527    steps: 246    lr: 0.0001     evaluation reward: 4.32\n",
      "episode: 1283   score: 12.0   memory length: 264663   epsilon: 0.6739652800070779    steps: 586    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1284   score: 2.0   memory length: 264844   epsilon: 0.6736069000070857    steps: 181    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 1285   score: 3.0   memory length: 265089   epsilon: 0.6731218000070962    steps: 245    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 1286   score: 2.0   memory length: 265309   epsilon: 0.6726862000071057    steps: 220    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 1287   score: 8.0   memory length: 265752   epsilon: 0.6718090600071247    steps: 443    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1288   score: 6.0   memory length: 266090   epsilon: 0.6711398200071392    steps: 338    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1289   score: 4.0   memory length: 266385   epsilon: 0.6705557200071519    steps: 295    lr: 0.0001     evaluation reward: 4.39\n",
      "episode: 1290   score: 7.0   memory length: 266792   epsilon: 0.6697498600071694    steps: 407    lr: 0.0001     evaluation reward: 4.42\n",
      "episode: 1291   score: 3.0   memory length: 267018   epsilon: 0.6693023800071791    steps: 226    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1292   score: 6.0   memory length: 267362   epsilon: 0.6686212600071939    steps: 344    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1293   score: 3.0   memory length: 267609   epsilon: 0.6681322000072045    steps: 247    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 1294   score: 4.0   memory length: 267905   epsilon: 0.6675461200072172    steps: 296    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 1295   score: 7.0   memory length: 268273   epsilon: 0.6668174800072331    steps: 368    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 1296   score: 5.0   memory length: 268596   epsilon: 0.666177940007247    steps: 323    lr: 0.0001     evaluation reward: 4.36\n",
      "episode: 1297   score: 4.0   memory length: 268856   epsilon: 0.6656631400072581    steps: 260    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 1298   score: 4.0   memory length: 269098   epsilon: 0.6651839800072685    steps: 242    lr: 0.0001     evaluation reward: 4.35\n",
      "episode: 1299   score: 3.0   memory length: 269327   epsilon: 0.6647305600072784    steps: 229    lr: 0.0001     evaluation reward: 4.34\n",
      "episode: 1300   score: 2.0   memory length: 269524   epsilon: 0.6643405000072868    steps: 197    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 1301   score: 5.0   memory length: 269846   epsilon: 0.6637029400073007    steps: 322    lr: 0.0001     evaluation reward: 4.3\n",
      "episode: 1302   score: 6.0   memory length: 270204   epsilon: 0.6629941000073161    steps: 358    lr: 0.0001     evaluation reward: 4.33\n",
      "episode: 1303   score: 6.0   memory length: 270556   epsilon: 0.6622971400073312    steps: 352    lr: 0.0001     evaluation reward: 4.37\n",
      "episode: 1304   score: 6.0   memory length: 270915   epsilon: 0.6615863200073466    steps: 359    lr: 0.0001     evaluation reward: 4.38\n",
      "episode: 1305   score: 4.0   memory length: 271174   epsilon: 0.6610735000073578    steps: 259    lr: 0.0001     evaluation reward: 4.4\n",
      "episode: 1306   score: 8.0   memory length: 271619   epsilon: 0.6601924000073769    steps: 445    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1307   score: 6.0   memory length: 271957   epsilon: 0.6595231600073914    steps: 338    lr: 0.0001     evaluation reward: 4.45\n",
      "episode: 1308   score: 6.0   memory length: 272293   epsilon: 0.6588578800074059    steps: 336    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 1309   score: 5.0   memory length: 272605   epsilon: 0.6582401200074193    steps: 312    lr: 0.0001     evaluation reward: 4.48\n",
      "episode: 1310   score: 4.0   memory length: 272897   epsilon: 0.6576619600074318    steps: 292    lr: 0.0001     evaluation reward: 4.49\n",
      "episode: 1311   score: 8.0   memory length: 273327   epsilon: 0.6568105600074503    steps: 430    lr: 0.0001     evaluation reward: 4.54\n",
      "episode: 1312   score: 3.0   memory length: 273552   epsilon: 0.65636506000746    steps: 225    lr: 0.0001     evaluation reward: 4.55\n",
      "episode: 1313   score: 4.0   memory length: 273818   epsilon: 0.6558383800074714    steps: 266    lr: 0.0001     evaluation reward: 4.53\n",
      "episode: 1314   score: 12.0   memory length: 274310   epsilon: 0.6548642200074926    steps: 492    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 1315   score: 2.0   memory length: 274491   epsilon: 0.6545058400075003    steps: 181    lr: 0.0001     evaluation reward: 4.56\n",
      "episode: 1316   score: 7.0   memory length: 274914   epsilon: 0.6536683000075185    steps: 423    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 1317   score: 4.0   memory length: 275175   epsilon: 0.6531515200075297    steps: 261    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 1318   score: 3.0   memory length: 275402   epsilon: 0.6527020600075395    steps: 227    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 1319   score: 3.0   memory length: 275628   epsilon: 0.6522545800075492    steps: 226    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 1320   score: 3.0   memory length: 275854   epsilon: 0.6518071000075589    steps: 226    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 1321   score: 7.0   memory length: 276274   epsilon: 0.650975500007577    steps: 420    lr: 0.0001     evaluation reward: 4.63\n",
      "episode: 1322   score: 4.0   memory length: 276550   epsilon: 0.6504290200075888    steps: 276    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 1323   score: 7.0   memory length: 276935   epsilon: 0.6496667200076054    steps: 385    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 1324   score: 4.0   memory length: 277215   epsilon: 0.6491123200076174    steps: 280    lr: 0.0001     evaluation reward: 4.6\n",
      "episode: 1325   score: 5.0   memory length: 277541   epsilon: 0.6484668400076314    steps: 326    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 1326   score: 5.0   memory length: 277863   epsilon: 0.6478292800076453    steps: 322    lr: 0.0001     evaluation reward: 4.62\n",
      "episode: 1327   score: 6.0   memory length: 278197   epsilon: 0.6471679600076596    steps: 334    lr: 0.0001     evaluation reward: 4.68\n",
      "episode: 1328   score: 6.0   memory length: 278539   epsilon: 0.6464908000076743    steps: 342    lr: 0.0001     evaluation reward: 4.69\n",
      "episode: 1329   score: 3.0   memory length: 278788   epsilon: 0.645997780007685    steps: 249    lr: 0.0001     evaluation reward: 4.65\n",
      "episode: 1330   score: 3.0   memory length: 279034   epsilon: 0.6455107000076956    steps: 246    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 1331   score: 5.0   memory length: 279343   epsilon: 0.6448988800077089    steps: 309    lr: 0.0001     evaluation reward: 4.66\n",
      "episode: 1332   score: 5.0   memory length: 279646   epsilon: 0.6442989400077219    steps: 303    lr: 0.0001     evaluation reward: 4.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1333   score: 6.0   memory length: 280001   epsilon: 0.6435960400077372    steps: 355    lr: 0.0001     evaluation reward: 4.61\n",
      "episode: 1334   score: 3.0   memory length: 280230   epsilon: 0.643142620007747    steps: 229    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 1335   score: 4.0   memory length: 280530   epsilon: 0.6425486200077599    steps: 300    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 1336   score: 3.0   memory length: 280762   epsilon: 0.6420892600077699    steps: 232    lr: 0.0001     evaluation reward: 4.55\n",
      "episode: 1337   score: 8.0   memory length: 281144   epsilon: 0.6413329000077863    steps: 382    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 1338   score: 8.0   memory length: 281544   epsilon: 0.6405409000078035    steps: 400    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 1339   score: 3.0   memory length: 281810   epsilon: 0.6400142200078149    steps: 266    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 1340   score: 5.0   memory length: 282171   epsilon: 0.6392994400078305    steps: 361    lr: 0.0001     evaluation reward: 4.56\n",
      "episode: 1341   score: 6.0   memory length: 282534   epsilon: 0.6385807000078461    steps: 363    lr: 0.0001     evaluation reward: 4.58\n",
      "episode: 1342   score: 4.0   memory length: 282827   epsilon: 0.6380005600078587    steps: 293    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 1343   score: 3.0   memory length: 283040   epsilon: 0.6375788200078678    steps: 213    lr: 0.0001     evaluation reward: 4.57\n",
      "episode: 1344   score: 4.0   memory length: 283335   epsilon: 0.6369947200078805    steps: 295    lr: 0.0001     evaluation reward: 4.59\n",
      "episode: 1345   score: 9.0   memory length: 283838   epsilon: 0.6359987800079021    steps: 503    lr: 0.0001     evaluation reward: 4.64\n",
      "episode: 1346   score: 6.0   memory length: 284215   epsilon: 0.6352523200079183    steps: 377    lr: 0.0001     evaluation reward: 4.67\n",
      "episode: 1347   score: 6.0   memory length: 284592   epsilon: 0.6345058600079345    steps: 377    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 1348   score: 9.0   memory length: 285042   epsilon: 0.6336148600079539    steps: 450    lr: 0.0001     evaluation reward: 4.78\n",
      "episode: 1349   score: 8.0   memory length: 285457   epsilon: 0.6327931600079717    steps: 415    lr: 0.0001     evaluation reward: 4.79\n",
      "episode: 1350   score: 1.0   memory length: 285608   epsilon: 0.6324941800079782    steps: 151    lr: 0.0001     evaluation reward: 4.71\n",
      "episode: 1351   score: 4.0   memory length: 285883   epsilon: 0.63194968000799    steps: 275    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 1352   score: 6.0   memory length: 286242   epsilon: 0.6312388600080054    steps: 359    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 1353   score: 6.0   memory length: 286615   epsilon: 0.6305003200080215    steps: 373    lr: 0.0001     evaluation reward: 4.7\n",
      "episode: 1354   score: 5.0   memory length: 286909   epsilon: 0.6299182000080341    steps: 294    lr: 0.0001     evaluation reward: 4.73\n",
      "episode: 1355   score: 2.0   memory length: 287109   epsilon: 0.6295222000080427    steps: 200    lr: 0.0001     evaluation reward: 4.72\n",
      "episode: 1356   score: 5.0   memory length: 287438   epsilon: 0.6288707800080569    steps: 329    lr: 0.0001     evaluation reward: 4.74\n",
      "episode: 1357   score: 4.0   memory length: 287713   epsilon: 0.6283262800080687    steps: 275    lr: 0.0001     evaluation reward: 4.75\n",
      "episode: 1358   score: 4.0   memory length: 287988   epsilon: 0.6277817800080805    steps: 275    lr: 0.0001     evaluation reward: 4.75\n",
      "episode: 1359   score: 5.0   memory length: 288311   epsilon: 0.6271422400080944    steps: 323    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 1360   score: 10.0   memory length: 288758   epsilon: 0.6262571800081136    steps: 447    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 1361   score: 6.0   memory length: 289066   epsilon: 0.6256473400081268    steps: 308    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 1362   score: 4.0   memory length: 289366   epsilon: 0.6250533400081397    steps: 300    lr: 0.0001     evaluation reward: 4.77\n",
      "episode: 1363   score: 6.0   memory length: 289711   epsilon: 0.6243702400081546    steps: 345    lr: 0.0001     evaluation reward: 4.8\n",
      "episode: 1364   score: 4.0   memory length: 289970   epsilon: 0.6238574200081657    steps: 259    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 1365   score: 3.0   memory length: 290183   epsilon: 0.6234356800081748    steps: 213    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 1366   score: 5.0   memory length: 290495   epsilon: 0.6228179200081883    steps: 312    lr: 0.0001     evaluation reward: 4.82\n",
      "episode: 1367   score: 4.0   memory length: 290759   epsilon: 0.6222952000081996    steps: 264    lr: 0.0001     evaluation reward: 4.85\n",
      "episode: 1368   score: 5.0   memory length: 291069   epsilon: 0.6216814000082129    steps: 310    lr: 0.0001     evaluation reward: 4.87\n",
      "episode: 1369   score: 6.0   memory length: 291426   epsilon: 0.6209745400082283    steps: 357    lr: 0.0001     evaluation reward: 4.9\n",
      "episode: 1370   score: 5.0   memory length: 291769   epsilon: 0.620295400008243    steps: 343    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 1371   score: 2.0   memory length: 291967   epsilon: 0.6199033600082515    steps: 198    lr: 0.0001     evaluation reward: 4.86\n",
      "episode: 1372   score: 3.0   memory length: 292198   epsilon: 0.6194459800082615    steps: 231    lr: 0.0001     evaluation reward: 4.88\n",
      "episode: 1373   score: 8.0   memory length: 292616   epsilon: 0.6186183400082794    steps: 418    lr: 0.0001     evaluation reward: 4.93\n",
      "episode: 1374   score: 5.0   memory length: 292921   epsilon: 0.6180144400082925    steps: 305    lr: 0.0001     evaluation reward: 4.98\n",
      "episode: 1375   score: 3.0   memory length: 293150   epsilon: 0.6175610200083024    steps: 229    lr: 0.0001     evaluation reward: 4.97\n",
      "episode: 1376   score: 9.0   memory length: 293642   epsilon: 0.6165868600083235    steps: 492    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 1377   score: 3.0   memory length: 293869   epsilon: 0.6161374000083333    steps: 227    lr: 0.0001     evaluation reward: 5.0\n",
      "episode: 1378   score: 3.0   memory length: 294102   epsilon: 0.6156760600083433    steps: 233    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 1379   score: 3.0   memory length: 294349   epsilon: 0.6151870000083539    steps: 247    lr: 0.0001     evaluation reward: 4.96\n",
      "episode: 1380   score: 6.0   memory length: 294704   epsilon: 0.6144841000083692    steps: 355    lr: 0.0001     evaluation reward: 4.99\n",
      "episode: 1381   score: 10.0   memory length: 295127   epsilon: 0.6136465600083874    steps: 423    lr: 0.0001     evaluation reward: 5.03\n",
      "episode: 1382   score: 6.0   memory length: 295491   epsilon: 0.612925840008403    steps: 364    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 1383   score: 8.0   memory length: 295787   epsilon: 0.6123397600084157    steps: 296    lr: 0.0001     evaluation reward: 5.02\n",
      "episode: 1384   score: 6.0   memory length: 296145   epsilon: 0.6116309200084311    steps: 358    lr: 0.0001     evaluation reward: 5.06\n",
      "episode: 1385   score: 6.0   memory length: 296525   epsilon: 0.6108785200084474    steps: 380    lr: 0.0001     evaluation reward: 5.09\n",
      "episode: 1386   score: 11.0   memory length: 297075   epsilon: 0.6097895200084711    steps: 550    lr: 0.0001     evaluation reward: 5.18\n",
      "episode: 1387   score: 5.0   memory length: 297366   epsilon: 0.6092133400084836    steps: 291    lr: 0.0001     evaluation reward: 5.15\n",
      "episode: 1388   score: 15.0   memory length: 297957   epsilon: 0.608043160008509    steps: 591    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 1389   score: 6.0   memory length: 298294   epsilon: 0.6073759000085235    steps: 337    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 1390   score: 7.0   memory length: 298737   epsilon: 0.6064987600085425    steps: 443    lr: 0.0001     evaluation reward: 5.26\n",
      "episode: 1391   score: 4.0   memory length: 299033   epsilon: 0.6059126800085552    steps: 296    lr: 0.0001     evaluation reward: 5.27\n",
      "episode: 1392   score: 4.0   memory length: 299308   epsilon: 0.6053681800085671    steps: 275    lr: 0.0001     evaluation reward: 5.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1393   score: 3.0   memory length: 299537   epsilon: 0.6049147600085769    steps: 229    lr: 0.0001     evaluation reward: 5.25\n",
      "episode: 1394   score: 3.0   memory length: 299766   epsilon: 0.6044613400085868    steps: 229    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 1395   score: 6.0   memory length: 300140   epsilon: 0.6037208200086028    steps: 374    lr: 0.0001     evaluation reward: 5.23\n",
      "episode: 1396   score: 6.0   memory length: 300501   epsilon: 0.6030060400086183    steps: 361    lr: 0.0001     evaluation reward: 5.24\n",
      "episode: 1397   score: 9.0   memory length: 300936   epsilon: 0.602144740008637    steps: 435    lr: 0.0001     evaluation reward: 5.29\n",
      "episode: 1398   score: 5.0   memory length: 301246   epsilon: 0.6015309400086504    steps: 310    lr: 0.0001     evaluation reward: 5.3\n",
      "episode: 1399   score: 9.0   memory length: 301686   epsilon: 0.6006597400086693    steps: 440    lr: 0.0001     evaluation reward: 5.36\n",
      "episode: 1400   score: 9.0   memory length: 302178   epsilon: 0.5996855800086904    steps: 492    lr: 0.0001     evaluation reward: 5.43\n",
      "episode: 1401   score: 6.0   memory length: 302550   epsilon: 0.5989490200087064    steps: 372    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 1402   score: 11.0   memory length: 303090   epsilon: 0.5978798200087296    steps: 540    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 1403   score: 7.0   memory length: 303493   epsilon: 0.597081880008747    steps: 403    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 1404   score: 4.0   memory length: 303789   epsilon: 0.5964958000087597    steps: 296    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 1405   score: 4.0   memory length: 304084   epsilon: 0.5959117000087724    steps: 295    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 1406   score: 4.0   memory length: 304362   epsilon: 0.5953612600087843    steps: 278    lr: 0.0001     evaluation reward: 5.44\n",
      "episode: 1407   score: 7.0   memory length: 304747   epsilon: 0.5945989600088009    steps: 385    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 1408   score: 6.0   memory length: 305109   epsilon: 0.5938822000088164    steps: 362    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 1409   score: 8.0   memory length: 305512   epsilon: 0.5930842600088337    steps: 403    lr: 0.0001     evaluation reward: 5.48\n",
      "episode: 1410   score: 6.0   memory length: 305868   epsilon: 0.592379380008849    steps: 356    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 1411   score: 7.0   memory length: 306272   epsilon: 0.5915794600088664    steps: 404    lr: 0.0001     evaluation reward: 5.49\n",
      "episode: 1412   score: 4.0   memory length: 306531   epsilon: 0.5910666400088775    steps: 259    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 1413   score: 8.0   memory length: 306968   epsilon: 0.5902013800088963    steps: 437    lr: 0.0001     evaluation reward: 5.54\n",
      "episode: 1414   score: 3.0   memory length: 307215   epsilon: 0.5897123200089069    steps: 247    lr: 0.0001     evaluation reward: 5.45\n",
      "episode: 1415   score: 8.0   memory length: 307639   epsilon: 0.5888728000089252    steps: 424    lr: 0.0001     evaluation reward: 5.51\n",
      "episode: 1416   score: 6.0   memory length: 308035   epsilon: 0.5880887200089422    steps: 396    lr: 0.0001     evaluation reward: 5.5\n",
      "episode: 1417   score: 6.0   memory length: 308407   epsilon: 0.5873521600089582    steps: 372    lr: 0.0001     evaluation reward: 5.52\n",
      "episode: 1418   score: 8.0   memory length: 308835   epsilon: 0.5865047200089766    steps: 428    lr: 0.0001     evaluation reward: 5.57\n",
      "episode: 1419   score: 12.0   memory length: 309282   epsilon: 0.5856196600089958    steps: 447    lr: 0.0001     evaluation reward: 5.66\n",
      "episode: 1420   score: 8.0   memory length: 309658   epsilon: 0.584875180009012    steps: 376    lr: 0.0001     evaluation reward: 5.71\n",
      "episode: 1421   score: 7.0   memory length: 310049   epsilon: 0.5841010000090288    steps: 391    lr: 0.0001     evaluation reward: 5.71\n",
      "episode: 1422   score: 5.0   memory length: 310394   epsilon: 0.5834179000090436    steps: 345    lr: 0.0001     evaluation reward: 5.72\n",
      "episode: 1423   score: 7.0   memory length: 310770   epsilon: 0.5826734200090598    steps: 376    lr: 0.0001     evaluation reward: 5.72\n",
      "episode: 1424   score: 5.0   memory length: 311096   epsilon: 0.5820279400090738    steps: 326    lr: 0.0001     evaluation reward: 5.73\n",
      "episode: 1425   score: 3.0   memory length: 311307   epsilon: 0.5816101600090828    steps: 211    lr: 0.0001     evaluation reward: 5.71\n",
      "episode: 1426   score: 9.0   memory length: 311816   epsilon: 0.5806023400091047    steps: 509    lr: 0.0001     evaluation reward: 5.75\n",
      "episode: 1427   score: 5.0   memory length: 312089   epsilon: 0.5800618000091164    steps: 273    lr: 0.0001     evaluation reward: 5.74\n",
      "episode: 1428   score: 5.0   memory length: 312397   epsilon: 0.5794519600091297    steps: 308    lr: 0.0001     evaluation reward: 5.73\n",
      "episode: 1429   score: 3.0   memory length: 312623   epsilon: 0.5790044800091394    steps: 226    lr: 0.0001     evaluation reward: 5.73\n",
      "episode: 1430   score: 2.0   memory length: 312821   epsilon: 0.5786124400091479    steps: 198    lr: 0.0001     evaluation reward: 5.72\n",
      "episode: 1431   score: 14.0   memory length: 313355   epsilon: 0.5775551200091709    steps: 534    lr: 0.0001     evaluation reward: 5.81\n",
      "episode: 1432   score: 9.0   memory length: 313878   epsilon: 0.5765195800091933    steps: 523    lr: 0.0001     evaluation reward: 5.85\n",
      "episode: 1433   score: 4.0   memory length: 314154   epsilon: 0.5759731000092052    steps: 276    lr: 0.0001     evaluation reward: 5.83\n",
      "episode: 1434   score: 6.0   memory length: 314512   epsilon: 0.5752642600092206    steps: 358    lr: 0.0001     evaluation reward: 5.86\n",
      "episode: 1435   score: 3.0   memory length: 314758   epsilon: 0.5747771800092312    steps: 246    lr: 0.0001     evaluation reward: 5.85\n",
      "episode: 1436   score: 4.0   memory length: 315033   epsilon: 0.574232680009243    steps: 275    lr: 0.0001     evaluation reward: 5.86\n",
      "episode: 1437   score: 11.0   memory length: 315479   epsilon: 0.5733496000092622    steps: 446    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1438   score: 4.0   memory length: 315774   epsilon: 0.5727655000092748    steps: 295    lr: 0.0001     evaluation reward: 5.85\n",
      "episode: 1439   score: 6.0   memory length: 316148   epsilon: 0.5720249800092909    steps: 374    lr: 0.0001     evaluation reward: 5.88\n",
      "episode: 1440   score: 6.0   memory length: 316521   epsilon: 0.571286440009307    steps: 373    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1441   score: 7.0   memory length: 316899   epsilon: 0.5705380000093232    steps: 378    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 1442   score: 4.0   memory length: 317174   epsilon: 0.569993500009335    steps: 275    lr: 0.0001     evaluation reward: 5.9\n",
      "episode: 1443   score: 8.0   memory length: 317587   epsilon: 0.5691757600093528    steps: 413    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1444   score: 4.0   memory length: 317846   epsilon: 0.5686629400093639    steps: 259    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1445   score: 3.0   memory length: 318057   epsilon: 0.568245160009373    steps: 211    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1446   score: 5.0   memory length: 318361   epsilon: 0.567643240009386    steps: 304    lr: 0.0001     evaluation reward: 5.88\n",
      "episode: 1447   score: 7.0   memory length: 318786   epsilon: 0.5668017400094043    steps: 425    lr: 0.0001     evaluation reward: 5.89\n",
      "episode: 1448   score: 8.0   memory length: 319238   epsilon: 0.5659067800094237    steps: 452    lr: 0.0001     evaluation reward: 5.88\n",
      "episode: 1449   score: 5.0   memory length: 319565   epsilon: 0.5652593200094378    steps: 327    lr: 0.0001     evaluation reward: 5.85\n",
      "episode: 1450   score: 11.0   memory length: 320018   epsilon: 0.5643623800094573    steps: 453    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1451   score: 3.0   memory length: 320230   epsilon: 0.5639426200094664    steps: 212    lr: 0.0001     evaluation reward: 5.94\n",
      "episode: 1452   score: 7.0   memory length: 320634   epsilon: 0.5631427000094837    steps: 404    lr: 0.0001     evaluation reward: 5.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1453   score: 6.0   memory length: 320992   epsilon: 0.5624338600094991    steps: 358    lr: 0.0001     evaluation reward: 5.95\n",
      "episode: 1454   score: 6.0   memory length: 321368   epsilon: 0.5616893800095153    steps: 376    lr: 0.0001     evaluation reward: 5.96\n",
      "episode: 1455   score: 7.0   memory length: 321791   epsilon: 0.5608518400095335    steps: 423    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 1456   score: 10.0   memory length: 322313   epsilon: 0.5598182800095559    steps: 522    lr: 0.0001     evaluation reward: 6.06\n",
      "episode: 1457   score: 5.0   memory length: 322599   epsilon: 0.5592520000095682    steps: 286    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1458   score: 5.0   memory length: 322894   epsilon: 0.5586679000095809    steps: 295    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 1459   score: 4.0   memory length: 323192   epsilon: 0.5580778600095937    steps: 298    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1460   score: 4.0   memory length: 323469   epsilon: 0.5575294000096056    steps: 277    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 1461   score: 3.0   memory length: 323679   epsilon: 0.5571136000096146    steps: 210    lr: 0.0001     evaluation reward: 5.98\n",
      "episode: 1462   score: 4.0   memory length: 323955   epsilon: 0.5565671200096265    steps: 276    lr: 0.0001     evaluation reward: 5.98\n",
      "episode: 1463   score: 9.0   memory length: 324415   epsilon: 0.5556563200096463    steps: 460    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 1464   score: 7.0   memory length: 324821   epsilon: 0.5548524400096637    steps: 406    lr: 0.0001     evaluation reward: 6.04\n",
      "episode: 1465   score: 7.0   memory length: 325220   epsilon: 0.5540624200096809    steps: 399    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 1466   score: 4.0   memory length: 325477   epsilon: 0.5535535600096919    steps: 257    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1467   score: 11.0   memory length: 326063   epsilon: 0.5523932800097171    steps: 586    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 1468   score: 4.0   memory length: 326338   epsilon: 0.5518487800097289    steps: 275    lr: 0.0001     evaluation reward: 6.13\n",
      "episode: 1469   score: 5.0   memory length: 326662   epsilon: 0.5512072600097428    steps: 324    lr: 0.0001     evaluation reward: 6.12\n",
      "episode: 1470   score: 8.0   memory length: 327112   epsilon: 0.5503162600097622    steps: 450    lr: 0.0001     evaluation reward: 6.15\n",
      "episode: 1471   score: 9.0   memory length: 327622   epsilon: 0.5493064600097841    steps: 510    lr: 0.0001     evaluation reward: 6.22\n",
      "episode: 1472   score: 6.0   memory length: 327976   epsilon: 0.5486055400097993    steps: 354    lr: 0.0001     evaluation reward: 6.25\n",
      "episode: 1473   score: 7.0   memory length: 328364   epsilon: 0.547837300009816    steps: 388    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1474   score: 5.0   memory length: 328659   epsilon: 0.5472532000098287    steps: 295    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1475   score: 3.0   memory length: 328870   epsilon: 0.5468354200098378    steps: 211    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1476   score: 5.0   memory length: 329183   epsilon: 0.5462156800098512    steps: 313    lr: 0.0001     evaluation reward: 6.2\n",
      "episode: 1477   score: 4.0   memory length: 329479   epsilon: 0.5456296000098639    steps: 296    lr: 0.0001     evaluation reward: 6.21\n",
      "episode: 1478   score: 8.0   memory length: 329949   epsilon: 0.5446990000098841    steps: 470    lr: 0.0001     evaluation reward: 6.26\n",
      "episode: 1479   score: 5.0   memory length: 330293   epsilon: 0.5440178800098989    steps: 344    lr: 0.0001     evaluation reward: 6.28\n",
      "episode: 1480   score: 6.0   memory length: 330687   epsilon: 0.5432377600099159    steps: 394    lr: 0.0001     evaluation reward: 6.28\n",
      "episode: 1481   score: 6.0   memory length: 331032   epsilon: 0.5425546600099307    steps: 345    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1482   score: 5.0   memory length: 331339   epsilon: 0.5419468000099439    steps: 307    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1483   score: 12.0   memory length: 331824   epsilon: 0.5409865000099647    steps: 485    lr: 0.0001     evaluation reward: 6.27\n",
      "episode: 1484   score: 3.0   memory length: 332051   epsilon: 0.5405370400099745    steps: 227    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1485   score: 4.0   memory length: 332347   epsilon: 0.5399509600099872    steps: 296    lr: 0.0001     evaluation reward: 6.22\n",
      "episode: 1486   score: 3.0   memory length: 332573   epsilon: 0.5395034800099969    steps: 226    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 1487   score: 3.0   memory length: 332804   epsilon: 0.5390461000100069    steps: 231    lr: 0.0001     evaluation reward: 6.12\n",
      "episode: 1488   score: 9.0   memory length: 333269   epsilon: 0.5381254000100268    steps: 465    lr: 0.0001     evaluation reward: 6.06\n",
      "episode: 1489   score: 3.0   memory length: 333481   epsilon: 0.537705640010036    steps: 212    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 1490   score: 5.0   memory length: 333788   epsilon: 0.5370977800100492    steps: 307    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 1491   score: 3.0   memory length: 334017   epsilon: 0.536644360010059    steps: 229    lr: 0.0001     evaluation reward: 6.0\n",
      "episode: 1492   score: 12.0   memory length: 334589   epsilon: 0.5355118000100836    steps: 572    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 1493   score: 7.0   memory length: 334955   epsilon: 0.5347871200100993    steps: 366    lr: 0.0001     evaluation reward: 6.12\n",
      "episode: 1494   score: 4.0   memory length: 335214   epsilon: 0.5342743000101104    steps: 259    lr: 0.0001     evaluation reward: 6.13\n",
      "episode: 1495   score: 4.0   memory length: 335510   epsilon: 0.5336882200101232    steps: 296    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 1496   score: 6.0   memory length: 335867   epsilon: 0.5329813600101385    steps: 357    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 1497   score: 6.0   memory length: 336242   epsilon: 0.5322388600101546    steps: 375    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 1498   score: 6.0   memory length: 336600   epsilon: 0.53153002001017    steps: 358    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 1499   score: 7.0   memory length: 336991   epsilon: 0.5307558400101868    steps: 391    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1500   score: 7.0   memory length: 337365   epsilon: 0.5300153200102029    steps: 374    lr: 0.0001     evaluation reward: 6.05\n",
      "episode: 1501   score: 8.0   memory length: 337799   epsilon: 0.5291560000102216    steps: 434    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1502   score: 1.0   memory length: 337950   epsilon: 0.528857020010228    steps: 151    lr: 0.0001     evaluation reward: 5.97\n",
      "episode: 1503   score: 13.0   memory length: 338471   epsilon: 0.5278254400102504    steps: 521    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 1504   score: 4.0   memory length: 338746   epsilon: 0.5272809400102623    steps: 275    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 1505   score: 5.0   memory length: 339091   epsilon: 0.5265978400102771    steps: 345    lr: 0.0001     evaluation reward: 6.04\n",
      "episode: 1506   score: 3.0   memory length: 339320   epsilon: 0.5261444200102869    steps: 229    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 1507   score: 10.0   memory length: 339810   epsilon: 0.525174220010308    steps: 490    lr: 0.0001     evaluation reward: 6.06\n",
      "episode: 1508   score: 9.0   memory length: 340300   epsilon: 0.5242040200103291    steps: 490    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 1509   score: 8.0   memory length: 340756   epsilon: 0.5233011400103487    steps: 456    lr: 0.0001     evaluation reward: 6.09\n",
      "episode: 1510   score: 7.0   memory length: 341126   epsilon: 0.5225685400103646    steps: 370    lr: 0.0001     evaluation reward: 6.1\n",
      "episode: 1511   score: 2.0   memory length: 341308   epsilon: 0.5222081800103724    steps: 182    lr: 0.0001     evaluation reward: 6.05\n",
      "episode: 1512   score: 7.0   memory length: 341726   epsilon: 0.5213805400103904    steps: 418    lr: 0.0001     evaluation reward: 6.08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1513   score: 3.0   memory length: 341937   epsilon: 0.5209627600103994    steps: 211    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 1514   score: 7.0   memory length: 342330   epsilon: 0.5201846200104163    steps: 393    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1515   score: 5.0   memory length: 342656   epsilon: 0.5195391400104303    steps: 326    lr: 0.0001     evaluation reward: 6.04\n",
      "episode: 1516   score: 4.0   memory length: 342919   epsilon: 0.5190184000104416    steps: 263    lr: 0.0001     evaluation reward: 6.02\n",
      "episode: 1517   score: 5.0   memory length: 343245   epsilon: 0.5183729200104557    steps: 326    lr: 0.0001     evaluation reward: 6.01\n",
      "episode: 1518   score: 9.0   memory length: 343717   epsilon: 0.5174383600104759    steps: 472    lr: 0.0001     evaluation reward: 6.02\n",
      "episode: 1519   score: 10.0   memory length: 344268   epsilon: 0.5163473800104996    steps: 551    lr: 0.0001     evaluation reward: 6.0\n",
      "episode: 1520   score: 8.0   memory length: 344710   epsilon: 0.5154722200105186    steps: 442    lr: 0.0001     evaluation reward: 6.0\n",
      "episode: 1521   score: 7.0   memory length: 345130   epsilon: 0.5146406200105367    steps: 420    lr: 0.0001     evaluation reward: 6.0\n",
      "episode: 1522   score: 8.0   memory length: 345570   epsilon: 0.5137694200105556    steps: 440    lr: 0.0001     evaluation reward: 6.03\n",
      "episode: 1523   score: 8.0   memory length: 346016   epsilon: 0.5128863400105748    steps: 446    lr: 0.0001     evaluation reward: 6.04\n",
      "episode: 1524   score: 6.0   memory length: 346390   epsilon: 0.5121458200105908    steps: 374    lr: 0.0001     evaluation reward: 6.05\n",
      "episode: 1525   score: 5.0   memory length: 346679   epsilon: 0.5115736000106033    steps: 289    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1526   score: 9.0   memory length: 347161   epsilon: 0.510619240010624    steps: 482    lr: 0.0001     evaluation reward: 6.07\n",
      "episode: 1527   score: 6.0   memory length: 347559   epsilon: 0.5098312000106411    steps: 398    lr: 0.0001     evaluation reward: 6.08\n",
      "episode: 1528   score: 8.0   memory length: 348004   epsilon: 0.5089501000106602    steps: 445    lr: 0.0001     evaluation reward: 6.11\n",
      "episode: 1529   score: 6.0   memory length: 348366   epsilon: 0.5082333400106758    steps: 362    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 1530   score: 8.0   memory length: 348821   epsilon: 0.5073324400106953    steps: 455    lr: 0.0001     evaluation reward: 6.2\n",
      "episode: 1531   score: 4.0   memory length: 349114   epsilon: 0.5067523000107079    steps: 293    lr: 0.0001     evaluation reward: 6.1\n",
      "episode: 1532   score: 5.0   memory length: 349439   epsilon: 0.5061088000107219    steps: 325    lr: 0.0001     evaluation reward: 6.06\n",
      "episode: 1533   score: 12.0   memory length: 350041   epsilon: 0.5049168400107478    steps: 602    lr: 0.0001     evaluation reward: 6.14\n",
      "episode: 1534   score: 12.0   memory length: 350519   epsilon: 0.5039704000107683    steps: 478    lr: 0.0001     evaluation reward: 6.2\n",
      "episode: 1535   score: 6.0   memory length: 350848   epsilon: 0.5033189800107825    steps: 329    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1536   score: 10.0   memory length: 351366   epsilon: 0.5022933400108047    steps: 518    lr: 0.0001     evaluation reward: 6.29\n",
      "episode: 1537   score: 6.0   memory length: 351690   epsilon: 0.5016518200108186    steps: 324    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1538   score: 7.0   memory length: 352077   epsilon: 0.5008855600108353    steps: 387    lr: 0.0001     evaluation reward: 6.27\n",
      "episode: 1539   score: 9.0   memory length: 352544   epsilon: 0.49996090001085425    steps: 467    lr: 0.0001     evaluation reward: 6.3\n",
      "episode: 1540   score: 7.0   memory length: 352920   epsilon: 0.49921642001084954    steps: 376    lr: 0.0001     evaluation reward: 6.31\n",
      "episode: 1541   score: 5.0   memory length: 353227   epsilon: 0.4986085600108457    steps: 307    lr: 0.0001     evaluation reward: 6.29\n",
      "episode: 1542   score: 3.0   memory length: 353440   epsilon: 0.498186820010843    steps: 213    lr: 0.0001     evaluation reward: 6.28\n",
      "episode: 1543   score: 4.0   memory length: 353682   epsilon: 0.49770766001084    steps: 242    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1544   score: 8.0   memory length: 354100   epsilon: 0.49688002001083476    steps: 418    lr: 0.0001     evaluation reward: 6.28\n",
      "episode: 1545   score: 4.0   memory length: 354356   epsilon: 0.49637314001083155    steps: 256    lr: 0.0001     evaluation reward: 6.29\n",
      "episode: 1546   score: 7.0   memory length: 354743   epsilon: 0.4956068800108267    steps: 387    lr: 0.0001     evaluation reward: 6.31\n",
      "episode: 1547   score: 3.0   memory length: 354975   epsilon: 0.4951475200108238    steps: 232    lr: 0.0001     evaluation reward: 6.27\n",
      "episode: 1548   score: 6.0   memory length: 355327   epsilon: 0.4944505600108194    steps: 352    lr: 0.0001     evaluation reward: 6.25\n",
      "episode: 1549   score: 3.0   memory length: 355553   epsilon: 0.49400308001081655    steps: 226    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1550   score: 4.0   memory length: 355817   epsilon: 0.49348036001081325    steps: 264    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1551   score: 7.0   memory length: 356210   epsilon: 0.4927022200108083    steps: 393    lr: 0.0001     evaluation reward: 6.2\n",
      "episode: 1552   score: 3.0   memory length: 356476   epsilon: 0.492175540010805    steps: 266    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1553   score: 6.0   memory length: 356795   epsilon: 0.491543920010801    steps: 319    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1554   score: 6.0   memory length: 357114   epsilon: 0.490912300010797    steps: 319    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1555   score: 11.0   memory length: 357683   epsilon: 0.48978568001078987    steps: 569    lr: 0.0001     evaluation reward: 6.2\n",
      "episode: 1556   score: 5.0   memory length: 357992   epsilon: 0.489173860010786    steps: 309    lr: 0.0001     evaluation reward: 6.15\n",
      "episode: 1557   score: 6.0   memory length: 358326   epsilon: 0.4885125400107818    steps: 334    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1558   score: 7.0   memory length: 358769   epsilon: 0.48763540001077627    steps: 443    lr: 0.0001     evaluation reward: 6.18\n",
      "episode: 1559   score: 2.0   memory length: 358967   epsilon: 0.4872433600107738    steps: 198    lr: 0.0001     evaluation reward: 6.16\n",
      "episode: 1560   score: 7.0   memory length: 359363   epsilon: 0.4864592800107688    steps: 396    lr: 0.0001     evaluation reward: 6.19\n",
      "episode: 1561   score: 8.0   memory length: 359823   epsilon: 0.48554848001076306    steps: 460    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1562   score: 3.0   memory length: 360036   epsilon: 0.4851267400107604    steps: 213    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1563   score: 11.0   memory length: 360448   epsilon: 0.48431098001075523    steps: 412    lr: 0.0001     evaluation reward: 6.25\n",
      "episode: 1564   score: 5.0   memory length: 360749   epsilon: 0.48371500001075146    steps: 301    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1565   score: 5.0   memory length: 361055   epsilon: 0.48310912001074763    steps: 306    lr: 0.0001     evaluation reward: 6.21\n",
      "episode: 1566   score: 9.0   memory length: 361513   epsilon: 0.4822022800107419    steps: 458    lr: 0.0001     evaluation reward: 6.26\n",
      "episode: 1567   score: 7.0   memory length: 361922   epsilon: 0.48139246001073677    steps: 409    lr: 0.0001     evaluation reward: 6.22\n",
      "episode: 1568   score: 5.0   memory length: 362227   epsilon: 0.48078856001073295    steps: 305    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1569   score: 5.0   memory length: 362575   epsilon: 0.4800995200107286    steps: 348    lr: 0.0001     evaluation reward: 6.23\n",
      "episode: 1570   score: 5.0   memory length: 362882   epsilon: 0.47949166001072474    steps: 307    lr: 0.0001     evaluation reward: 6.2\n",
      "episode: 1571   score: 13.0   memory length: 363327   epsilon: 0.47861056001071917    steps: 445    lr: 0.0001     evaluation reward: 6.24\n",
      "episode: 1572   score: 3.0   memory length: 363539   epsilon: 0.4781908000107165    steps: 212    lr: 0.0001     evaluation reward: 6.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1573   score: 8.0   memory length: 363974   epsilon: 0.47732950001071106    steps: 435    lr: 0.0001     evaluation reward: 6.22\n",
      "episode: 1574   score: 10.0   memory length: 364461   epsilon: 0.47636524001070496    steps: 487    lr: 0.0001     evaluation reward: 6.27\n",
      "episode: 1575   score: 9.0   memory length: 364989   epsilon: 0.47531980001069835    steps: 528    lr: 0.0001     evaluation reward: 6.33\n",
      "episode: 1576   score: 4.0   memory length: 365288   epsilon: 0.4747277800106946    steps: 299    lr: 0.0001     evaluation reward: 6.32\n",
      "episode: 1577   score: 3.0   memory length: 365501   epsilon: 0.47430604001069193    steps: 213    lr: 0.0001     evaluation reward: 6.31\n",
      "episode: 1578   score: 10.0   memory length: 365996   epsilon: 0.47332594001068573    steps: 495    lr: 0.0001     evaluation reward: 6.33\n",
      "episode: 1579   score: 8.0   memory length: 366386   epsilon: 0.47255374001068084    steps: 390    lr: 0.0001     evaluation reward: 6.36\n",
      "episode: 1580   score: 8.0   memory length: 366841   epsilon: 0.47165284001067515    steps: 455    lr: 0.0001     evaluation reward: 6.38\n",
      "episode: 1581   score: 6.0   memory length: 367181   epsilon: 0.4709796400106709    steps: 340    lr: 0.0001     evaluation reward: 6.38\n",
      "episode: 1582   score: 11.0   memory length: 367726   epsilon: 0.46990054001066406    steps: 545    lr: 0.0001     evaluation reward: 6.44\n",
      "episode: 1583   score: 7.0   memory length: 368088   epsilon: 0.4691837800106595    steps: 362    lr: 0.0001     evaluation reward: 6.39\n",
      "episode: 1584   score: 9.0   memory length: 368537   epsilon: 0.4682947600106539    steps: 449    lr: 0.0001     evaluation reward: 6.45\n",
      "episode: 1585   score: 9.0   memory length: 368991   epsilon: 0.4673958400106482    steps: 454    lr: 0.0001     evaluation reward: 6.5\n",
      "episode: 1586   score: 4.0   memory length: 369269   epsilon: 0.46684540001064473    steps: 278    lr: 0.0001     evaluation reward: 6.51\n",
      "episode: 1587   score: 8.0   memory length: 369697   epsilon: 0.46599796001063937    steps: 428    lr: 0.0001     evaluation reward: 6.56\n",
      "episode: 1588   score: 5.0   memory length: 370007   epsilon: 0.4653841600106355    steps: 310    lr: 0.0001     evaluation reward: 6.52\n",
      "episode: 1589   score: 7.0   memory length: 370414   epsilon: 0.4645783000106304    steps: 407    lr: 0.0001     evaluation reward: 6.56\n",
      "episode: 1590   score: 12.0   memory length: 370873   epsilon: 0.46366948001062464    steps: 459    lr: 0.0001     evaluation reward: 6.63\n",
      "episode: 1591   score: 4.0   memory length: 371148   epsilon: 0.4631249800106212    steps: 275    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 1592   score: 8.0   memory length: 371538   epsilon: 0.4623527800106163    steps: 390    lr: 0.0001     evaluation reward: 6.6\n",
      "episode: 1593   score: 7.0   memory length: 371976   epsilon: 0.4614855400106108    steps: 438    lr: 0.0001     evaluation reward: 6.6\n",
      "episode: 1594   score: 8.0   memory length: 372382   epsilon: 0.46068166001060573    steps: 406    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 1595   score: 3.0   memory length: 372608   epsilon: 0.4602341800106029    steps: 226    lr: 0.0001     evaluation reward: 6.63\n",
      "episode: 1596   score: 6.0   memory length: 372936   epsilon: 0.4595847400105988    steps: 328    lr: 0.0001     evaluation reward: 6.63\n",
      "episode: 1597   score: 7.0   memory length: 373327   epsilon: 0.4588105600105939    steps: 391    lr: 0.0001     evaluation reward: 6.64\n",
      "episode: 1598   score: 9.0   memory length: 373794   epsilon: 0.45788590001058804    steps: 467    lr: 0.0001     evaluation reward: 6.67\n",
      "episode: 1599   score: 6.0   memory length: 374146   epsilon: 0.45718894001058363    steps: 352    lr: 0.0001     evaluation reward: 6.66\n",
      "episode: 1600   score: 9.0   memory length: 374640   epsilon: 0.45621082001057744    steps: 494    lr: 0.0001     evaluation reward: 6.68\n",
      "episode: 1601   score: 9.0   memory length: 375117   epsilon: 0.45526636001057147    steps: 477    lr: 0.0001     evaluation reward: 6.69\n",
      "episode: 1602   score: 6.0   memory length: 375471   epsilon: 0.45456544001056703    steps: 354    lr: 0.0001     evaluation reward: 6.74\n",
      "episode: 1603   score: 10.0   memory length: 375946   epsilon: 0.4536249400105611    steps: 475    lr: 0.0001     evaluation reward: 6.71\n",
      "episode: 1604   score: 5.0   memory length: 376252   epsilon: 0.45301906001055725    steps: 306    lr: 0.0001     evaluation reward: 6.72\n",
      "episode: 1605   score: 3.0   memory length: 376498   epsilon: 0.45253198001055417    steps: 246    lr: 0.0001     evaluation reward: 6.7\n",
      "episode: 1606   score: 10.0   memory length: 376984   epsilon: 0.4515697000105481    steps: 486    lr: 0.0001     evaluation reward: 6.77\n",
      "episode: 1607   score: 6.0   memory length: 377329   epsilon: 0.45088660001054376    steps: 345    lr: 0.0001     evaluation reward: 6.73\n",
      "episode: 1608   score: 7.0   memory length: 377714   epsilon: 0.45012430001053894    steps: 385    lr: 0.0001     evaluation reward: 6.71\n",
      "episode: 1609   score: 6.0   memory length: 378087   epsilon: 0.44938576001053426    steps: 373    lr: 0.0001     evaluation reward: 6.69\n",
      "episode: 1610   score: 4.0   memory length: 378362   epsilon: 0.4488412600105308    steps: 275    lr: 0.0001     evaluation reward: 6.66\n",
      "episode: 1611   score: 6.0   memory length: 378728   epsilon: 0.44811658001052623    steps: 366    lr: 0.0001     evaluation reward: 6.7\n",
      "episode: 1612   score: 19.0   memory length: 379414   epsilon: 0.44675830001051764    steps: 686    lr: 0.0001     evaluation reward: 6.82\n",
      "episode: 1613   score: 11.0   memory length: 379973   epsilon: 0.44565148001051064    steps: 559    lr: 0.0001     evaluation reward: 6.9\n",
      "episode: 1614   score: 9.0   memory length: 380414   epsilon: 0.4447783000105051    steps: 441    lr: 0.0001     evaluation reward: 6.92\n",
      "episode: 1615   score: 12.0   memory length: 381060   epsilon: 0.443499220010497    steps: 646    lr: 0.0001     evaluation reward: 6.99\n",
      "episode: 1616   score: 5.0   memory length: 381386   epsilon: 0.44285374001049294    steps: 326    lr: 0.0001     evaluation reward: 7.0\n",
      "episode: 1617   score: 9.0   memory length: 381887   epsilon: 0.44186176001048666    steps: 501    lr: 0.0001     evaluation reward: 7.04\n",
      "episode: 1618   score: 10.0   memory length: 382435   epsilon: 0.4407767200104798    steps: 548    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 1619   score: 5.0   memory length: 382707   epsilon: 0.4402381600104764    steps: 272    lr: 0.0001     evaluation reward: 7.0\n",
      "episode: 1620   score: 5.0   memory length: 383015   epsilon: 0.43962832001047253    steps: 308    lr: 0.0001     evaluation reward: 6.97\n",
      "episode: 1621   score: 6.0   memory length: 383349   epsilon: 0.43896700001046834    steps: 334    lr: 0.0001     evaluation reward: 6.96\n",
      "episode: 1622   score: 9.0   memory length: 383856   epsilon: 0.437963140010462    steps: 507    lr: 0.0001     evaluation reward: 6.97\n",
      "episode: 1623   score: 7.0   memory length: 384202   epsilon: 0.43727806001045766    steps: 346    lr: 0.0001     evaluation reward: 6.96\n",
      "episode: 1624   score: 12.0   memory length: 384644   epsilon: 0.4364029000104521    steps: 442    lr: 0.0001     evaluation reward: 7.02\n",
      "episode: 1625   score: 8.0   memory length: 385080   epsilon: 0.43553962001044666    steps: 436    lr: 0.0001     evaluation reward: 7.05\n",
      "episode: 1626   score: 8.0   memory length: 385482   epsilon: 0.4347436600104416    steps: 402    lr: 0.0001     evaluation reward: 7.04\n",
      "episode: 1627   score: 12.0   memory length: 386047   epsilon: 0.43362496001043455    steps: 565    lr: 0.0001     evaluation reward: 7.1\n",
      "episode: 1628   score: 7.0   memory length: 386404   epsilon: 0.4329181000104301    steps: 357    lr: 0.0001     evaluation reward: 7.09\n",
      "episode: 1629   score: 13.0   memory length: 387031   epsilon: 0.4316766400104222    steps: 627    lr: 0.0001     evaluation reward: 7.16\n",
      "episode: 1630   score: 9.0   memory length: 387494   epsilon: 0.4307599000104164    steps: 463    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1631   score: 4.0   memory length: 387736   epsilon: 0.4302807400104134    steps: 242    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1632   score: 5.0   memory length: 388060   epsilon: 0.42963922001040933    steps: 324    lr: 0.0001     evaluation reward: 7.17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1633   score: 3.0   memory length: 388309   epsilon: 0.4291462000104062    steps: 249    lr: 0.0001     evaluation reward: 7.08\n",
      "episode: 1634   score: 5.0   memory length: 388650   epsilon: 0.42847102001040194    steps: 341    lr: 0.0001     evaluation reward: 7.01\n",
      "episode: 1635   score: 8.0   memory length: 389053   epsilon: 0.4276730800103969    steps: 403    lr: 0.0001     evaluation reward: 7.03\n",
      "episode: 1636   score: 7.0   memory length: 389481   epsilon: 0.4268256400103915    steps: 428    lr: 0.0001     evaluation reward: 7.0\n",
      "episode: 1637   score: 9.0   memory length: 389917   epsilon: 0.42596236001038607    steps: 436    lr: 0.0001     evaluation reward: 7.03\n",
      "episode: 1638   score: 16.0   memory length: 390441   epsilon: 0.4249248400103795    steps: 524    lr: 0.0001     evaluation reward: 7.12\n",
      "episode: 1639   score: 12.0   memory length: 391011   epsilon: 0.42379624001037236    steps: 570    lr: 0.0001     evaluation reward: 7.15\n",
      "episode: 1640   score: 10.0   memory length: 391482   epsilon: 0.42286366001036646    steps: 471    lr: 0.0001     evaluation reward: 7.18\n",
      "episode: 1641   score: 4.0   memory length: 391723   epsilon: 0.42238648001036344    steps: 241    lr: 0.0001     evaluation reward: 7.17\n",
      "episode: 1642   score: 6.0   memory length: 392078   epsilon: 0.421683580010359    steps: 355    lr: 0.0001     evaluation reward: 7.2\n",
      "episode: 1643   score: 7.0   memory length: 392461   epsilon: 0.4209252400103542    steps: 383    lr: 0.0001     evaluation reward: 7.23\n",
      "episode: 1644   score: 4.0   memory length: 392719   epsilon: 0.42041440001035096    steps: 258    lr: 0.0001     evaluation reward: 7.19\n",
      "episode: 1645   score: 14.0   memory length: 393267   epsilon: 0.4193293600103441    steps: 548    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 1646   score: 7.0   memory length: 393670   epsilon: 0.41853142001033905    steps: 403    lr: 0.0001     evaluation reward: 7.29\n",
      "episode: 1647   score: 13.0   memory length: 394191   epsilon: 0.4174998400103325    steps: 521    lr: 0.0001     evaluation reward: 7.39\n",
      "episode: 1648   score: 6.0   memory length: 394550   epsilon: 0.416789020010328    steps: 359    lr: 0.0001     evaluation reward: 7.39\n",
      "episode: 1649   score: 11.0   memory length: 395069   epsilon: 0.4157614000103215    steps: 519    lr: 0.0001     evaluation reward: 7.47\n",
      "episode: 1650   score: 10.0   memory length: 395587   epsilon: 0.41473576001031504    steps: 518    lr: 0.0001     evaluation reward: 7.53\n",
      "episode: 1651   score: 4.0   memory length: 395866   epsilon: 0.41418334001031154    steps: 279    lr: 0.0001     evaluation reward: 7.5\n",
      "episode: 1652   score: 3.0   memory length: 396113   epsilon: 0.41369428001030845    steps: 247    lr: 0.0001     evaluation reward: 7.5\n",
      "episode: 1653   score: 5.0   memory length: 396386   epsilon: 0.413153740010305    steps: 273    lr: 0.0001     evaluation reward: 7.49\n",
      "episode: 1654   score: 13.0   memory length: 396930   epsilon: 0.4120766200102982    steps: 544    lr: 0.0001     evaluation reward: 7.56\n",
      "episode: 1655   score: 5.0   memory length: 397233   epsilon: 0.4114766800102944    steps: 303    lr: 0.0001     evaluation reward: 7.5\n",
      "episode: 1656   score: 4.0   memory length: 397473   epsilon: 0.4110014800102914    steps: 240    lr: 0.0001     evaluation reward: 7.49\n",
      "episode: 1657   score: 10.0   memory length: 397980   epsilon: 0.40999762001028506    steps: 507    lr: 0.0001     evaluation reward: 7.53\n",
      "episode: 1658   score: 7.0   memory length: 398337   epsilon: 0.4092907600102806    steps: 357    lr: 0.0001     evaluation reward: 7.53\n",
      "episode: 1659   score: 5.0   memory length: 398612   epsilon: 0.40874626001027714    steps: 275    lr: 0.0001     evaluation reward: 7.56\n",
      "episode: 1660   score: 5.0   memory length: 398908   epsilon: 0.40816018001027343    steps: 296    lr: 0.0001     evaluation reward: 7.54\n",
      "episode: 1661   score: 9.0   memory length: 399382   epsilon: 0.4072216600102675    steps: 474    lr: 0.0001     evaluation reward: 7.55\n",
      "episode: 1662   score: 10.0   memory length: 399769   epsilon: 0.40645540001026265    steps: 387    lr: 0.0001     evaluation reward: 7.62\n",
      "episode: 1663   score: 12.0   memory length: 400324   epsilon: 0.4053565000102557    steps: 555    lr: 0.0001     evaluation reward: 7.63\n",
      "episode: 1664   score: 4.0   memory length: 400581   epsilon: 0.4048476400102525    steps: 257    lr: 0.0001     evaluation reward: 7.62\n",
      "episode: 1665   score: 10.0   memory length: 401083   epsilon: 0.4038536800102462    steps: 502    lr: 0.0001     evaluation reward: 7.67\n",
      "episode: 1666   score: 8.0   memory length: 401503   epsilon: 0.4030220800102409    steps: 420    lr: 0.0001     evaluation reward: 7.66\n",
      "episode: 1667   score: 8.0   memory length: 401909   epsilon: 0.40221820001023584    steps: 406    lr: 0.0001     evaluation reward: 7.67\n",
      "episode: 1668   score: 11.0   memory length: 402459   epsilon: 0.40112920001022895    steps: 550    lr: 0.0001     evaluation reward: 7.73\n",
      "episode: 1669   score: 7.0   memory length: 402884   epsilon: 0.4002877000102236    steps: 425    lr: 0.0001     evaluation reward: 7.75\n",
      "episode: 1670   score: 11.0   memory length: 403277   epsilon: 0.3995095600102187    steps: 393    lr: 0.0001     evaluation reward: 7.81\n",
      "episode: 1671   score: 9.0   memory length: 403748   epsilon: 0.3985769800102128    steps: 471    lr: 0.0001     evaluation reward: 7.77\n",
      "episode: 1672   score: 13.0   memory length: 404235   epsilon: 0.3976127200102067    steps: 487    lr: 0.0001     evaluation reward: 7.87\n",
      "episode: 1673   score: 7.0   memory length: 404600   epsilon: 0.3968900200102021    steps: 365    lr: 0.0001     evaluation reward: 7.86\n",
      "episode: 1674   score: 7.0   memory length: 404936   epsilon: 0.3962247400101979    steps: 336    lr: 0.0001     evaluation reward: 7.83\n",
      "episode: 1675   score: 7.0   memory length: 405326   epsilon: 0.39545254001019303    steps: 390    lr: 0.0001     evaluation reward: 7.81\n",
      "episode: 1676   score: 8.0   memory length: 405750   epsilon: 0.3946130200101877    steps: 424    lr: 0.0001     evaluation reward: 7.85\n",
      "episode: 1677   score: 6.0   memory length: 406109   epsilon: 0.3939022000101832    steps: 359    lr: 0.0001     evaluation reward: 7.88\n",
      "episode: 1678   score: 8.0   memory length: 406561   epsilon: 0.39300724001017756    steps: 452    lr: 0.0001     evaluation reward: 7.86\n",
      "episode: 1679   score: 10.0   memory length: 407048   epsilon: 0.39204298001017146    steps: 487    lr: 0.0001     evaluation reward: 7.88\n",
      "episode: 1680   score: 10.0   memory length: 407560   epsilon: 0.39102922001016505    steps: 512    lr: 0.0001     evaluation reward: 7.9\n",
      "episode: 1681   score: 10.0   memory length: 408060   epsilon: 0.3900392200101588    steps: 500    lr: 0.0001     evaluation reward: 7.94\n",
      "episode: 1682   score: 5.0   memory length: 408353   epsilon: 0.3894590800101551    steps: 293    lr: 0.0001     evaluation reward: 7.88\n",
      "episode: 1683   score: 11.0   memory length: 408895   epsilon: 0.3883859200101483    steps: 542    lr: 0.0001     evaluation reward: 7.92\n",
      "episode: 1684   score: 7.0   memory length: 409298   epsilon: 0.3875879800101433    steps: 403    lr: 0.0001     evaluation reward: 7.9\n",
      "episode: 1685   score: 4.0   memory length: 409594   epsilon: 0.38700190001013957    steps: 296    lr: 0.0001     evaluation reward: 7.85\n",
      "episode: 1686   score: 9.0   memory length: 410085   epsilon: 0.3860297200101334    steps: 491    lr: 0.0001     evaluation reward: 7.9\n",
      "episode: 1687   score: 8.0   memory length: 410537   epsilon: 0.38513476001012775    steps: 452    lr: 0.0001     evaluation reward: 7.9\n",
      "episode: 1688   score: 10.0   memory length: 410914   epsilon: 0.38438830001012303    steps: 377    lr: 0.0001     evaluation reward: 7.95\n",
      "episode: 1689   score: 4.0   memory length: 411188   epsilon: 0.3838457800101196    steps: 274    lr: 0.0001     evaluation reward: 7.92\n",
      "episode: 1690   score: 7.0   memory length: 411579   epsilon: 0.3830716000101147    steps: 391    lr: 0.0001     evaluation reward: 7.87\n",
      "episode: 1691   score: 8.0   memory length: 411994   epsilon: 0.3822499000101095    steps: 415    lr: 0.0001     evaluation reward: 7.91\n",
      "episode: 1692   score: 5.0   memory length: 412284   epsilon: 0.38167570001010587    steps: 290    lr: 0.0001     evaluation reward: 7.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1693   score: 9.0   memory length: 412798   epsilon: 0.38065798001009943    steps: 514    lr: 0.0001     evaluation reward: 7.9\n",
      "episode: 1694   score: 6.0   memory length: 413162   epsilon: 0.37993726001009487    steps: 364    lr: 0.0001     evaluation reward: 7.88\n",
      "episode: 1695   score: 10.0   memory length: 413653   epsilon: 0.3789650800100887    steps: 491    lr: 0.0001     evaluation reward: 7.95\n",
      "episode: 1696   score: 12.0   memory length: 414188   epsilon: 0.377905780010082    steps: 535    lr: 0.0001     evaluation reward: 8.01\n",
      "episode: 1697   score: 10.0   memory length: 414722   epsilon: 0.3768484600100753    steps: 534    lr: 0.0001     evaluation reward: 8.04\n",
      "episode: 1698   score: 14.0   memory length: 415347   epsilon: 0.3756109600100675    steps: 625    lr: 0.0001     evaluation reward: 8.09\n",
      "episode: 1699   score: 20.0   memory length: 416000   epsilon: 0.3743180200100593    steps: 653    lr: 0.0001     evaluation reward: 8.23\n",
      "episode: 1700   score: 6.0   memory length: 416360   epsilon: 0.3736052200100548    steps: 360    lr: 0.0001     evaluation reward: 8.2\n",
      "episode: 1701   score: 8.0   memory length: 416812   epsilon: 0.37271026001004914    steps: 452    lr: 0.0001     evaluation reward: 8.19\n",
      "episode: 1702   score: 8.0   memory length: 417256   epsilon: 0.3718311400100436    steps: 444    lr: 0.0001     evaluation reward: 8.21\n",
      "episode: 1703   score: 8.0   memory length: 417674   epsilon: 0.37100350001003835    steps: 418    lr: 0.0001     evaluation reward: 8.19\n",
      "episode: 1704   score: 5.0   memory length: 418007   epsilon: 0.3703441600100342    steps: 333    lr: 0.0001     evaluation reward: 8.19\n",
      "episode: 1705   score: 5.0   memory length: 418335   epsilon: 0.36969472001003006    steps: 328    lr: 0.0001     evaluation reward: 8.21\n",
      "episode: 1706   score: 4.0   memory length: 418592   epsilon: 0.36918586001002685    steps: 257    lr: 0.0001     evaluation reward: 8.15\n",
      "episode: 1707   score: 6.0   memory length: 418932   epsilon: 0.3685126600100226    steps: 340    lr: 0.0001     evaluation reward: 8.15\n",
      "episode: 1708   score: 5.0   memory length: 419255   epsilon: 0.36787312001001854    steps: 323    lr: 0.0001     evaluation reward: 8.13\n",
      "episode: 1709   score: 8.0   memory length: 419713   epsilon: 0.3669662800100128    steps: 458    lr: 0.0001     evaluation reward: 8.15\n",
      "episode: 1710   score: 6.0   memory length: 420068   epsilon: 0.36626338001000835    steps: 355    lr: 0.0001     evaluation reward: 8.17\n",
      "episode: 1711   score: 10.0   memory length: 420613   epsilon: 0.3651842800100015    steps: 545    lr: 0.0001     evaluation reward: 8.21\n",
      "episode: 1712   score: 6.0   memory length: 420986   epsilon: 0.36444574000999685    steps: 373    lr: 0.0001     evaluation reward: 8.08\n",
      "episode: 1713   score: 4.0   memory length: 421230   epsilon: 0.3639626200099938    steps: 244    lr: 0.0001     evaluation reward: 8.01\n",
      "episode: 1714   score: 9.0   memory length: 421684   epsilon: 0.3630637000099881    steps: 454    lr: 0.0001     evaluation reward: 8.01\n",
      "episode: 1715   score: 12.0   memory length: 422138   epsilon: 0.3621647800099824    steps: 454    lr: 0.0001     evaluation reward: 8.01\n",
      "episode: 1716   score: 11.0   memory length: 422559   epsilon: 0.36133120000997715    steps: 421    lr: 0.0001     evaluation reward: 8.07\n",
      "episode: 1717   score: 8.0   memory length: 422950   epsilon: 0.36055702000997225    steps: 391    lr: 0.0001     evaluation reward: 8.06\n",
      "episode: 1718   score: 7.0   memory length: 423320   epsilon: 0.3598244200099676    steps: 370    lr: 0.0001     evaluation reward: 8.03\n",
      "episode: 1719   score: 9.0   memory length: 423774   epsilon: 0.35892550000996193    steps: 454    lr: 0.0001     evaluation reward: 8.07\n",
      "episode: 1720   score: 9.0   memory length: 424194   epsilon: 0.35809390000995667    steps: 420    lr: 0.0001     evaluation reward: 8.11\n",
      "episode: 1721   score: 10.0   memory length: 424685   epsilon: 0.3571217200099505    steps: 491    lr: 0.0001     evaluation reward: 8.15\n",
      "episode: 1722   score: 8.0   memory length: 425083   epsilon: 0.35633368000994553    steps: 398    lr: 0.0001     evaluation reward: 8.14\n",
      "episode: 1723   score: 10.0   memory length: 425570   epsilon: 0.35536942000993943    steps: 487    lr: 0.0001     evaluation reward: 8.17\n",
      "episode: 1724   score: 8.0   memory length: 426011   epsilon: 0.3544962400099339    steps: 441    lr: 0.0001     evaluation reward: 8.13\n",
      "episode: 1725   score: 4.0   memory length: 426289   epsilon: 0.3539458000099304    steps: 278    lr: 0.0001     evaluation reward: 8.09\n",
      "episode: 1726   score: 8.0   memory length: 426703   epsilon: 0.35312608000992524    steps: 414    lr: 0.0001     evaluation reward: 8.09\n",
      "episode: 1727   score: 15.0   memory length: 427401   epsilon: 0.3517440400099165    steps: 698    lr: 0.0001     evaluation reward: 8.12\n",
      "episode: 1728   score: 4.0   memory length: 427640   epsilon: 0.3512708200099135    steps: 239    lr: 0.0001     evaluation reward: 8.09\n",
      "episode: 1729   score: 16.0   memory length: 428371   epsilon: 0.34982344000990434    steps: 731    lr: 0.0001     evaluation reward: 8.12\n",
      "episode: 1730   score: 12.0   memory length: 428782   epsilon: 0.3490096600098992    steps: 411    lr: 0.0001     evaluation reward: 8.15\n",
      "episode: 1731   score: 7.0   memory length: 429206   epsilon: 0.3481701400098939    steps: 424    lr: 0.0001     evaluation reward: 8.18\n",
      "episode: 1732   score: 12.0   memory length: 429686   epsilon: 0.34721974000988787    steps: 480    lr: 0.0001     evaluation reward: 8.25\n",
      "episode: 1733   score: 6.0   memory length: 430021   epsilon: 0.34655644000988367    steps: 335    lr: 0.0001     evaluation reward: 8.28\n",
      "episode: 1734   score: 8.0   memory length: 430421   epsilon: 0.34576444000987866    steps: 400    lr: 0.0001     evaluation reward: 8.31\n",
      "episode: 1735   score: 12.0   memory length: 430962   epsilon: 0.3446932600098719    steps: 541    lr: 0.0001     evaluation reward: 8.35\n",
      "episode: 1736   score: 11.0   memory length: 431480   epsilon: 0.3436676200098654    steps: 518    lr: 0.0001     evaluation reward: 8.39\n",
      "episode: 1737   score: 14.0   memory length: 432017   epsilon: 0.34260436000985867    steps: 537    lr: 0.0001     evaluation reward: 8.44\n",
      "episode: 1738   score: 7.0   memory length: 432438   epsilon: 0.3417707800098534    steps: 421    lr: 0.0001     evaluation reward: 8.35\n",
      "episode: 1739   score: 8.0   memory length: 432848   epsilon: 0.34095898000984826    steps: 410    lr: 0.0001     evaluation reward: 8.31\n",
      "episode: 1740   score: 4.0   memory length: 433110   epsilon: 0.340440220009845    steps: 262    lr: 0.0001     evaluation reward: 8.25\n",
      "episode: 1741   score: 6.0   memory length: 433482   epsilon: 0.3397036600098403    steps: 372    lr: 0.0001     evaluation reward: 8.27\n",
      "episode: 1742   score: 9.0   memory length: 433973   epsilon: 0.33873148000983416    steps: 491    lr: 0.0001     evaluation reward: 8.3\n",
      "episode: 1743   score: 11.0   memory length: 434513   epsilon: 0.3376622800098274    steps: 540    lr: 0.0001     evaluation reward: 8.34\n",
      "episode: 1744   score: 10.0   memory length: 434863   epsilon: 0.336969280009823    steps: 350    lr: 0.0001     evaluation reward: 8.4\n",
      "episode: 1745   score: 14.0   memory length: 435505   epsilon: 0.33569812000981497    steps: 642    lr: 0.0001     evaluation reward: 8.4\n",
      "episode: 1746   score: 19.0   memory length: 436298   epsilon: 0.33412798000980504    steps: 793    lr: 0.0001     evaluation reward: 8.52\n",
      "episode: 1747   score: 12.0   memory length: 436769   epsilon: 0.33319540000979914    steps: 471    lr: 0.0001     evaluation reward: 8.51\n",
      "episode: 1748   score: 11.0   memory length: 437297   epsilon: 0.3321499600097925    steps: 528    lr: 0.0001     evaluation reward: 8.56\n",
      "episode: 1749   score: 7.0   memory length: 437684   epsilon: 0.3313837000097877    steps: 387    lr: 0.0001     evaluation reward: 8.52\n",
      "episode: 1750   score: 9.0   memory length: 438156   epsilon: 0.33044914000978176    steps: 472    lr: 0.0001     evaluation reward: 8.51\n",
      "episode: 1751   score: 10.0   memory length: 438528   epsilon: 0.3297125800097771    steps: 372    lr: 0.0001     evaluation reward: 8.57\n",
      "episode: 1752   score: 8.0   memory length: 438980   epsilon: 0.32881762000977144    steps: 452    lr: 0.0001     evaluation reward: 8.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1753   score: 5.0   memory length: 439287   epsilon: 0.3282097600097676    steps: 307    lr: 0.0001     evaluation reward: 8.62\n",
      "episode: 1754   score: 12.0   memory length: 439887   epsilon: 0.3270217600097601    steps: 600    lr: 0.0001     evaluation reward: 8.61\n",
      "episode: 1755   score: 13.0   memory length: 440499   epsilon: 0.3258100000097524    steps: 612    lr: 0.0001     evaluation reward: 8.69\n",
      "episode: 1756   score: 11.0   memory length: 441031   epsilon: 0.32475664000974575    steps: 532    lr: 0.0001     evaluation reward: 8.76\n",
      "episode: 1757   score: 9.0   memory length: 441527   epsilon: 0.32377456000973953    steps: 496    lr: 0.0001     evaluation reward: 8.75\n",
      "episode: 1758   score: 9.0   memory length: 442008   epsilon: 0.3228221800097335    steps: 481    lr: 0.0001     evaluation reward: 8.77\n",
      "episode: 1759   score: 25.0   memory length: 442766   epsilon: 0.321321340009724    steps: 758    lr: 0.0001     evaluation reward: 8.97\n",
      "episode: 1760   score: 10.0   memory length: 443259   epsilon: 0.32034520000971783    steps: 493    lr: 0.0001     evaluation reward: 9.02\n",
      "episode: 1761   score: 8.0   memory length: 443699   epsilon: 0.3194740000097123    steps: 440    lr: 0.0001     evaluation reward: 9.01\n",
      "episode: 1762   score: 5.0   memory length: 444027   epsilon: 0.3188245600097082    steps: 328    lr: 0.0001     evaluation reward: 8.96\n",
      "episode: 1763   score: 4.0   memory length: 444268   epsilon: 0.3183473800097052    steps: 241    lr: 0.0001     evaluation reward: 8.88\n",
      "episode: 1764   score: 7.0   memory length: 444620   epsilon: 0.3176504200097008    steps: 352    lr: 0.0001     evaluation reward: 8.91\n",
      "episode: 1765   score: 8.0   memory length: 445073   epsilon: 0.3167534800096951    steps: 453    lr: 0.0001     evaluation reward: 8.89\n",
      "episode: 1766   score: 9.0   memory length: 445505   epsilon: 0.3158981200096897    steps: 432    lr: 0.0001     evaluation reward: 8.9\n",
      "episode: 1767   score: 11.0   memory length: 445957   epsilon: 0.31500316000968404    steps: 452    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1768   score: 8.0   memory length: 446355   epsilon: 0.31421512000967905    steps: 398    lr: 0.0001     evaluation reward: 8.9\n",
      "episode: 1769   score: 11.0   memory length: 446887   epsilon: 0.3131617600096724    steps: 532    lr: 0.0001     evaluation reward: 8.94\n",
      "episode: 1770   score: 6.0   memory length: 447266   epsilon: 0.31241134000966764    steps: 379    lr: 0.0001     evaluation reward: 8.89\n",
      "episode: 1771   score: 9.0   memory length: 447730   epsilon: 0.3114926200096618    steps: 464    lr: 0.0001     evaluation reward: 8.89\n",
      "episode: 1772   score: 15.0   memory length: 448294   epsilon: 0.31037590000965476    steps: 564    lr: 0.0001     evaluation reward: 8.91\n",
      "episode: 1773   score: 7.0   memory length: 448683   epsilon: 0.3096056800096499    steps: 389    lr: 0.0001     evaluation reward: 8.91\n",
      "episode: 1774   score: 7.0   memory length: 449101   epsilon: 0.30877804000964465    steps: 418    lr: 0.0001     evaluation reward: 8.91\n",
      "episode: 1775   score: 7.0   memory length: 449507   epsilon: 0.30797416000963956    steps: 406    lr: 0.0001     evaluation reward: 8.91\n",
      "episode: 1776   score: 10.0   memory length: 449878   epsilon: 0.3072395800096349    steps: 371    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1777   score: 9.0   memory length: 450385   epsilon: 0.30623572000962856    steps: 507    lr: 0.0001     evaluation reward: 8.96\n",
      "episode: 1778   score: 5.0   memory length: 450711   epsilon: 0.3055902400096245    steps: 326    lr: 0.0001     evaluation reward: 8.93\n",
      "episode: 1779   score: 12.0   memory length: 451236   epsilon: 0.3045507400096179    steps: 525    lr: 0.0001     evaluation reward: 8.95\n",
      "episode: 1780   score: 6.0   memory length: 451612   epsilon: 0.3038062600096132    steps: 376    lr: 0.0001     evaluation reward: 8.91\n",
      "episode: 1781   score: 13.0   memory length: 452141   epsilon: 0.30275884000960657    steps: 529    lr: 0.0001     evaluation reward: 8.94\n",
      "episode: 1782   score: 5.0   memory length: 452464   epsilon: 0.3021193000096025    steps: 323    lr: 0.0001     evaluation reward: 8.94\n",
      "episode: 1783   score: 13.0   memory length: 453077   epsilon: 0.30090556000959484    steps: 613    lr: 0.0001     evaluation reward: 8.96\n",
      "episode: 1784   score: 6.0   memory length: 453416   epsilon: 0.3002343400095906    steps: 339    lr: 0.0001     evaluation reward: 8.95\n",
      "episode: 1785   score: 12.0   memory length: 453949   epsilon: 0.2991790000095839    steps: 533    lr: 0.0001     evaluation reward: 9.03\n",
      "episode: 1786   score: 16.0   memory length: 454553   epsilon: 0.29798308000957635    steps: 604    lr: 0.0001     evaluation reward: 9.1\n",
      "episode: 1787   score: 13.0   memory length: 455125   epsilon: 0.2968505200095692    steps: 572    lr: 0.0001     evaluation reward: 9.15\n",
      "episode: 1788   score: 15.0   memory length: 455701   epsilon: 0.29571004000956197    steps: 576    lr: 0.0001     evaluation reward: 9.2\n",
      "episode: 1789   score: 7.0   memory length: 456046   epsilon: 0.29502694000955765    steps: 345    lr: 0.0001     evaluation reward: 9.23\n",
      "episode: 1790   score: 8.0   memory length: 456479   epsilon: 0.2941696000095522    steps: 433    lr: 0.0001     evaluation reward: 9.24\n",
      "episode: 1791   score: 10.0   memory length: 456977   epsilon: 0.293183560009546    steps: 498    lr: 0.0001     evaluation reward: 9.26\n",
      "episode: 1792   score: 6.0   memory length: 457314   epsilon: 0.29251630000954176    steps: 337    lr: 0.0001     evaluation reward: 9.27\n",
      "episode: 1793   score: 5.0   memory length: 457619   epsilon: 0.29191240000953794    steps: 305    lr: 0.0001     evaluation reward: 9.23\n",
      "episode: 1794   score: 10.0   memory length: 458008   epsilon: 0.29114218000953307    steps: 389    lr: 0.0001     evaluation reward: 9.27\n",
      "episode: 1795   score: 6.0   memory length: 458379   epsilon: 0.2904076000095284    steps: 371    lr: 0.0001     evaluation reward: 9.23\n",
      "episode: 1796   score: 13.0   memory length: 458826   epsilon: 0.2895225400095228    steps: 447    lr: 0.0001     evaluation reward: 9.24\n",
      "episode: 1797   score: 9.0   memory length: 459291   epsilon: 0.288601840009517    steps: 465    lr: 0.0001     evaluation reward: 9.23\n",
      "episode: 1798   score: 4.0   memory length: 459532   epsilon: 0.288124660009514    steps: 241    lr: 0.0001     evaluation reward: 9.13\n",
      "episode: 1799   score: 7.0   memory length: 459935   epsilon: 0.28732672000950893    steps: 403    lr: 0.0001     evaluation reward: 9.0\n",
      "episode: 1800   score: 6.0   memory length: 460299   epsilon: 0.28660600000950437    steps: 364    lr: 0.0001     evaluation reward: 9.0\n",
      "episode: 1801   score: 4.0   memory length: 460573   epsilon: 0.28606348000950094    steps: 274    lr: 0.0001     evaluation reward: 8.96\n",
      "episode: 1802   score: 8.0   memory length: 460970   epsilon: 0.28527742000949596    steps: 397    lr: 0.0001     evaluation reward: 8.96\n",
      "episode: 1803   score: 9.0   memory length: 461471   epsilon: 0.2842854400094897    steps: 501    lr: 0.0001     evaluation reward: 8.97\n",
      "episode: 1804   score: 14.0   memory length: 462040   epsilon: 0.28315882000948256    steps: 569    lr: 0.0001     evaluation reward: 9.06\n",
      "episode: 1805   score: 9.0   memory length: 462550   epsilon: 0.28214902000947617    steps: 510    lr: 0.0001     evaluation reward: 9.1\n",
      "episode: 1806   score: 16.0   memory length: 463145   epsilon: 0.2809709200094687    steps: 595    lr: 0.0001     evaluation reward: 9.22\n",
      "episode: 1807   score: 8.0   memory length: 463600   epsilon: 0.280070020009463    steps: 455    lr: 0.0001     evaluation reward: 9.24\n",
      "episode: 1808   score: 9.0   memory length: 464039   epsilon: 0.2792008000094575    steps: 439    lr: 0.0001     evaluation reward: 9.28\n",
      "episode: 1809   score: 5.0   memory length: 464345   epsilon: 0.2785949200094537    steps: 306    lr: 0.0001     evaluation reward: 9.25\n",
      "episode: 1810   score: 6.0   memory length: 464698   epsilon: 0.27789598000944926    steps: 353    lr: 0.0001     evaluation reward: 9.25\n",
      "episode: 1811   score: 10.0   memory length: 465219   epsilon: 0.27686440000944273    steps: 521    lr: 0.0001     evaluation reward: 9.25\n",
      "episode: 1812   score: 8.0   memory length: 465659   epsilon: 0.2759932000094372    steps: 440    lr: 0.0001     evaluation reward: 9.27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1813   score: 9.0   memory length: 466132   epsilon: 0.2750566600094313    steps: 473    lr: 0.0001     evaluation reward: 9.32\n",
      "episode: 1814   score: 12.0   memory length: 466618   epsilon: 0.2740943800094252    steps: 486    lr: 0.0001     evaluation reward: 9.35\n",
      "episode: 1815   score: 8.0   memory length: 467042   epsilon: 0.2732548600094199    steps: 424    lr: 0.0001     evaluation reward: 9.31\n",
      "episode: 1816   score: 12.0   memory length: 467626   epsilon: 0.2720985400094126    steps: 584    lr: 0.0001     evaluation reward: 9.32\n",
      "episode: 1817   score: 9.0   memory length: 468078   epsilon: 0.2712035800094069    steps: 452    lr: 0.0001     evaluation reward: 9.33\n",
      "episode: 1818   score: 23.0   memory length: 468689   epsilon: 0.26999380000939927    steps: 611    lr: 0.0001     evaluation reward: 9.49\n",
      "episode: 1819   score: 14.0   memory length: 469234   epsilon: 0.26891470000939244    steps: 545    lr: 0.0001     evaluation reward: 9.54\n",
      "episode: 1820   score: 13.0   memory length: 469754   epsilon: 0.2678851000093859    steps: 520    lr: 0.0001     evaluation reward: 9.58\n",
      "episode: 1821   score: 5.0   memory length: 470048   epsilon: 0.26730298000938224    steps: 294    lr: 0.0001     evaluation reward: 9.53\n",
      "episode: 1822   score: 13.0   memory length: 470684   epsilon: 0.2660437000093743    steps: 636    lr: 0.0001     evaluation reward: 9.58\n",
      "episode: 1823   score: 4.0   memory length: 470961   epsilon: 0.2654952400093708    steps: 277    lr: 0.0001     evaluation reward: 9.52\n",
      "episode: 1824   score: 4.0   memory length: 471207   epsilon: 0.2650081600093677    steps: 246    lr: 0.0001     evaluation reward: 9.48\n",
      "episode: 1825   score: 8.0   memory length: 471678   epsilon: 0.2640755800093618    steps: 471    lr: 0.0001     evaluation reward: 9.52\n",
      "episode: 1826   score: 23.0   memory length: 472261   epsilon: 0.2629212400093545    steps: 583    lr: 0.0001     evaluation reward: 9.67\n",
      "episode: 1827   score: 8.0   memory length: 472705   epsilon: 0.26204212000934896    steps: 444    lr: 0.0001     evaluation reward: 9.6\n",
      "episode: 1828   score: 8.0   memory length: 473122   epsilon: 0.26121646000934373    steps: 417    lr: 0.0001     evaluation reward: 9.64\n",
      "episode: 1829   score: 7.0   memory length: 473525   epsilon: 0.2604185200093387    steps: 403    lr: 0.0001     evaluation reward: 9.55\n",
      "episode: 1830   score: 19.0   memory length: 474106   epsilon: 0.2592681400093314    steps: 581    lr: 0.0001     evaluation reward: 9.62\n",
      "episode: 1831   score: 4.0   memory length: 474364   epsilon: 0.2587573000093282    steps: 258    lr: 0.0001     evaluation reward: 9.59\n",
      "episode: 1832   score: 9.0   memory length: 474873   epsilon: 0.2577494800093218    steps: 509    lr: 0.0001     evaluation reward: 9.56\n",
      "episode: 1833   score: 8.0   memory length: 475308   epsilon: 0.25688818000931635    steps: 435    lr: 0.0001     evaluation reward: 9.58\n",
      "episode: 1834   score: 20.0   memory length: 475957   epsilon: 0.2556031600093082    steps: 649    lr: 0.0001     evaluation reward: 9.7\n",
      "episode: 1835   score: 8.0   memory length: 476391   epsilon: 0.2547438400093028    steps: 434    lr: 0.0001     evaluation reward: 9.66\n",
      "episode: 1836   score: 6.0   memory length: 476766   epsilon: 0.2540013400092981    steps: 375    lr: 0.0001     evaluation reward: 9.61\n",
      "episode: 1837   score: 9.0   memory length: 477235   epsilon: 0.2530727200092922    steps: 469    lr: 0.0001     evaluation reward: 9.56\n",
      "episode: 1838   score: 9.0   memory length: 477638   epsilon: 0.25227478000928716    steps: 403    lr: 0.0001     evaluation reward: 9.58\n",
      "episode: 1839   score: 9.0   memory length: 478155   epsilon: 0.2512511200092807    steps: 517    lr: 0.0001     evaluation reward: 9.59\n",
      "episode: 1840   score: 11.0   memory length: 478714   epsilon: 0.2501443000092737    steps: 559    lr: 0.0001     evaluation reward: 9.66\n",
      "episode: 1841   score: 12.0   memory length: 479172   epsilon: 0.24923746000926794    steps: 458    lr: 0.0001     evaluation reward: 9.72\n",
      "episode: 1842   score: 11.0   memory length: 479724   epsilon: 0.24814450000926103    steps: 552    lr: 0.0001     evaluation reward: 9.74\n",
      "episode: 1843   score: 10.0   memory length: 480210   epsilon: 0.24718222000925494    steps: 486    lr: 0.0001     evaluation reward: 9.73\n",
      "episode: 1844   score: 10.0   memory length: 480723   epsilon: 0.2461664800092485    steps: 513    lr: 0.0001     evaluation reward: 9.73\n",
      "episode: 1845   score: 15.0   memory length: 481320   epsilon: 0.24498442000924103    steps: 597    lr: 0.0001     evaluation reward: 9.74\n",
      "episode: 1846   score: 9.0   memory length: 481784   epsilon: 0.24406570000923522    steps: 464    lr: 0.0001     evaluation reward: 9.64\n",
      "episode: 1847   score: 8.0   memory length: 482246   epsilon: 0.24315094000922943    steps: 462    lr: 0.0001     evaluation reward: 9.6\n",
      "episode: 1848   score: 6.0   memory length: 482569   epsilon: 0.2425114000092254    steps: 323    lr: 0.0001     evaluation reward: 9.55\n",
      "episode: 1849   score: 9.0   memory length: 482879   epsilon: 0.2418976000092215    steps: 310    lr: 0.0001     evaluation reward: 9.57\n",
      "episode: 1850   score: 10.0   memory length: 483380   epsilon: 0.24090562000921523    steps: 501    lr: 0.0001     evaluation reward: 9.58\n",
      "episode: 1851   score: 12.0   memory length: 483834   epsilon: 0.24000670000920954    steps: 454    lr: 0.0001     evaluation reward: 9.6\n",
      "episode: 1852   score: 12.0   memory length: 484424   epsilon: 0.23883850000920215    steps: 590    lr: 0.0001     evaluation reward: 9.64\n",
      "episode: 1853   score: 12.0   memory length: 484984   epsilon: 0.23772970000919513    steps: 560    lr: 0.0001     evaluation reward: 9.71\n",
      "episode: 1854   score: 11.0   memory length: 485504   epsilon: 0.23670010000918862    steps: 520    lr: 0.0001     evaluation reward: 9.7\n",
      "episode: 1855   score: 10.0   memory length: 486031   epsilon: 0.23565664000918202    steps: 527    lr: 0.0001     evaluation reward: 9.67\n",
      "episode: 1856   score: 7.0   memory length: 486419   epsilon: 0.23488840000917716    steps: 388    lr: 0.0001     evaluation reward: 9.63\n",
      "episode: 1857   score: 8.0   memory length: 486836   epsilon: 0.23406274000917193    steps: 417    lr: 0.0001     evaluation reward: 9.62\n",
      "episode: 1858   score: 9.0   memory length: 487323   epsilon: 0.23309848000916583    steps: 487    lr: 0.0001     evaluation reward: 9.62\n",
      "episode: 1859   score: 12.0   memory length: 487822   epsilon: 0.23211046000915958    steps: 499    lr: 0.0001     evaluation reward: 9.49\n",
      "episode: 1860   score: 9.0   memory length: 488245   epsilon: 0.23127292000915428    steps: 423    lr: 0.0001     evaluation reward: 9.48\n",
      "episode: 1861   score: 7.0   memory length: 488647   epsilon: 0.23047696000914925    steps: 402    lr: 0.0001     evaluation reward: 9.47\n",
      "episode: 1862   score: 8.0   memory length: 489085   epsilon: 0.22960972000914376    steps: 438    lr: 0.0001     evaluation reward: 9.5\n",
      "episode: 1863   score: 6.0   memory length: 489480   epsilon: 0.2288276200091388    steps: 395    lr: 0.0001     evaluation reward: 9.52\n",
      "episode: 1864   score: 9.0   memory length: 489942   epsilon: 0.22791286000913302    steps: 462    lr: 0.0001     evaluation reward: 9.54\n",
      "episode: 1865   score: 4.0   memory length: 490218   epsilon: 0.22736638000912957    steps: 276    lr: 0.0001     evaluation reward: 9.5\n",
      "episode: 1866   score: 16.0   memory length: 490923   epsilon: 0.22597048000912073    steps: 705    lr: 0.0001     evaluation reward: 9.57\n",
      "episode: 1867   score: 16.0   memory length: 491542   epsilon: 0.22474486000911298    steps: 619    lr: 0.0001     evaluation reward: 9.62\n",
      "episode: 1868   score: 17.0   memory length: 492157   epsilon: 0.22352716000910527    steps: 615    lr: 0.0001     evaluation reward: 9.71\n",
      "episode: 1869   score: 4.0   memory length: 492432   epsilon: 0.22298266000910183    steps: 275    lr: 0.0001     evaluation reward: 9.64\n",
      "episode: 1870   score: 13.0   memory length: 493013   epsilon: 0.22183228000909455    steps: 581    lr: 0.0001     evaluation reward: 9.71\n",
      "episode: 1871   score: 13.0   memory length: 493536   epsilon: 0.220796740009088    steps: 523    lr: 0.0001     evaluation reward: 9.75\n",
      "episode: 1872   score: 8.0   memory length: 494008   epsilon: 0.2198621800090821    steps: 472    lr: 0.0001     evaluation reward: 9.68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1873   score: 10.0   memory length: 494514   epsilon: 0.21886030000907575    steps: 506    lr: 0.0001     evaluation reward: 9.71\n",
      "episode: 1874   score: 11.0   memory length: 495009   epsilon: 0.21788020000906955    steps: 495    lr: 0.0001     evaluation reward: 9.75\n",
      "episode: 1875   score: 8.0   memory length: 495480   epsilon: 0.21694762000906365    steps: 471    lr: 0.0001     evaluation reward: 9.76\n",
      "episode: 1876   score: 6.0   memory length: 495839   epsilon: 0.21623680000905915    steps: 359    lr: 0.0001     evaluation reward: 9.72\n",
      "episode: 1877   score: 9.0   memory length: 496274   epsilon: 0.2153755000090537    steps: 435    lr: 0.0001     evaluation reward: 9.72\n",
      "episode: 1878   score: 18.0   memory length: 496967   epsilon: 0.21400336000904502    steps: 693    lr: 0.0001     evaluation reward: 9.85\n",
      "episode: 1879   score: 9.0   memory length: 497424   epsilon: 0.2130985000090393    steps: 457    lr: 0.0001     evaluation reward: 9.82\n",
      "episode: 1880   score: 8.0   memory length: 497869   epsilon: 0.21221740000903372    steps: 445    lr: 0.0001     evaluation reward: 9.84\n",
      "episode: 1881   score: 11.0   memory length: 498383   epsilon: 0.21119968000902728    steps: 514    lr: 0.0001     evaluation reward: 9.82\n",
      "episode: 1882   score: 18.0   memory length: 498952   epsilon: 0.21007306000902015    steps: 569    lr: 0.0001     evaluation reward: 9.95\n",
      "episode: 1883   score: 8.0   memory length: 499410   epsilon: 0.20916622000901441    steps: 458    lr: 0.0001     evaluation reward: 9.9\n",
      "episode: 1884   score: 10.0   memory length: 499917   epsilon: 0.20816236000900806    steps: 507    lr: 0.0001     evaluation reward: 9.94\n",
      "episode: 1885   score: 6.0   memory length: 500276   epsilon: 0.20745154000900357    steps: 359    lr: 0.0001     evaluation reward: 9.88\n",
      "episode: 1886   score: 5.0   memory length: 500567   epsilon: 0.20687536000899992    steps: 291    lr: 0.0001     evaluation reward: 9.77\n",
      "episode: 1887   score: 11.0   memory length: 501057   epsilon: 0.20590516000899378    steps: 490    lr: 0.0001     evaluation reward: 9.75\n",
      "episode: 1888   score: 9.0   memory length: 501537   epsilon: 0.20495476000898777    steps: 480    lr: 0.0001     evaluation reward: 9.69\n",
      "episode: 1889   score: 13.0   memory length: 502177   epsilon: 0.20368756000897975    steps: 640    lr: 0.0001     evaluation reward: 9.75\n",
      "episode: 1890   score: 5.0   memory length: 502468   epsilon: 0.2031113800089761    steps: 291    lr: 0.0001     evaluation reward: 9.72\n",
      "episode: 1891   score: 12.0   memory length: 503047   epsilon: 0.20196496000896885    steps: 579    lr: 0.0001     evaluation reward: 9.74\n",
      "episode: 1892   score: 6.0   memory length: 503405   epsilon: 0.20125612000896437    steps: 358    lr: 0.0001     evaluation reward: 9.74\n",
      "episode: 1893   score: 13.0   memory length: 503890   epsilon: 0.2002958200089583    steps: 485    lr: 0.0001     evaluation reward: 9.82\n",
      "episode: 1894   score: 5.0   memory length: 504217   epsilon: 0.1996483600089542    steps: 327    lr: 0.0001     evaluation reward: 9.77\n",
      "episode: 1895   score: 14.0   memory length: 504878   epsilon: 0.19833958000894591    steps: 661    lr: 0.0001     evaluation reward: 9.85\n",
      "episode: 1896   score: 6.0   memory length: 505194   epsilon: 0.19771390000894196    steps: 316    lr: 0.0001     evaluation reward: 9.78\n",
      "episode: 1897   score: 18.0   memory length: 505854   epsilon: 0.1964071000089337    steps: 660    lr: 0.0001     evaluation reward: 9.87\n",
      "episode: 1898   score: 8.0   memory length: 506309   epsilon: 0.195506200008928    steps: 455    lr: 0.0001     evaluation reward: 9.91\n",
      "episode: 1899   score: 6.0   memory length: 506650   epsilon: 0.19483102000892372    steps: 341    lr: 0.0001     evaluation reward: 9.9\n",
      "episode: 1900   score: 18.0   memory length: 507272   epsilon: 0.19359946000891592    steps: 622    lr: 0.0001     evaluation reward: 10.02\n",
      "episode: 1901   score: 6.0   memory length: 507612   epsilon: 0.19292626000891167    steps: 340    lr: 0.0001     evaluation reward: 10.04\n",
      "episode: 1902   score: 11.0   memory length: 508117   epsilon: 0.19192636000890534    steps: 505    lr: 0.0001     evaluation reward: 10.07\n",
      "episode: 1903   score: 14.0   memory length: 508789   epsilon: 0.19059580000889692    steps: 672    lr: 0.0001     evaluation reward: 10.12\n",
      "episode: 1904   score: 7.0   memory length: 509146   epsilon: 0.18988894000889245    steps: 357    lr: 0.0001     evaluation reward: 10.05\n",
      "episode: 1905   score: 7.0   memory length: 509500   epsilon: 0.189188020008888    steps: 354    lr: 0.0001     evaluation reward: 10.03\n",
      "episode: 1906   score: 8.0   memory length: 509913   epsilon: 0.18837028000888284    steps: 413    lr: 0.0001     evaluation reward: 9.95\n",
      "episode: 1907   score: 18.0   memory length: 510524   epsilon: 0.18716050000887519    steps: 611    lr: 0.0001     evaluation reward: 10.05\n",
      "episode: 1908   score: 10.0   memory length: 511075   epsilon: 0.18606952000886828    steps: 551    lr: 0.0001     evaluation reward: 10.06\n",
      "episode: 1909   score: 11.0   memory length: 511448   epsilon: 0.1853309800088636    steps: 373    lr: 0.0001     evaluation reward: 10.12\n",
      "episode: 1910   score: 11.0   memory length: 512011   epsilon: 0.18421624000885656    steps: 563    lr: 0.0001     evaluation reward: 10.17\n",
      "episode: 1911   score: 13.0   memory length: 512497   epsilon: 0.18325396000885047    steps: 486    lr: 0.0001     evaluation reward: 10.2\n",
      "episode: 1912   score: 16.0   memory length: 512984   epsilon: 0.18228970000884437    steps: 487    lr: 0.0001     evaluation reward: 10.28\n",
      "episode: 1913   score: 11.0   memory length: 513593   epsilon: 0.18108388000883674    steps: 609    lr: 0.0001     evaluation reward: 10.3\n",
      "episode: 1914   score: 11.0   memory length: 514131   epsilon: 0.18001864000883    steps: 538    lr: 0.0001     evaluation reward: 10.29\n",
      "episode: 1915   score: 14.0   memory length: 514635   epsilon: 0.17902072000882369    steps: 504    lr: 0.0001     evaluation reward: 10.35\n",
      "episode: 1916   score: 8.0   memory length: 515113   epsilon: 0.1780742800088177    steps: 478    lr: 0.0001     evaluation reward: 10.31\n",
      "episode: 1917   score: 5.0   memory length: 515422   epsilon: 0.17746246000881383    steps: 309    lr: 0.0001     evaluation reward: 10.27\n",
      "episode: 1918   score: 11.0   memory length: 515937   epsilon: 0.17644276000880738    steps: 515    lr: 0.0001     evaluation reward: 10.15\n",
      "episode: 1919   score: 17.0   memory length: 516593   epsilon: 0.17514388000879916    steps: 656    lr: 0.0001     evaluation reward: 10.18\n",
      "episode: 1920   score: 6.0   memory length: 516914   epsilon: 0.17450830000879514    steps: 321    lr: 0.0001     evaluation reward: 10.11\n",
      "episode: 1921   score: 6.0   memory length: 517229   epsilon: 0.1738846000087912    steps: 315    lr: 0.0001     evaluation reward: 10.12\n",
      "episode: 1922   score: 11.0   memory length: 517750   epsilon: 0.17285302000878466    steps: 521    lr: 0.0001     evaluation reward: 10.1\n",
      "episode: 1923   score: 8.0   memory length: 518245   epsilon: 0.17187292000877846    steps: 495    lr: 0.0001     evaluation reward: 10.14\n",
      "episode: 1924   score: 7.0   memory length: 518617   epsilon: 0.1711363600087738    steps: 372    lr: 0.0001     evaluation reward: 10.17\n",
      "episode: 1925   score: 8.0   memory length: 519023   epsilon: 0.17033248000876872    steps: 406    lr: 0.0001     evaluation reward: 10.17\n",
      "episode: 1926   score: 14.0   memory length: 519534   epsilon: 0.16932070000876231    steps: 511    lr: 0.0001     evaluation reward: 10.08\n",
      "episode: 1927   score: 8.0   memory length: 519971   epsilon: 0.16845544000875684    steps: 437    lr: 0.0001     evaluation reward: 10.08\n",
      "episode: 1928   score: 8.0   memory length: 520399   epsilon: 0.16760800000875148    steps: 428    lr: 0.0001     evaluation reward: 10.08\n",
      "episode: 1929   score: 13.0   memory length: 521036   epsilon: 0.1663467400087435    steps: 637    lr: 0.0001     evaluation reward: 10.14\n",
      "episode: 1930   score: 13.0   memory length: 521664   epsilon: 0.16510330000873563    steps: 628    lr: 0.0001     evaluation reward: 10.08\n",
      "episode: 1931   score: 9.0   memory length: 522134   epsilon: 0.16417270000872974    steps: 470    lr: 0.0001     evaluation reward: 10.13\n",
      "episode: 1932   score: 7.0   memory length: 522491   epsilon: 0.16346584000872527    steps: 357    lr: 0.0001     evaluation reward: 10.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1933   score: 17.0   memory length: 523115   epsilon: 0.16223032000871745    steps: 624    lr: 0.0001     evaluation reward: 10.2\n",
      "episode: 1934   score: 19.0   memory length: 523769   epsilon: 0.16093540000870926    steps: 654    lr: 0.0001     evaluation reward: 10.19\n",
      "episode: 1935   score: 14.0   memory length: 524361   epsilon: 0.15976324000870185    steps: 592    lr: 0.0001     evaluation reward: 10.25\n",
      "episode: 1936   score: 6.0   memory length: 524680   epsilon: 0.15913162000869785    steps: 319    lr: 0.0001     evaluation reward: 10.25\n",
      "episode: 1937   score: 9.0   memory length: 525115   epsilon: 0.1582703200086924    steps: 435    lr: 0.0001     evaluation reward: 10.25\n",
      "episode: 1938   score: 12.0   memory length: 525640   epsilon: 0.15723082000868582    steps: 525    lr: 0.0001     evaluation reward: 10.28\n",
      "episode: 1939   score: 9.0   memory length: 526121   epsilon: 0.1562784400086798    steps: 481    lr: 0.0001     evaluation reward: 10.28\n",
      "episode: 1940   score: 11.0   memory length: 526725   epsilon: 0.15508252000867223    steps: 604    lr: 0.0001     evaluation reward: 10.28\n",
      "episode: 1941   score: 11.0   memory length: 527216   epsilon: 0.15411034000866608    steps: 491    lr: 0.0001     evaluation reward: 10.27\n",
      "episode: 1942   score: 8.0   memory length: 527604   epsilon: 0.15334210000866122    steps: 388    lr: 0.0001     evaluation reward: 10.24\n",
      "episode: 1943   score: 12.0   memory length: 528164   epsilon: 0.1522333000086542    steps: 560    lr: 0.0001     evaluation reward: 10.26\n",
      "episode: 1944   score: 6.0   memory length: 528501   epsilon: 0.15156604000864998    steps: 337    lr: 0.0001     evaluation reward: 10.22\n",
      "episode: 1945   score: 23.0   memory length: 529240   epsilon: 0.15010282000864072    steps: 739    lr: 0.0001     evaluation reward: 10.3\n",
      "episode: 1946   score: 12.0   memory length: 529829   epsilon: 0.14893660000863335    steps: 589    lr: 0.0001     evaluation reward: 10.33\n",
      "episode: 1947   score: 11.0   memory length: 530400   epsilon: 0.1478060200086262    steps: 571    lr: 0.0001     evaluation reward: 10.36\n",
      "episode: 1948   score: 9.0   memory length: 530811   epsilon: 0.14699224000862104    steps: 411    lr: 0.0001     evaluation reward: 10.39\n",
      "episode: 1949   score: 14.0   memory length: 531400   epsilon: 0.14582602000861367    steps: 589    lr: 0.0001     evaluation reward: 10.44\n",
      "episode: 1950   score: 15.0   memory length: 532104   epsilon: 0.14443210000860485    steps: 704    lr: 0.0001     evaluation reward: 10.49\n",
      "episode: 1951   score: 11.0   memory length: 532610   epsilon: 0.1434302200085985    steps: 506    lr: 0.0001     evaluation reward: 10.48\n",
      "episode: 1952   score: 17.0   memory length: 533225   epsilon: 0.1422125200085908    steps: 615    lr: 0.0001     evaluation reward: 10.53\n",
      "episode: 1953   score: 15.0   memory length: 533820   epsilon: 0.14103442000858335    steps: 595    lr: 0.0001     evaluation reward: 10.56\n",
      "episode: 1954   score: 10.0   memory length: 534313   epsilon: 0.14005828000857717    steps: 493    lr: 0.0001     evaluation reward: 10.55\n",
      "episode: 1955   score: 6.0   memory length: 534671   epsilon: 0.1393494400085727    steps: 358    lr: 0.0001     evaluation reward: 10.51\n",
      "episode: 1956   score: 7.0   memory length: 535056   epsilon: 0.13858714000856787    steps: 385    lr: 0.0001     evaluation reward: 10.51\n",
      "episode: 1957   score: 9.0   memory length: 535527   epsilon: 0.13765456000856197    steps: 471    lr: 0.0001     evaluation reward: 10.52\n",
      "episode: 1958   score: 14.0   memory length: 536164   epsilon: 0.13639330000855399    steps: 637    lr: 0.0001     evaluation reward: 10.57\n",
      "episode: 1959   score: 4.0   memory length: 536407   epsilon: 0.13591216000855094    steps: 243    lr: 0.0001     evaluation reward: 10.49\n",
      "episode: 1960   score: 24.0   memory length: 536893   epsilon: 0.13494988000854485    steps: 486    lr: 0.0001     evaluation reward: 10.64\n",
      "episode: 1961   score: 11.0   memory length: 537288   epsilon: 0.1341677800085399    steps: 395    lr: 0.0001     evaluation reward: 10.68\n",
      "episode: 1962   score: 8.0   memory length: 537674   epsilon: 0.13340350000853507    steps: 386    lr: 0.0001     evaluation reward: 10.68\n",
      "episode: 1963   score: 10.0   memory length: 538215   epsilon: 0.1323323200085283    steps: 541    lr: 0.0001     evaluation reward: 10.72\n",
      "episode: 1964   score: 11.0   memory length: 538779   epsilon: 0.13121560000852123    steps: 564    lr: 0.0001     evaluation reward: 10.74\n",
      "episode: 1965   score: 6.0   memory length: 539114   epsilon: 0.13055230000851703    steps: 335    lr: 0.0001     evaluation reward: 10.76\n",
      "episode: 1966   score: 11.0   memory length: 539614   epsilon: 0.12956230000851077    steps: 500    lr: 0.0001     evaluation reward: 10.71\n",
      "episode: 1967   score: 4.0   memory length: 539908   epsilon: 0.12898018000850708    steps: 294    lr: 0.0001     evaluation reward: 10.59\n",
      "episode: 1968   score: 7.0   memory length: 540268   epsilon: 0.12826738000850257    steps: 360    lr: 0.0001     evaluation reward: 10.49\n",
      "episode: 1969   score: 6.0   memory length: 540644   epsilon: 0.12752290000849786    steps: 376    lr: 0.0001     evaluation reward: 10.51\n",
      "episode: 1970   score: 7.0   memory length: 541039   epsilon: 0.12674080000849292    steps: 395    lr: 0.0001     evaluation reward: 10.45\n",
      "episode: 1971   score: 14.0   memory length: 541589   epsilon: 0.12565180000848603    steps: 550    lr: 0.0001     evaluation reward: 10.46\n",
      "episode: 1972   score: 14.0   memory length: 542230   epsilon: 0.12438262000848233    steps: 641    lr: 0.0001     evaluation reward: 10.52\n",
      "episode: 1973   score: 6.0   memory length: 542568   epsilon: 0.12371338000848278    steps: 338    lr: 0.0001     evaluation reward: 10.48\n",
      "episode: 1974   score: 9.0   memory length: 543009   epsilon: 0.12284020000848338    steps: 441    lr: 0.0001     evaluation reward: 10.46\n",
      "episode: 1975   score: 7.0   memory length: 543404   epsilon: 0.12205810000848391    steps: 395    lr: 0.0001     evaluation reward: 10.45\n",
      "episode: 1976   score: 8.0   memory length: 543797   epsilon: 0.12127996000848444    steps: 393    lr: 0.0001     evaluation reward: 10.47\n",
      "episode: 1977   score: 9.0   memory length: 544242   epsilon: 0.12039886000848504    steps: 445    lr: 0.0001     evaluation reward: 10.47\n",
      "episode: 1978   score: 9.0   memory length: 544697   epsilon: 0.11949796000848566    steps: 455    lr: 0.0001     evaluation reward: 10.38\n",
      "episode: 1979   score: 13.0   memory length: 545143   epsilon: 0.11861488000848626    steps: 446    lr: 0.0001     evaluation reward: 10.42\n",
      "episode: 1980   score: 12.0   memory length: 545682   epsilon: 0.11754766000848699    steps: 539    lr: 0.0001     evaluation reward: 10.46\n",
      "episode: 1981   score: 11.0   memory length: 546236   epsilon: 0.11645074000848774    steps: 554    lr: 0.0001     evaluation reward: 10.46\n",
      "episode: 1982   score: 16.0   memory length: 546790   epsilon: 0.11535382000848848    steps: 554    lr: 0.0001     evaluation reward: 10.44\n",
      "episode: 1983   score: 13.0   memory length: 547172   epsilon: 0.114597460008489    steps: 382    lr: 0.0001     evaluation reward: 10.49\n",
      "episode: 1984   score: 26.0   memory length: 547881   epsilon: 0.11319364000848996    steps: 709    lr: 0.0001     evaluation reward: 10.65\n",
      "episode: 1985   score: 11.0   memory length: 548431   epsilon: 0.1121046400084907    steps: 550    lr: 0.0001     evaluation reward: 10.7\n",
      "episode: 1986   score: 28.0   memory length: 549217   epsilon: 0.11054836000849176    steps: 786    lr: 0.0001     evaluation reward: 10.93\n",
      "episode: 1987   score: 7.0   memory length: 549610   epsilon: 0.10977022000849229    steps: 393    lr: 0.0001     evaluation reward: 10.89\n",
      "episode: 1988   score: 14.0   memory length: 550103   epsilon: 0.10879408000849296    steps: 493    lr: 0.0001     evaluation reward: 10.94\n",
      "episode: 1989   score: 7.0   memory length: 550493   epsilon: 0.10802188000849348    steps: 390    lr: 0.0001     evaluation reward: 10.88\n",
      "episode: 1990   score: 9.0   memory length: 550926   epsilon: 0.10716454000849407    steps: 433    lr: 0.0001     evaluation reward: 10.92\n",
      "episode: 1991   score: 15.0   memory length: 551504   epsilon: 0.10602010000849485    steps: 578    lr: 0.0001     evaluation reward: 10.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1992   score: 14.0   memory length: 552051   epsilon: 0.10493704000849559    steps: 547    lr: 0.0001     evaluation reward: 11.03\n",
      "episode: 1993   score: 8.0   memory length: 552433   epsilon: 0.1041806800084961    steps: 382    lr: 0.0001     evaluation reward: 10.98\n",
      "episode: 1994   score: 11.0   memory length: 552994   epsilon: 0.10306990000849686    steps: 561    lr: 0.0001     evaluation reward: 11.04\n",
      "episode: 1995   score: 9.0   memory length: 553468   epsilon: 0.1021313800084975    steps: 474    lr: 0.0001     evaluation reward: 10.99\n",
      "episode: 1996   score: 14.0   memory length: 553976   epsilon: 0.10112554000849819    steps: 508    lr: 0.0001     evaluation reward: 11.07\n",
      "episode: 1997   score: 10.0   memory length: 554465   epsilon: 0.10015732000849885    steps: 489    lr: 0.0001     evaluation reward: 10.99\n",
      "episode: 1998   score: 11.0   memory length: 555024   epsilon: 0.0990505000084996    steps: 559    lr: 0.0001     evaluation reward: 11.02\n",
      "episode: 1999   score: 9.0   memory length: 555455   epsilon: 0.09819712000850019    steps: 431    lr: 0.0001     evaluation reward: 11.05\n",
      "episode: 2000   score: 11.0   memory length: 555991   epsilon: 0.09713584000850091    steps: 536    lr: 0.0001     evaluation reward: 10.98\n",
      "episode: 2001   score: 11.0   memory length: 556552   epsilon: 0.09602506000850167    steps: 561    lr: 0.0001     evaluation reward: 11.03\n",
      "episode: 2002   score: 20.0   memory length: 557233   epsilon: 0.09467668000850259    steps: 681    lr: 0.0001     evaluation reward: 11.12\n",
      "episode: 2003   score: 13.0   memory length: 557874   epsilon: 0.09340750000850345    steps: 641    lr: 0.0001     evaluation reward: 11.11\n",
      "episode: 2004   score: 26.0   memory length: 558638   epsilon: 0.09189478000850448    steps: 764    lr: 0.0001     evaluation reward: 11.3\n",
      "episode: 2005   score: 16.0   memory length: 559201   epsilon: 0.09078004000850524    steps: 563    lr: 0.0001     evaluation reward: 11.39\n",
      "episode: 2006   score: 18.0   memory length: 559956   epsilon: 0.08928514000850626    steps: 755    lr: 0.0001     evaluation reward: 11.49\n",
      "episode: 2007   score: 12.0   memory length: 560482   epsilon: 0.08824366000850697    steps: 526    lr: 0.0001     evaluation reward: 11.43\n",
      "episode: 2008   score: 9.0   memory length: 560949   epsilon: 0.0873190000085076    steps: 467    lr: 0.0001     evaluation reward: 11.42\n",
      "episode: 2009   score: 11.0   memory length: 561432   epsilon: 0.08636266000850826    steps: 483    lr: 0.0001     evaluation reward: 11.42\n",
      "episode: 2010   score: 11.0   memory length: 561958   epsilon: 0.08532118000850897    steps: 526    lr: 0.0001     evaluation reward: 11.42\n",
      "episode: 2011   score: 13.0   memory length: 562548   epsilon: 0.08415298000850976    steps: 590    lr: 0.0001     evaluation reward: 11.42\n",
      "episode: 2012   score: 9.0   memory length: 562980   epsilon: 0.08329762000851035    steps: 432    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 2013   score: 11.0   memory length: 563539   epsilon: 0.0821908000085111    steps: 559    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 2014   score: 8.0   memory length: 564026   epsilon: 0.08122654000851176    steps: 487    lr: 0.0001     evaluation reward: 11.32\n",
      "episode: 2015   score: 11.0   memory length: 564612   epsilon: 0.08006626000851255    steps: 586    lr: 0.0001     evaluation reward: 11.29\n",
      "episode: 2016   score: 8.0   memory length: 565045   epsilon: 0.07920892000851314    steps: 433    lr: 0.0001     evaluation reward: 11.29\n",
      "episode: 2017   score: 12.0   memory length: 565635   epsilon: 0.07804072000851393    steps: 590    lr: 0.0001     evaluation reward: 11.36\n",
      "episode: 2018   score: 14.0   memory length: 566158   epsilon: 0.07700518000851464    steps: 523    lr: 0.0001     evaluation reward: 11.39\n",
      "episode: 2019   score: 7.0   memory length: 566540   epsilon: 0.07624882000851516    steps: 382    lr: 0.0001     evaluation reward: 11.29\n",
      "episode: 2020   score: 12.0   memory length: 566949   epsilon: 0.07543900000851571    steps: 409    lr: 0.0001     evaluation reward: 11.35\n",
      "episode: 2021   score: 9.0   memory length: 567388   epsilon: 0.0745697800085163    steps: 439    lr: 0.0001     evaluation reward: 11.38\n",
      "episode: 2022   score: 13.0   memory length: 567955   epsilon: 0.07344712000851707    steps: 567    lr: 0.0001     evaluation reward: 11.4\n",
      "episode: 2023   score: 15.0   memory length: 568629   epsilon: 0.07211260000851798    steps: 674    lr: 0.0001     evaluation reward: 11.47\n",
      "episode: 2024   score: 11.0   memory length: 569175   epsilon: 0.07103152000851871    steps: 546    lr: 0.0001     evaluation reward: 11.51\n",
      "episode: 2025   score: 21.0   memory length: 569812   epsilon: 0.06977026000851957    steps: 637    lr: 0.0001     evaluation reward: 11.64\n",
      "episode: 2026   score: 20.0   memory length: 570427   epsilon: 0.0685525600085204    steps: 615    lr: 0.0001     evaluation reward: 11.7\n",
      "episode: 2027   score: 6.0   memory length: 570789   epsilon: 0.0678358000085209    steps: 362    lr: 0.0001     evaluation reward: 11.68\n",
      "episode: 2028   score: 8.0   memory length: 571218   epsilon: 0.06698638000852147    steps: 429    lr: 0.0001     evaluation reward: 11.68\n",
      "episode: 2029   score: 8.0   memory length: 571631   epsilon: 0.06616864000852203    steps: 413    lr: 0.0001     evaluation reward: 11.63\n",
      "episode: 2030   score: 9.0   memory length: 571957   epsilon: 0.06552316000852247    steps: 326    lr: 0.0001     evaluation reward: 11.59\n",
      "episode: 2031   score: 16.0   memory length: 572541   epsilon: 0.06436684000852326    steps: 584    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 2032   score: 11.0   memory length: 573004   epsilon: 0.06345010000852389    steps: 463    lr: 0.0001     evaluation reward: 11.7\n",
      "episode: 2033   score: 9.0   memory length: 573479   epsilon: 0.06250960000852453    steps: 475    lr: 0.0001     evaluation reward: 11.62\n",
      "episode: 2034   score: 11.0   memory length: 574052   epsilon: 0.0613750600085253    steps: 573    lr: 0.0001     evaluation reward: 11.54\n",
      "episode: 2035   score: 14.0   memory length: 574566   epsilon: 0.060357340008525995    steps: 514    lr: 0.0001     evaluation reward: 11.54\n",
      "episode: 2036   score: 11.0   memory length: 575062   epsilon: 0.059375260008526665    steps: 496    lr: 0.0001     evaluation reward: 11.59\n",
      "episode: 2037   score: 14.0   memory length: 575693   epsilon: 0.05812588000852752    steps: 631    lr: 0.0001     evaluation reward: 11.64\n",
      "episode: 2038   score: 18.0   memory length: 576207   epsilon: 0.05710816000852821    steps: 514    lr: 0.0001     evaluation reward: 11.7\n",
      "episode: 2039   score: 14.0   memory length: 576853   epsilon: 0.05582908000852908    steps: 646    lr: 0.0001     evaluation reward: 11.75\n",
      "episode: 2040   score: 18.0   memory length: 577520   epsilon: 0.054508420008529984    steps: 667    lr: 0.0001     evaluation reward: 11.82\n",
      "episode: 2041   score: 19.0   memory length: 578095   epsilon: 0.05336992000853076    steps: 575    lr: 0.0001     evaluation reward: 11.9\n",
      "episode: 2042   score: 8.0   memory length: 578528   epsilon: 0.052512580008531345    steps: 433    lr: 0.0001     evaluation reward: 11.9\n",
      "episode: 2043   score: 11.0   memory length: 578984   epsilon: 0.05160970000853196    steps: 456    lr: 0.0001     evaluation reward: 11.89\n",
      "episode: 2044   score: 17.0   memory length: 579579   epsilon: 0.050431600008532765    steps: 595    lr: 0.0001     evaluation reward: 12.0\n",
      "episode: 2045   score: 5.0   memory length: 579887   epsilon: 0.04982176000853318    steps: 308    lr: 0.0001     evaluation reward: 11.82\n",
      "episode: 2046   score: 15.0   memory length: 580490   epsilon: 0.048627820008533995    steps: 603    lr: 0.0001     evaluation reward: 11.85\n",
      "episode: 2047   score: 8.0   memory length: 580895   epsilon: 0.04782592000853454    steps: 405    lr: 0.0001     evaluation reward: 11.82\n",
      "episode: 2048   score: 17.0   memory length: 581531   epsilon: 0.0465666400085354    steps: 636    lr: 0.0001     evaluation reward: 11.9\n",
      "episode: 2049   score: 4.0   memory length: 581790   epsilon: 0.04605382000853575    steps: 259    lr: 0.0001     evaluation reward: 11.8\n",
      "episode: 2050   score: 10.0   memory length: 582327   epsilon: 0.044990560008536476    steps: 537    lr: 0.0001     evaluation reward: 11.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2051   score: 12.0   memory length: 582774   epsilon: 0.04410550000853708    steps: 447    lr: 0.0001     evaluation reward: 11.76\n",
      "episode: 2052   score: 7.0   memory length: 583201   epsilon: 0.043260040008537656    steps: 427    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 2053   score: 15.0   memory length: 583752   epsilon: 0.0421690600085384    steps: 551    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 2054   score: 10.0   memory length: 584262   epsilon: 0.04115926000853909    steps: 510    lr: 0.0001     evaluation reward: 11.66\n",
      "episode: 2055   score: 9.0   memory length: 584702   epsilon: 0.04028806000853968    steps: 440    lr: 0.0001     evaluation reward: 11.69\n",
      "episode: 2056   score: 20.0   memory length: 585411   epsilon: 0.03888424000854064    steps: 709    lr: 0.0001     evaluation reward: 11.82\n",
      "episode: 2057   score: 9.0   memory length: 585894   epsilon: 0.03792790000854129    steps: 483    lr: 0.0001     evaluation reward: 11.82\n",
      "episode: 2058   score: 13.0   memory length: 586455   epsilon: 0.03681712000854205    steps: 561    lr: 0.0001     evaluation reward: 11.81\n",
      "episode: 2059   score: 19.0   memory length: 587177   epsilon: 0.035387560008543026    steps: 722    lr: 0.0001     evaluation reward: 11.96\n",
      "episode: 2060   score: 15.0   memory length: 587726   epsilon: 0.03430054000854377    steps: 549    lr: 0.0001     evaluation reward: 11.87\n",
      "episode: 2061   score: 8.0   memory length: 588159   epsilon: 0.03344320000854435    steps: 433    lr: 0.0001     evaluation reward: 11.84\n",
      "episode: 2062   score: 8.0   memory length: 588638   epsilon: 0.032494780008545    steps: 479    lr: 0.0001     evaluation reward: 11.84\n",
      "episode: 2063   score: 11.0   memory length: 589225   epsilon: 0.03133252000854579    steps: 587    lr: 0.0001     evaluation reward: 11.85\n",
      "episode: 2064   score: 14.0   memory length: 589842   epsilon: 0.030110860008546625    steps: 617    lr: 0.0001     evaluation reward: 11.88\n",
      "episode: 2065   score: 10.0   memory length: 590330   epsilon: 0.029144620008547284    steps: 488    lr: 0.0001     evaluation reward: 11.92\n",
      "episode: 2066   score: 13.0   memory length: 591023   epsilon: 0.02777248000854822    steps: 693    lr: 0.0001     evaluation reward: 11.94\n",
      "episode: 2067   score: 22.0   memory length: 591732   epsilon: 0.026368660008549177    steps: 709    lr: 0.0001     evaluation reward: 12.12\n",
      "episode: 2068   score: 22.0   memory length: 592512   epsilon: 0.02482426000855023    steps: 780    lr: 0.0001     evaluation reward: 12.27\n",
      "episode: 2069   score: 9.0   memory length: 593009   epsilon: 0.023840200008550902    steps: 497    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 2070   score: 10.0   memory length: 593563   epsilon: 0.02274328000855165    steps: 554    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 2071   score: 13.0   memory length: 594190   epsilon: 0.021501820008552497    steps: 627    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2072   score: 14.0   memory length: 594841   epsilon: 0.020212840008553376    steps: 651    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2073   score: 14.0   memory length: 595361   epsilon: 0.01918324000855408    steps: 520    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2074   score: 9.0   memory length: 595822   epsilon: 0.0182704600085547    steps: 461    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2075   score: 12.0   memory length: 596382   epsilon: 0.017161660008555457    steps: 560    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2076   score: 20.0   memory length: 597059   epsilon: 0.01582120000855637    steps: 677    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2077   score: 13.0   memory length: 597670   epsilon: 0.014611420008556308    steps: 611    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2078   score: 16.0   memory length: 598346   epsilon: 0.013272940008556049    steps: 676    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2079   score: 8.0   memory length: 598766   epsilon: 0.012441340008555887    steps: 420    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2080   score: 20.0   memory length: 599392   epsilon: 0.011201860008555647    steps: 626    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2081   score: 8.0   memory length: 599808   epsilon: 0.010378180008555487    steps: 416    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2082   score: 7.0   memory length: 600184   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2083   score: 15.0   memory length: 600742   epsilon: 0.009998020008555413    steps: 558    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2084   score: 18.0   memory length: 601381   epsilon: 0.009998020008555413    steps: 639    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2085   score: 7.0   memory length: 601791   epsilon: 0.009998020008555413    steps: 410    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2086   score: 18.0   memory length: 602416   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 2087   score: 9.0   memory length: 602904   epsilon: 0.009998020008555413    steps: 488    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2088   score: 17.0   memory length: 603575   epsilon: 0.009998020008555413    steps: 671    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2089   score: 12.0   memory length: 604111   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2090   score: 9.0   memory length: 604571   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2091   score: 15.0   memory length: 605151   epsilon: 0.009998020008555413    steps: 580    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2092   score: 13.0   memory length: 605810   epsilon: 0.009998020008555413    steps: 659    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 2093   score: 11.0   memory length: 606315   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2094   score: 12.0   memory length: 606872   epsilon: 0.009998020008555413    steps: 557    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2095   score: 10.0   memory length: 607372   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2096   score: 16.0   memory length: 608086   epsilon: 0.009998020008555413    steps: 714    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2097   score: 15.0   memory length: 608754   epsilon: 0.009998020008555413    steps: 668    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2098   score: 11.0   memory length: 609307   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2099   score: 15.0   memory length: 609824   epsilon: 0.009998020008555413    steps: 517    lr: 0.0001     evaluation reward: 12.66\n",
      "episode: 2100   score: 11.0   memory length: 610336   epsilon: 0.009998020008555413    steps: 512    lr: 0.0001     evaluation reward: 12.66\n",
      "episode: 2101   score: 12.0   memory length: 610975   epsilon: 0.009998020008555413    steps: 639    lr: 0.0001     evaluation reward: 12.67\n",
      "episode: 2102   score: 17.0   memory length: 611645   epsilon: 0.009998020008555413    steps: 670    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2103   score: 13.0   memory length: 612257   epsilon: 0.009998020008555413    steps: 612    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2104   score: 17.0   memory length: 612908   epsilon: 0.009998020008555413    steps: 651    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2105   score: 22.0   memory length: 613473   epsilon: 0.009998020008555413    steps: 565    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2106   score: 20.0   memory length: 614169   epsilon: 0.009998020008555413    steps: 696    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2107   score: 15.0   memory length: 614869   epsilon: 0.009998020008555413    steps: 700    lr: 0.0001     evaluation reward: 12.66\n",
      "episode: 2108   score: 15.0   memory length: 615558   epsilon: 0.009998020008555413    steps: 689    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2109   score: 8.0   memory length: 615983   epsilon: 0.009998020008555413    steps: 425    lr: 0.0001     evaluation reward: 12.69\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2110   score: 17.0   memory length: 616541   epsilon: 0.009998020008555413    steps: 558    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2111   score: 16.0   memory length: 617184   epsilon: 0.009998020008555413    steps: 643    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2112   score: 16.0   memory length: 617788   epsilon: 0.009998020008555413    steps: 604    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2113   score: 9.0   memory length: 618203   epsilon: 0.009998020008555413    steps: 415    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 2114   score: 12.0   memory length: 618813   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 2115   score: 16.0   memory length: 619422   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2116   score: 21.0   memory length: 620083   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2117   score: 8.0   memory length: 620515   epsilon: 0.009998020008555413    steps: 432    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2118   score: 10.0   memory length: 621046   epsilon: 0.009998020008555413    steps: 531    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 2119   score: 16.0   memory length: 621653   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 13.06\n",
      "episode: 2120   score: 14.0   memory length: 622290   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2121   score: 11.0   memory length: 622802   epsilon: 0.009998020008555413    steps: 512    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2122   score: 8.0   memory length: 623258   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2123   score: 6.0   memory length: 623602   epsilon: 0.009998020008555413    steps: 344    lr: 0.0001     evaluation reward: 12.96\n",
      "episode: 2124   score: 13.0   memory length: 624189   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2125   score: 25.0   memory length: 625045   epsilon: 0.009998020008555413    steps: 856    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2126   score: 8.0   memory length: 625519   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 2127   score: 14.0   memory length: 626164   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2128   score: 13.0   memory length: 626756   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2129   score: 9.0   memory length: 627256   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 13.04\n",
      "episode: 2130   score: 15.0   memory length: 627950   epsilon: 0.009998020008555413    steps: 694    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2131   score: 8.0   memory length: 628355   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2132   score: 12.0   memory length: 628936   epsilon: 0.009998020008555413    steps: 581    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2133   score: 18.0   memory length: 629624   epsilon: 0.009998020008555413    steps: 688    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2134   score: 6.0   memory length: 629941   epsilon: 0.009998020008555413    steps: 317    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2135   score: 10.0   memory length: 630448   epsilon: 0.009998020008555413    steps: 507    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2136   score: 11.0   memory length: 631041   epsilon: 0.009998020008555413    steps: 593    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2137   score: 5.0   memory length: 631348   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 2138   score: 17.0   memory length: 632049   epsilon: 0.009998020008555413    steps: 701    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2139   score: 11.0   memory length: 632547   epsilon: 0.009998020008555413    steps: 498    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 2140   score: 14.0   memory length: 633209   epsilon: 0.009998020008555413    steps: 662    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 2141   score: 13.0   memory length: 633818   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2142   score: 19.0   memory length: 634524   epsilon: 0.009998020008555413    steps: 706    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 2143   score: 17.0   memory length: 635111   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 2144   score: 13.0   memory length: 635592   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2145   score: 8.0   memory length: 635999   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 12.96\n",
      "episode: 2146   score: 11.0   memory length: 636459   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2147   score: 9.0   memory length: 636974   epsilon: 0.009998020008555413    steps: 515    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2148   score: 9.0   memory length: 637446   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2149   score: 16.0   memory length: 637946   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 2150   score: 13.0   memory length: 638561   epsilon: 0.009998020008555413    steps: 615    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 2151   score: 19.0   memory length: 639254   epsilon: 0.009998020008555413    steps: 693    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2152   score: 10.0   memory length: 639779   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2153   score: 18.0   memory length: 640339   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2154   score: 7.0   memory length: 640746   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2155   score: 15.0   memory length: 641431   epsilon: 0.009998020008555413    steps: 685    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 2156   score: 16.0   memory length: 642039   epsilon: 0.009998020008555413    steps: 608    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2157   score: 10.0   memory length: 642539   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2158   score: 17.0   memory length: 643117   epsilon: 0.009998020008555413    steps: 578    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2159   score: 12.0   memory length: 643694   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2160   score: 12.0   memory length: 644263   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2161   score: 9.0   memory length: 644697   epsilon: 0.009998020008555413    steps: 434    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2162   score: 14.0   memory length: 645373   epsilon: 0.009998020008555413    steps: 676    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2163   score: 17.0   memory length: 646026   epsilon: 0.009998020008555413    steps: 653    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2164   score: 8.0   memory length: 646497   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2165   score: 17.0   memory length: 647328   epsilon: 0.009998020008555413    steps: 831    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2166   score: 17.0   memory length: 647939   epsilon: 0.009998020008555413    steps: 611    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 2167   score: 14.0   memory length: 648628   epsilon: 0.009998020008555413    steps: 689    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2168   score: 15.0   memory length: 649304   epsilon: 0.009998020008555413    steps: 676    lr: 0.0001     evaluation reward: 13.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2169   score: 13.0   memory length: 649838   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2170   score: 23.0   memory length: 650592   epsilon: 0.009998020008555413    steps: 754    lr: 0.0001     evaluation reward: 13.27\n",
      "episode: 2171   score: 7.0   memory length: 650987   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2172   score: 13.0   memory length: 651464   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2173   score: 15.0   memory length: 652059   epsilon: 0.009998020008555413    steps: 595    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2174   score: 17.0   memory length: 652746   epsilon: 0.009998020008555413    steps: 687    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 2175   score: 13.0   memory length: 653292   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2176   score: 16.0   memory length: 653973   epsilon: 0.009998020008555413    steps: 681    lr: 0.0001     evaluation reward: 13.26\n",
      "episode: 2177   score: 12.0   memory length: 654546   epsilon: 0.009998020008555413    steps: 573    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 2178   score: 10.0   memory length: 655027   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2179   score: 11.0   memory length: 655558   epsilon: 0.009998020008555413    steps: 531    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2180   score: 8.0   memory length: 656012   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2181   score: 12.0   memory length: 656609   epsilon: 0.009998020008555413    steps: 597    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2182   score: 12.0   memory length: 657194   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2183   score: 11.0   memory length: 657765   epsilon: 0.009998020008555413    steps: 571    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2184   score: 20.0   memory length: 658520   epsilon: 0.009998020008555413    steps: 755    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2185   score: 13.0   memory length: 659079   epsilon: 0.009998020008555413    steps: 559    lr: 0.0001     evaluation reward: 13.23\n",
      "episode: 2186   score: 9.0   memory length: 659548   epsilon: 0.009998020008555413    steps: 469    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2187   score: 15.0   memory length: 660206   epsilon: 0.009998020008555413    steps: 658    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2188   score: 17.0   memory length: 660922   epsilon: 0.009998020008555413    steps: 716    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2189   score: 12.0   memory length: 661469   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2190   score: 10.0   memory length: 661989   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2191   score: 21.0   memory length: 662758   epsilon: 0.009998020008555413    steps: 769    lr: 0.0001     evaluation reward: 13.27\n",
      "episode: 2192   score: 23.0   memory length: 663629   epsilon: 0.009998020008555413    steps: 871    lr: 0.0001     evaluation reward: 13.37\n",
      "episode: 2193   score: 19.0   memory length: 664238   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 13.45\n",
      "episode: 2194   score: 6.0   memory length: 664593   epsilon: 0.009998020008555413    steps: 355    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2195   score: 10.0   memory length: 665064   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2196   score: 18.0   memory length: 665773   epsilon: 0.009998020008555413    steps: 709    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2197   score: 13.0   memory length: 666390   epsilon: 0.009998020008555413    steps: 617    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2198   score: 17.0   memory length: 667147   epsilon: 0.009998020008555413    steps: 757    lr: 0.0001     evaluation reward: 13.45\n",
      "episode: 2199   score: 14.0   memory length: 667824   epsilon: 0.009998020008555413    steps: 677    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2200   score: 22.0   memory length: 668502   epsilon: 0.009998020008555413    steps: 678    lr: 0.0001     evaluation reward: 13.55\n",
      "episode: 2201   score: 10.0   memory length: 669010   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2202   score: 16.0   memory length: 669677   epsilon: 0.009998020008555413    steps: 667    lr: 0.0001     evaluation reward: 13.52\n",
      "episode: 2203   score: 23.0   memory length: 670413   epsilon: 0.009998020008555413    steps: 736    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2204   score: 10.0   memory length: 670902   epsilon: 0.009998020008555413    steps: 489    lr: 0.0001     evaluation reward: 13.55\n",
      "episode: 2205   score: 16.0   memory length: 671501   epsilon: 0.009998020008555413    steps: 599    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2206   score: 11.0   memory length: 672067   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2207   score: 21.0   memory length: 672851   epsilon: 0.009998020008555413    steps: 784    lr: 0.0001     evaluation reward: 13.46\n",
      "episode: 2208   score: 9.0   memory length: 673321   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2209   score: 19.0   memory length: 673883   epsilon: 0.009998020008555413    steps: 562    lr: 0.0001     evaluation reward: 13.51\n",
      "episode: 2210   score: 15.0   memory length: 674550   epsilon: 0.009998020008555413    steps: 667    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2211   score: 10.0   memory length: 675072   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 13.43\n",
      "episode: 2212   score: 15.0   memory length: 675750   epsilon: 0.009998020008555413    steps: 678    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 2213   score: 20.0   memory length: 676313   epsilon: 0.009998020008555413    steps: 563    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2214   score: 10.0   memory length: 676773   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 13.51\n",
      "episode: 2215   score: 18.0   memory length: 677384   epsilon: 0.009998020008555413    steps: 611    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2216   score: 10.0   memory length: 677832   epsilon: 0.009998020008555413    steps: 448    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 2217   score: 18.0   memory length: 678578   epsilon: 0.009998020008555413    steps: 746    lr: 0.0001     evaluation reward: 13.52\n",
      "episode: 2218   score: 11.0   memory length: 679154   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2219   score: 16.0   memory length: 679788   epsilon: 0.009998020008555413    steps: 634    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2220   score: 11.0   memory length: 680322   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.5\n",
      "episode: 2221   score: 10.0   memory length: 680855   epsilon: 0.009998020008555413    steps: 533    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2222   score: 15.0   memory length: 681339   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2223   score: 13.0   memory length: 681976   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2224   score: 11.0   memory length: 682459   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2225   score: 12.0   memory length: 683069   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2226   score: 18.0   memory length: 683752   epsilon: 0.009998020008555413    steps: 683    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2227   score: 10.0   memory length: 684292   epsilon: 0.009998020008555413    steps: 540    lr: 0.0001     evaluation reward: 13.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2228   score: 10.0   memory length: 684759   epsilon: 0.009998020008555413    steps: 467    lr: 0.0001     evaluation reward: 13.51\n",
      "episode: 2229   score: 7.0   memory length: 685147   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2230   score: 10.0   memory length: 685689   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2231   score: 12.0   memory length: 686232   epsilon: 0.009998020008555413    steps: 543    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2232   score: 11.0   memory length: 686745   epsilon: 0.009998020008555413    steps: 513    lr: 0.0001     evaluation reward: 13.47\n",
      "episode: 2233   score: 9.0   memory length: 687158   epsilon: 0.009998020008555413    steps: 413    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 2234   score: 13.0   memory length: 687732   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 13.45\n",
      "episode: 2235   score: 21.0   memory length: 688507   epsilon: 0.009998020008555413    steps: 775    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2236   score: 29.0   memory length: 689367   epsilon: 0.009998020008555413    steps: 860    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 2237   score: 16.0   memory length: 690044   epsilon: 0.009998020008555413    steps: 677    lr: 0.0001     evaluation reward: 13.85\n",
      "episode: 2238   score: 24.0   memory length: 690919   epsilon: 0.009998020008555413    steps: 875    lr: 0.0001     evaluation reward: 13.92\n",
      "episode: 2239   score: 15.0   memory length: 691606   epsilon: 0.009998020008555413    steps: 687    lr: 0.0001     evaluation reward: 13.96\n",
      "episode: 2240   score: 17.0   memory length: 692280   epsilon: 0.009998020008555413    steps: 674    lr: 0.0001     evaluation reward: 13.99\n",
      "episode: 2241   score: 12.0   memory length: 692822   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2242   score: 17.0   memory length: 693602   epsilon: 0.009998020008555413    steps: 780    lr: 0.0001     evaluation reward: 13.96\n",
      "episode: 2243   score: 13.0   memory length: 694219   epsilon: 0.009998020008555413    steps: 617    lr: 0.0001     evaluation reward: 13.92\n",
      "episode: 2244   score: 15.0   memory length: 694913   epsilon: 0.009998020008555413    steps: 694    lr: 0.0001     evaluation reward: 13.94\n",
      "episode: 2245   score: 12.0   memory length: 695567   epsilon: 0.009998020008555413    steps: 654    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2246   score: 11.0   memory length: 696091   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2247   score: 10.0   memory length: 696633   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 13.99\n",
      "episode: 2248   score: 14.0   memory length: 697118   epsilon: 0.009998020008555413    steps: 485    lr: 0.0001     evaluation reward: 14.04\n",
      "episode: 2249   score: 20.0   memory length: 697924   epsilon: 0.009998020008555413    steps: 806    lr: 0.0001     evaluation reward: 14.08\n",
      "episode: 2250   score: 18.0   memory length: 698569   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 14.13\n",
      "episode: 2251   score: 22.0   memory length: 699355   epsilon: 0.009998020008555413    steps: 786    lr: 0.0001     evaluation reward: 14.16\n",
      "episode: 2252   score: 17.0   memory length: 699995   epsilon: 0.009998020008555413    steps: 640    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2253   score: 19.0   memory length: 700669   epsilon: 0.009998020008555413    steps: 674    lr: 0.0001     evaluation reward: 14.24\n",
      "episode: 2254   score: 8.0   memory length: 701114   epsilon: 0.009998020008555413    steps: 445    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2255   score: 12.0   memory length: 701710   epsilon: 0.009998020008555413    steps: 596    lr: 0.0001     evaluation reward: 14.22\n",
      "episode: 2256   score: 10.0   memory length: 702197   epsilon: 0.009998020008555413    steps: 487    lr: 0.0001     evaluation reward: 14.16\n",
      "episode: 2257   score: 12.0   memory length: 702754   epsilon: 0.009998020008555413    steps: 557    lr: 0.0001     evaluation reward: 14.18\n",
      "episode: 2258   score: 16.0   memory length: 703419   epsilon: 0.009998020008555413    steps: 665    lr: 0.0001     evaluation reward: 14.17\n",
      "episode: 2259   score: 14.0   memory length: 704023   epsilon: 0.009998020008555413    steps: 604    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2260   score: 18.0   memory length: 704772   epsilon: 0.009998020008555413    steps: 749    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2261   score: 11.0   memory length: 705146   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2262   score: 13.0   memory length: 705736   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 14.26\n",
      "episode: 2263   score: 11.0   memory length: 706275   epsilon: 0.009998020008555413    steps: 539    lr: 0.0001     evaluation reward: 14.2\n",
      "episode: 2264   score: 15.0   memory length: 706873   epsilon: 0.009998020008555413    steps: 598    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2265   score: 14.0   memory length: 707489   epsilon: 0.009998020008555413    steps: 616    lr: 0.0001     evaluation reward: 14.24\n",
      "episode: 2266   score: 15.0   memory length: 708180   epsilon: 0.009998020008555413    steps: 691    lr: 0.0001     evaluation reward: 14.22\n",
      "episode: 2267   score: 15.0   memory length: 708884   epsilon: 0.009998020008555413    steps: 704    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2268   score: 13.0   memory length: 709483   epsilon: 0.009998020008555413    steps: 599    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2269   score: 15.0   memory length: 710057   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2270   score: 15.0   memory length: 710746   epsilon: 0.009998020008555413    steps: 689    lr: 0.0001     evaluation reward: 14.15\n",
      "episode: 2271   score: 12.0   memory length: 711347   epsilon: 0.009998020008555413    steps: 601    lr: 0.0001     evaluation reward: 14.2\n",
      "episode: 2272   score: 26.0   memory length: 712130   epsilon: 0.009998020008555413    steps: 783    lr: 0.0001     evaluation reward: 14.33\n",
      "episode: 2273   score: 9.0   memory length: 712633   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2274   score: 11.0   memory length: 713171   epsilon: 0.009998020008555413    steps: 538    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2275   score: 33.0   memory length: 714219   epsilon: 0.009998020008555413    steps: 1048    lr: 0.0001     evaluation reward: 14.41\n",
      "episode: 2276   score: 10.0   memory length: 714759   epsilon: 0.009998020008555413    steps: 540    lr: 0.0001     evaluation reward: 14.35\n",
      "episode: 2277   score: 9.0   memory length: 715243   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 14.32\n",
      "episode: 2278   score: 22.0   memory length: 716030   epsilon: 0.009998020008555413    steps: 787    lr: 0.0001     evaluation reward: 14.44\n",
      "episode: 2279   score: 14.0   memory length: 716700   epsilon: 0.009998020008555413    steps: 670    lr: 0.0001     evaluation reward: 14.47\n",
      "episode: 2280   score: 27.0   memory length: 717455   epsilon: 0.009998020008555413    steps: 755    lr: 0.0001     evaluation reward: 14.66\n",
      "episode: 2281   score: 11.0   memory length: 717955   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 14.65\n",
      "episode: 2282   score: 9.0   memory length: 718382   epsilon: 0.009998020008555413    steps: 427    lr: 0.0001     evaluation reward: 14.62\n",
      "episode: 2283   score: 14.0   memory length: 719033   epsilon: 0.009998020008555413    steps: 651    lr: 0.0001     evaluation reward: 14.65\n",
      "episode: 2284   score: 15.0   memory length: 719690   epsilon: 0.009998020008555413    steps: 657    lr: 0.0001     evaluation reward: 14.6\n",
      "episode: 2285   score: 20.0   memory length: 720344   epsilon: 0.009998020008555413    steps: 654    lr: 0.0001     evaluation reward: 14.67\n",
      "episode: 2286   score: 13.0   memory length: 720899   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 14.71\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2287   score: 12.0   memory length: 721491   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 14.68\n",
      "episode: 2288   score: 6.0   memory length: 721831   epsilon: 0.009998020008555413    steps: 340    lr: 0.0001     evaluation reward: 14.57\n",
      "episode: 2289   score: 13.0   memory length: 722327   epsilon: 0.009998020008555413    steps: 496    lr: 0.0001     evaluation reward: 14.58\n",
      "episode: 2290   score: 14.0   memory length: 722883   epsilon: 0.009998020008555413    steps: 556    lr: 0.0001     evaluation reward: 14.62\n",
      "episode: 2291   score: 15.0   memory length: 723516   epsilon: 0.009998020008555413    steps: 633    lr: 0.0001     evaluation reward: 14.56\n",
      "episode: 2292   score: 10.0   memory length: 724022   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 14.43\n",
      "episode: 2293   score: 16.0   memory length: 724554   epsilon: 0.009998020008555413    steps: 532    lr: 0.0001     evaluation reward: 14.4\n",
      "episode: 2294   score: 13.0   memory length: 725147   epsilon: 0.009998020008555413    steps: 593    lr: 0.0001     evaluation reward: 14.47\n",
      "episode: 2295   score: 14.0   memory length: 725745   epsilon: 0.009998020008555413    steps: 598    lr: 0.0001     evaluation reward: 14.51\n",
      "episode: 2296   score: 29.0   memory length: 726593   epsilon: 0.009998020008555413    steps: 848    lr: 0.0001     evaluation reward: 14.62\n",
      "episode: 2297   score: 14.0   memory length: 727080   epsilon: 0.009998020008555413    steps: 487    lr: 0.0001     evaluation reward: 14.63\n",
      "episode: 2298   score: 11.0   memory length: 727652   epsilon: 0.009998020008555413    steps: 572    lr: 0.0001     evaluation reward: 14.57\n",
      "episode: 2299   score: 12.0   memory length: 728239   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 14.55\n",
      "episode: 2300   score: 8.0   memory length: 728693   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 14.41\n",
      "episode: 2301   score: 15.0   memory length: 729404   epsilon: 0.009998020008555413    steps: 711    lr: 0.0001     evaluation reward: 14.46\n",
      "episode: 2302   score: 11.0   memory length: 729948   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 14.41\n",
      "episode: 2303   score: 14.0   memory length: 730624   epsilon: 0.009998020008555413    steps: 676    lr: 0.0001     evaluation reward: 14.32\n",
      "episode: 2304   score: 24.0   memory length: 731350   epsilon: 0.009998020008555413    steps: 726    lr: 0.0001     evaluation reward: 14.46\n",
      "episode: 2305   score: 15.0   memory length: 731864   epsilon: 0.009998020008555413    steps: 514    lr: 0.0001     evaluation reward: 14.45\n",
      "episode: 2306   score: 11.0   memory length: 732346   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 14.45\n",
      "episode: 2307   score: 10.0   memory length: 732871   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 14.34\n",
      "episode: 2308   score: 12.0   memory length: 733400   epsilon: 0.009998020008555413    steps: 529    lr: 0.0001     evaluation reward: 14.37\n",
      "episode: 2309   score: 7.0   memory length: 733828   epsilon: 0.009998020008555413    steps: 428    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2310   score: 16.0   memory length: 734537   epsilon: 0.009998020008555413    steps: 709    lr: 0.0001     evaluation reward: 14.26\n",
      "episode: 2311   score: 18.0   memory length: 735216   epsilon: 0.009998020008555413    steps: 679    lr: 0.0001     evaluation reward: 14.34\n",
      "episode: 2312   score: 17.0   memory length: 735882   epsilon: 0.009998020008555413    steps: 666    lr: 0.0001     evaluation reward: 14.36\n",
      "episode: 2313   score: 14.0   memory length: 736543   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 14.3\n",
      "episode: 2314   score: 12.0   memory length: 737138   epsilon: 0.009998020008555413    steps: 595    lr: 0.0001     evaluation reward: 14.32\n",
      "episode: 2315   score: 18.0   memory length: 737776   epsilon: 0.009998020008555413    steps: 638    lr: 0.0001     evaluation reward: 14.32\n",
      "episode: 2316   score: 6.0   memory length: 738129   epsilon: 0.009998020008555413    steps: 353    lr: 0.0001     evaluation reward: 14.28\n",
      "episode: 2317   score: 9.0   memory length: 738634   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2318   score: 11.0   memory length: 739188   epsilon: 0.009998020008555413    steps: 554    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2319   score: 21.0   memory length: 739951   epsilon: 0.009998020008555413    steps: 763    lr: 0.0001     evaluation reward: 14.24\n",
      "episode: 2320   score: 10.0   memory length: 740441   epsilon: 0.009998020008555413    steps: 490    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2321   score: 7.0   memory length: 740829   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 14.2\n",
      "episode: 2322   score: 17.0   memory length: 741451   epsilon: 0.009998020008555413    steps: 622    lr: 0.0001     evaluation reward: 14.22\n",
      "episode: 2323   score: 23.0   memory length: 742178   epsilon: 0.009998020008555413    steps: 727    lr: 0.0001     evaluation reward: 14.32\n",
      "episode: 2324   score: 6.0   memory length: 742534   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2325   score: 11.0   memory length: 743062   epsilon: 0.009998020008555413    steps: 528    lr: 0.0001     evaluation reward: 14.26\n",
      "episode: 2326   score: 9.0   memory length: 743520   epsilon: 0.009998020008555413    steps: 458    lr: 0.0001     evaluation reward: 14.17\n",
      "episode: 2327   score: 8.0   memory length: 743963   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 14.15\n",
      "episode: 2328   score: 14.0   memory length: 744559   epsilon: 0.009998020008555413    steps: 596    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2329   score: 4.0   memory length: 744819   epsilon: 0.009998020008555413    steps: 260    lr: 0.0001     evaluation reward: 14.16\n",
      "episode: 2330   score: 21.0   memory length: 745532   epsilon: 0.009998020008555413    steps: 713    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2331   score: 15.0   memory length: 746131   epsilon: 0.009998020008555413    steps: 599    lr: 0.0001     evaluation reward: 14.3\n",
      "episode: 2332   score: 7.0   memory length: 746518   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 14.26\n",
      "episode: 2333   score: 6.0   memory length: 746896   epsilon: 0.009998020008555413    steps: 378    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2334   score: 7.0   memory length: 747303   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 14.17\n",
      "episode: 2335   score: 9.0   memory length: 747783   epsilon: 0.009998020008555413    steps: 480    lr: 0.0001     evaluation reward: 14.05\n",
      "episode: 2336   score: 18.0   memory length: 748389   epsilon: 0.009998020008555413    steps: 606    lr: 0.0001     evaluation reward: 13.94\n",
      "episode: 2337   score: 16.0   memory length: 749034   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 13.94\n",
      "episode: 2338   score: 12.0   memory length: 749636   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 13.82\n",
      "episode: 2339   score: 18.0   memory length: 750194   epsilon: 0.009998020008555413    steps: 558    lr: 0.0001     evaluation reward: 13.85\n",
      "episode: 2340   score: 16.0   memory length: 750915   epsilon: 0.009998020008555413    steps: 721    lr: 0.0001     evaluation reward: 13.84\n",
      "episode: 2341   score: 13.0   memory length: 751399   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 13.85\n",
      "episode: 2342   score: 19.0   memory length: 752073   epsilon: 0.009998020008555413    steps: 674    lr: 0.0001     evaluation reward: 13.87\n",
      "episode: 2343   score: 14.0   memory length: 752706   epsilon: 0.009998020008555413    steps: 633    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2344   score: 15.0   memory length: 753211   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2345   score: 10.0   memory length: 753561   epsilon: 0.009998020008555413    steps: 350    lr: 0.0001     evaluation reward: 13.86\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2346   score: 10.0   memory length: 754029   epsilon: 0.009998020008555413    steps: 468    lr: 0.0001     evaluation reward: 13.85\n",
      "episode: 2347   score: 11.0   memory length: 754558   epsilon: 0.009998020008555413    steps: 529    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2348   score: 15.0   memory length: 755216   epsilon: 0.009998020008555413    steps: 658    lr: 0.0001     evaluation reward: 13.87\n",
      "episode: 2349   score: 11.0   memory length: 755741   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2350   score: 12.0   memory length: 756263   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2351   score: 23.0   memory length: 756965   epsilon: 0.009998020008555413    steps: 702    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2352   score: 14.0   memory length: 757590   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2353   score: 13.0   memory length: 758175   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2354   score: 17.0   memory length: 758897   epsilon: 0.009998020008555413    steps: 722    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2355   score: 10.0   memory length: 759448   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2356   score: 9.0   memory length: 759906   epsilon: 0.009998020008555413    steps: 458    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2357   score: 17.0   memory length: 760552   epsilon: 0.009998020008555413    steps: 646    lr: 0.0001     evaluation reward: 13.75\n",
      "episode: 2358   score: 8.0   memory length: 760986   epsilon: 0.009998020008555413    steps: 434    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2359   score: 11.0   memory length: 761542   epsilon: 0.009998020008555413    steps: 556    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2360   score: 10.0   memory length: 762101   epsilon: 0.009998020008555413    steps: 559    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2361   score: 7.0   memory length: 762484   epsilon: 0.009998020008555413    steps: 383    lr: 0.0001     evaluation reward: 13.52\n",
      "episode: 2362   score: 10.0   memory length: 763026   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2363   score: 11.0   memory length: 763599   epsilon: 0.009998020008555413    steps: 573    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2364   score: 12.0   memory length: 764090   epsilon: 0.009998020008555413    steps: 491    lr: 0.0001     evaluation reward: 13.46\n",
      "episode: 2365   score: 11.0   memory length: 764625   epsilon: 0.009998020008555413    steps: 535    lr: 0.0001     evaluation reward: 13.43\n",
      "episode: 2366   score: 13.0   memory length: 765070   epsilon: 0.009998020008555413    steps: 445    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2367   score: 4.0   memory length: 765365   epsilon: 0.009998020008555413    steps: 295    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2368   score: 24.0   memory length: 766085   epsilon: 0.009998020008555413    steps: 720    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2369   score: 11.0   memory length: 766652   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 13.37\n",
      "episode: 2370   score: 7.0   memory length: 767045   epsilon: 0.009998020008555413    steps: 393    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 2371   score: 8.0   memory length: 767411   epsilon: 0.009998020008555413    steps: 366    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 2372   score: 11.0   memory length: 767948   epsilon: 0.009998020008555413    steps: 537    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2373   score: 17.0   memory length: 768524   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2374   score: 15.0   memory length: 769216   epsilon: 0.009998020008555413    steps: 692    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2375   score: 14.0   memory length: 769767   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2376   score: 15.0   memory length: 770313   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2377   score: 16.0   memory length: 770925   epsilon: 0.009998020008555413    steps: 612    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2378   score: 10.0   memory length: 771443   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2379   score: 14.0   memory length: 772053   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2380   score: 12.0   memory length: 772618   epsilon: 0.009998020008555413    steps: 565    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 2381   score: 7.0   memory length: 773024   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 2382   score: 13.0   memory length: 773644   epsilon: 0.009998020008555413    steps: 620    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 2383   score: 7.0   memory length: 774036   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2384   score: 17.0   memory length: 774542   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 2385   score: 13.0   memory length: 775001   epsilon: 0.009998020008555413    steps: 459    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2386   score: 10.0   memory length: 775502   epsilon: 0.009998020008555413    steps: 501    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 2387   score: 11.0   memory length: 776079   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2388   score: 5.0   memory length: 776347   epsilon: 0.009998020008555413    steps: 268    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2389   score: 19.0   memory length: 777054   epsilon: 0.009998020008555413    steps: 707    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 2390   score: 18.0   memory length: 777723   epsilon: 0.009998020008555413    steps: 669    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2391   score: 9.0   memory length: 778170   epsilon: 0.009998020008555413    steps: 447    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2392   score: 11.0   memory length: 778702   epsilon: 0.009998020008555413    steps: 532    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2393   score: 14.0   memory length: 779238   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2394   score: 18.0   memory length: 779880   epsilon: 0.009998020008555413    steps: 642    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 2395   score: 24.0   memory length: 780714   epsilon: 0.009998020008555413    steps: 834    lr: 0.0001     evaluation reward: 12.89\n",
      "episode: 2396   score: 7.0   memory length: 781140   epsilon: 0.009998020008555413    steps: 426    lr: 0.0001     evaluation reward: 12.67\n",
      "episode: 2397   score: 16.0   memory length: 781864   epsilon: 0.009998020008555413    steps: 724    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 2398   score: 21.0   memory length: 782644   epsilon: 0.009998020008555413    steps: 780    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 2399   score: 13.0   memory length: 783230   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2400   score: 8.0   memory length: 783656   epsilon: 0.009998020008555413    steps: 426    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2401   score: 16.0   memory length: 784230   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2402   score: 7.0   memory length: 784632   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 2403   score: 16.0   memory length: 785231   epsilon: 0.009998020008555413    steps: 599    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 2404   score: 12.0   memory length: 785786   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 12.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2405   score: 9.0   memory length: 786273   epsilon: 0.009998020008555413    steps: 487    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2406   score: 9.0   memory length: 786749   epsilon: 0.009998020008555413    steps: 476    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2407   score: 17.0   memory length: 787399   epsilon: 0.009998020008555413    steps: 650    lr: 0.0001     evaluation reward: 12.66\n",
      "episode: 2408   score: 15.0   memory length: 788012   epsilon: 0.009998020008555413    steps: 613    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 2409   score: 13.0   memory length: 788627   epsilon: 0.009998020008555413    steps: 615    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2410   score: 15.0   memory length: 789266   epsilon: 0.009998020008555413    steps: 639    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2411   score: 9.0   memory length: 789743   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2412   score: 11.0   memory length: 790277   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2413   score: 11.0   memory length: 790820   epsilon: 0.009998020008555413    steps: 543    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2414   score: 11.0   memory length: 791406   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2415   score: 13.0   memory length: 791882   epsilon: 0.009998020008555413    steps: 476    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2416   score: 15.0   memory length: 792443   epsilon: 0.009998020008555413    steps: 561    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2417   score: 12.0   memory length: 792995   epsilon: 0.009998020008555413    steps: 552    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2418   score: 24.0   memory length: 793675   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2419   score: 15.0   memory length: 794336   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 2420   score: 9.0   memory length: 794791   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2421   score: 17.0   memory length: 795327   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2422   score: 11.0   memory length: 795863   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 2423   score: 11.0   memory length: 796440   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2424   score: 12.0   memory length: 796945   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 12.66\n",
      "episode: 2425   score: 21.0   memory length: 797644   epsilon: 0.009998020008555413    steps: 699    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2426   score: 17.0   memory length: 798130   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 2427   score: 24.0   memory length: 798825   epsilon: 0.009998020008555413    steps: 695    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 2428   score: 15.0   memory length: 799375   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2429   score: 18.0   memory length: 799999   epsilon: 0.009998020008555413    steps: 624    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2430   score: 11.0   memory length: 800554   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2431   score: 13.0   memory length: 801152   epsilon: 0.009998020008555413    steps: 598    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2432   score: 17.0   memory length: 801899   epsilon: 0.009998020008555413    steps: 747    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2433   score: 13.0   memory length: 802527   epsilon: 0.009998020008555413    steps: 628    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2434   score: 15.0   memory length: 803119   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 13.28\n",
      "episode: 2435   score: 11.0   memory length: 803642   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2436   score: 6.0   memory length: 803947   epsilon: 0.009998020008555413    steps: 305    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2437   score: 12.0   memory length: 804531   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2438   score: 11.0   memory length: 805084   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2439   score: 26.0   memory length: 805900   epsilon: 0.009998020008555413    steps: 816    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2440   score: 10.0   memory length: 806420   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2441   score: 4.0   memory length: 806720   epsilon: 0.009998020008555413    steps: 300    lr: 0.0001     evaluation reward: 13.06\n",
      "episode: 2442   score: 17.0   memory length: 807307   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 13.04\n",
      "episode: 2443   score: 21.0   memory length: 807980   epsilon: 0.009998020008555413    steps: 673    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 2444   score: 24.0   memory length: 808993   epsilon: 0.009998020008555413    steps: 1013    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2445   score: 10.0   memory length: 809525   epsilon: 0.009998020008555413    steps: 532    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2446   score: 7.0   memory length: 809929   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2447   score: 6.0   memory length: 810271   epsilon: 0.009998020008555413    steps: 342    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2448   score: 17.0   memory length: 811022   epsilon: 0.009998020008555413    steps: 751    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2449   score: 20.0   memory length: 811727   epsilon: 0.009998020008555413    steps: 705    lr: 0.0001     evaluation reward: 13.23\n",
      "episode: 2450   score: 13.0   memory length: 812351   epsilon: 0.009998020008555413    steps: 624    lr: 0.0001     evaluation reward: 13.24\n",
      "episode: 2451   score: 8.0   memory length: 812813   epsilon: 0.009998020008555413    steps: 462    lr: 0.0001     evaluation reward: 13.09\n",
      "episode: 2452   score: 11.0   memory length: 813318   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 13.06\n",
      "episode: 2453   score: 14.0   memory length: 813961   epsilon: 0.009998020008555413    steps: 643    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2454   score: 12.0   memory length: 814498   epsilon: 0.009998020008555413    steps: 537    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2455   score: 16.0   memory length: 815191   epsilon: 0.009998020008555413    steps: 693    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2456   score: 30.0   memory length: 815959   epsilon: 0.009998020008555413    steps: 768    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 2457   score: 23.0   memory length: 816708   epsilon: 0.009998020008555413    steps: 749    lr: 0.0001     evaluation reward: 13.35\n",
      "episode: 2458   score: 11.0   memory length: 817227   epsilon: 0.009998020008555413    steps: 519    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 2459   score: 10.0   memory length: 817721   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 13.37\n",
      "episode: 2460   score: 12.0   memory length: 818271   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2461   score: 17.0   memory length: 818937   epsilon: 0.009998020008555413    steps: 666    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2462   score: 14.0   memory length: 819503   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2463   score: 9.0   memory length: 819963   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 13.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2464   score: 11.0   memory length: 820494   epsilon: 0.009998020008555413    steps: 531    lr: 0.0001     evaluation reward: 13.5\n",
      "episode: 2465   score: 21.0   memory length: 821209   epsilon: 0.009998020008555413    steps: 715    lr: 0.0001     evaluation reward: 13.6\n",
      "episode: 2466   score: 11.0   memory length: 821786   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2467   score: 15.0   memory length: 822398   epsilon: 0.009998020008555413    steps: 612    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 2468   score: 11.0   memory length: 822921   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2469   score: 10.0   memory length: 823410   epsilon: 0.009998020008555413    steps: 489    lr: 0.0001     evaluation reward: 13.55\n",
      "episode: 2470   score: 10.0   memory length: 823879   epsilon: 0.009998020008555413    steps: 469    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2471   score: 12.0   memory length: 824496   epsilon: 0.009998020008555413    steps: 617    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2472   score: 14.0   memory length: 824990   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2473   score: 25.0   memory length: 825747   epsilon: 0.009998020008555413    steps: 757    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2474   score: 13.0   memory length: 826303   epsilon: 0.009998020008555413    steps: 556    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2475   score: 8.0   memory length: 826740   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2476   score: 9.0   memory length: 827244   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2477   score: 9.0   memory length: 827699   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 13.52\n",
      "episode: 2478   score: 17.0   memory length: 828363   epsilon: 0.009998020008555413    steps: 664    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2479   score: 16.0   memory length: 829026   epsilon: 0.009998020008555413    steps: 663    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2480   score: 15.0   memory length: 829542   epsilon: 0.009998020008555413    steps: 516    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2481   score: 22.0   memory length: 830283   epsilon: 0.009998020008555413    steps: 741    lr: 0.0001     evaluation reward: 13.79\n",
      "episode: 2482   score: 12.0   memory length: 830830   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2483   score: 26.0   memory length: 831582   epsilon: 0.009998020008555413    steps: 752    lr: 0.0001     evaluation reward: 13.97\n",
      "episode: 2484   score: 21.0   memory length: 832362   epsilon: 0.009998020008555413    steps: 780    lr: 0.0001     evaluation reward: 14.01\n",
      "episode: 2485   score: 17.0   memory length: 833048   epsilon: 0.009998020008555413    steps: 686    lr: 0.0001     evaluation reward: 14.05\n",
      "episode: 2486   score: 25.0   memory length: 833731   epsilon: 0.009998020008555413    steps: 683    lr: 0.0001     evaluation reward: 14.2\n",
      "episode: 2487   score: 16.0   memory length: 834420   epsilon: 0.009998020008555413    steps: 689    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2488   score: 13.0   memory length: 835103   epsilon: 0.009998020008555413    steps: 683    lr: 0.0001     evaluation reward: 14.33\n",
      "episode: 2489   score: 11.0   memory length: 835614   epsilon: 0.009998020008555413    steps: 511    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2490   score: 19.0   memory length: 836298   epsilon: 0.009998020008555413    steps: 684    lr: 0.0001     evaluation reward: 14.26\n",
      "episode: 2491   score: 8.0   memory length: 836704   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2492   score: 13.0   memory length: 837291   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2493   score: 8.0   memory length: 837768   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 14.21\n",
      "episode: 2494   score: 7.0   memory length: 838194   epsilon: 0.009998020008555413    steps: 426    lr: 0.0001     evaluation reward: 14.1\n",
      "episode: 2495   score: 9.0   memory length: 838650   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 13.95\n",
      "episode: 2496   score: 10.0   memory length: 839154   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2497   score: 19.0   memory length: 839883   epsilon: 0.009998020008555413    steps: 729    lr: 0.0001     evaluation reward: 14.01\n",
      "episode: 2498   score: 8.0   memory length: 840336   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2499   score: 13.0   memory length: 840937   epsilon: 0.009998020008555413    steps: 601    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2500   score: 6.0   memory length: 841296   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2501   score: 21.0   memory length: 841934   epsilon: 0.009998020008555413    steps: 638    lr: 0.0001     evaluation reward: 13.91\n",
      "episode: 2502   score: 6.0   memory length: 842258   epsilon: 0.009998020008555413    steps: 324    lr: 0.0001     evaluation reward: 13.9\n",
      "episode: 2503   score: 7.0   memory length: 842638   epsilon: 0.009998020008555413    steps: 380    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2504   score: 23.0   memory length: 843380   epsilon: 0.009998020008555413    steps: 742    lr: 0.0001     evaluation reward: 13.92\n",
      "episode: 2505   score: 15.0   memory length: 843949   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2506   score: 10.0   memory length: 844418   epsilon: 0.009998020008555413    steps: 469    lr: 0.0001     evaluation reward: 13.99\n",
      "episode: 2507   score: 17.0   memory length: 845177   epsilon: 0.009998020008555413    steps: 759    lr: 0.0001     evaluation reward: 13.99\n",
      "episode: 2508   score: 9.0   memory length: 845592   epsilon: 0.009998020008555413    steps: 415    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2509   score: 11.0   memory length: 846116   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 13.91\n",
      "episode: 2510   score: 16.0   memory length: 846831   epsilon: 0.009998020008555413    steps: 715    lr: 0.0001     evaluation reward: 13.92\n",
      "episode: 2511   score: 5.0   memory length: 847142   epsilon: 0.009998020008555413    steps: 311    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2512   score: 5.0   memory length: 847469   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 13.82\n",
      "episode: 2513   score: 14.0   memory length: 848035   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 13.85\n",
      "episode: 2514   score: 7.0   memory length: 848462   epsilon: 0.009998020008555413    steps: 427    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2515   score: 9.0   memory length: 848954   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 13.77\n",
      "episode: 2516   score: 12.0   memory length: 849547   epsilon: 0.009998020008555413    steps: 593    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 2517   score: 21.0   memory length: 850329   epsilon: 0.009998020008555413    steps: 782    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2518   score: 14.0   memory length: 850905   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2519   score: 13.0   memory length: 851460   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2520   score: 10.0   memory length: 851977   epsilon: 0.009998020008555413    steps: 517    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2521   score: 6.0   memory length: 852312   epsilon: 0.009998020008555413    steps: 335    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2522   score: 14.0   memory length: 852928   epsilon: 0.009998020008555413    steps: 616    lr: 0.0001     evaluation reward: 13.64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2523   score: 6.0   memory length: 853307   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2524   score: 6.0   memory length: 853664   epsilon: 0.009998020008555413    steps: 357    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2525   score: 12.0   memory length: 854274   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2526   score: 7.0   memory length: 854700   epsilon: 0.009998020008555413    steps: 426    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2527   score: 7.0   memory length: 855064   epsilon: 0.009998020008555413    steps: 364    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2528   score: 7.0   memory length: 855452   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 13.09\n",
      "episode: 2529   score: 10.0   memory length: 856010   epsilon: 0.009998020008555413    steps: 558    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2530   score: 12.0   memory length: 856544   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2531   score: 5.0   memory length: 856888   epsilon: 0.009998020008555413    steps: 344    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 2532   score: 4.0   memory length: 857149   epsilon: 0.009998020008555413    steps: 261    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2533   score: 8.0   memory length: 857582   epsilon: 0.009998020008555413    steps: 433    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2534   score: 15.0   memory length: 858187   epsilon: 0.009998020008555413    steps: 605    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2535   score: 8.0   memory length: 858640   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 2536   score: 19.0   memory length: 859284   epsilon: 0.009998020008555413    steps: 644    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 2537   score: 11.0   memory length: 859850   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2538   score: 7.0   memory length: 860238   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 2539   score: 13.0   memory length: 860717   epsilon: 0.009998020008555413    steps: 479    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 2540   score: 13.0   memory length: 861324   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 12.71\n",
      "episode: 2541   score: 8.0   memory length: 861719   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 2542   score: 5.0   memory length: 862012   epsilon: 0.009998020008555413    steps: 293    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2543   score: 15.0   memory length: 862671   epsilon: 0.009998020008555413    steps: 659    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2544   score: 9.0   memory length: 863090   epsilon: 0.009998020008555413    steps: 419    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2545   score: 11.0   memory length: 863626   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2546   score: 13.0   memory length: 864252   epsilon: 0.009998020008555413    steps: 626    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 2547   score: 12.0   memory length: 864714   epsilon: 0.009998020008555413    steps: 462    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2548   score: 13.0   memory length: 865324   epsilon: 0.009998020008555413    steps: 610    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2549   score: 13.0   memory length: 865961   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2550   score: 11.0   memory length: 866467   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2551   score: 16.0   memory length: 867153   epsilon: 0.009998020008555413    steps: 686    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2552   score: 17.0   memory length: 867808   epsilon: 0.009998020008555413    steps: 655    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2553   score: 13.0   memory length: 868390   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2554   score: 10.0   memory length: 868872   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2555   score: 6.0   memory length: 869207   epsilon: 0.009998020008555413    steps: 335    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2556   score: 13.0   memory length: 869811   epsilon: 0.009998020008555413    steps: 604    lr: 0.0001     evaluation reward: 12.26\n",
      "episode: 2557   score: 18.0   memory length: 870560   epsilon: 0.009998020008555413    steps: 749    lr: 0.0001     evaluation reward: 12.21\n",
      "episode: 2558   score: 12.0   memory length: 871129   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 12.22\n",
      "episode: 2559   score: 13.0   memory length: 871742   epsilon: 0.009998020008555413    steps: 613    lr: 0.0001     evaluation reward: 12.25\n",
      "episode: 2560   score: 12.0   memory length: 872288   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 12.25\n",
      "episode: 2561   score: 8.0   memory length: 872710   epsilon: 0.009998020008555413    steps: 422    lr: 0.0001     evaluation reward: 12.16\n",
      "episode: 2562   score: 30.0   memory length: 873446   epsilon: 0.009998020008555413    steps: 736    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2563   score: 11.0   memory length: 874030   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2564   score: 9.0   memory length: 874554   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 2565   score: 18.0   memory length: 875242   epsilon: 0.009998020008555413    steps: 688    lr: 0.0001     evaluation reward: 12.29\n",
      "episode: 2566   score: 7.0   memory length: 875623   epsilon: 0.009998020008555413    steps: 381    lr: 0.0001     evaluation reward: 12.25\n",
      "episode: 2567   score: 5.0   memory length: 875950   epsilon: 0.009998020008555413    steps: 327    lr: 0.0001     evaluation reward: 12.15\n",
      "episode: 2568   score: 10.0   memory length: 876431   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 2569   score: 23.0   memory length: 877223   epsilon: 0.009998020008555413    steps: 792    lr: 0.0001     evaluation reward: 12.27\n",
      "episode: 2570   score: 14.0   memory length: 877888   epsilon: 0.009998020008555413    steps: 665    lr: 0.0001     evaluation reward: 12.31\n",
      "episode: 2571   score: 17.0   memory length: 878379   epsilon: 0.009998020008555413    steps: 491    lr: 0.0001     evaluation reward: 12.36\n",
      "episode: 2572   score: 12.0   memory length: 878889   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2573   score: 9.0   memory length: 879331   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 2574   score: 20.0   memory length: 879937   epsilon: 0.009998020008555413    steps: 606    lr: 0.0001     evaluation reward: 12.25\n",
      "episode: 2575   score: 9.0   memory length: 880404   epsilon: 0.009998020008555413    steps: 467    lr: 0.0001     evaluation reward: 12.26\n",
      "episode: 2576   score: 21.0   memory length: 880927   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 2577   score: 14.0   memory length: 881561   epsilon: 0.009998020008555413    steps: 634    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2578   score: 14.0   memory length: 882103   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2579   score: 7.0   memory length: 882513   epsilon: 0.009998020008555413    steps: 410    lr: 0.0001     evaluation reward: 12.31\n",
      "episode: 2580   score: 7.0   memory length: 882881   epsilon: 0.009998020008555413    steps: 368    lr: 0.0001     evaluation reward: 12.23\n",
      "episode: 2581   score: 20.0   memory length: 883513   epsilon: 0.009998020008555413    steps: 632    lr: 0.0001     evaluation reward: 12.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2582   score: 5.0   memory length: 883803   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 2583   score: 18.0   memory length: 884421   epsilon: 0.009998020008555413    steps: 618    lr: 0.0001     evaluation reward: 12.06\n",
      "episode: 2584   score: 15.0   memory length: 884996   epsilon: 0.009998020008555413    steps: 575    lr: 0.0001     evaluation reward: 12.0\n",
      "episode: 2585   score: 22.0   memory length: 885592   epsilon: 0.009998020008555413    steps: 596    lr: 0.0001     evaluation reward: 12.05\n",
      "episode: 2586   score: 19.0   memory length: 886136   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 11.99\n",
      "episode: 2587   score: 12.0   memory length: 886649   epsilon: 0.009998020008555413    steps: 513    lr: 0.0001     evaluation reward: 11.95\n",
      "episode: 2588   score: 13.0   memory length: 887241   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 11.95\n",
      "episode: 2589   score: 19.0   memory length: 887807   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 12.03\n",
      "episode: 2590   score: 14.0   memory length: 888425   epsilon: 0.009998020008555413    steps: 618    lr: 0.0001     evaluation reward: 11.98\n",
      "episode: 2591   score: 13.0   memory length: 888991   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 12.03\n",
      "episode: 2592   score: 24.0   memory length: 889807   epsilon: 0.009998020008555413    steps: 816    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 2593   score: 10.0   memory length: 890311   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.16\n",
      "episode: 2594   score: 10.0   memory length: 890811   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 12.19\n",
      "episode: 2595   score: 11.0   memory length: 891348   epsilon: 0.009998020008555413    steps: 537    lr: 0.0001     evaluation reward: 12.21\n",
      "episode: 2596   score: 11.0   memory length: 891854   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 12.22\n",
      "episode: 2597   score: 10.0   memory length: 892293   epsilon: 0.009998020008555413    steps: 439    lr: 0.0001     evaluation reward: 12.13\n",
      "episode: 2598   score: 16.0   memory length: 892707   epsilon: 0.009998020008555413    steps: 414    lr: 0.0001     evaluation reward: 12.21\n",
      "episode: 2599   score: 12.0   memory length: 893268   epsilon: 0.009998020008555413    steps: 561    lr: 0.0001     evaluation reward: 12.2\n",
      "episode: 2600   score: 14.0   memory length: 893839   epsilon: 0.009998020008555413    steps: 571    lr: 0.0001     evaluation reward: 12.28\n",
      "episode: 2601   score: 10.0   memory length: 894343   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.17\n",
      "episode: 2602   score: 7.0   memory length: 894730   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 2603   score: 15.0   memory length: 895315   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 12.26\n",
      "episode: 2604   score: 12.0   memory length: 895917   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 12.15\n",
      "episode: 2605   score: 8.0   memory length: 896331   epsilon: 0.009998020008555413    steps: 414    lr: 0.0001     evaluation reward: 12.08\n",
      "episode: 2606   score: 11.0   memory length: 896858   epsilon: 0.009998020008555413    steps: 527    lr: 0.0001     evaluation reward: 12.09\n",
      "episode: 2607   score: 18.0   memory length: 897411   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 12.1\n",
      "episode: 2608   score: 18.0   memory length: 898061   epsilon: 0.009998020008555413    steps: 650    lr: 0.0001     evaluation reward: 12.19\n",
      "episode: 2609   score: 16.0   memory length: 898703   epsilon: 0.009998020008555413    steps: 642    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 2610   score: 12.0   memory length: 899221   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 12.2\n",
      "episode: 2611   score: 18.0   memory length: 899947   epsilon: 0.009998020008555413    steps: 726    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 2612   score: 13.0   memory length: 900572   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2613   score: 26.0   memory length: 901289   epsilon: 0.009998020008555413    steps: 717    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 2614   score: 10.0   memory length: 901787   epsilon: 0.009998020008555413    steps: 498    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2615   score: 9.0   memory length: 902240   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 2616   score: 17.0   memory length: 902881   epsilon: 0.009998020008555413    steps: 641    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2617   score: 10.0   memory length: 903394   epsilon: 0.009998020008555413    steps: 513    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2618   score: 14.0   memory length: 903902   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2619   score: 7.0   memory length: 904264   epsilon: 0.009998020008555413    steps: 362    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2620   score: 7.0   memory length: 904652   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2621   score: 6.0   memory length: 905006   epsilon: 0.009998020008555413    steps: 354    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2622   score: 7.0   memory length: 905398   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2623   score: 16.0   memory length: 906023   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2624   score: 7.0   memory length: 906408   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2625   score: 27.0   memory length: 907186   epsilon: 0.009998020008555413    steps: 778    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 2626   score: 11.0   memory length: 907598   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2627   score: 12.0   memory length: 908045   epsilon: 0.009998020008555413    steps: 447    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 2628   score: 11.0   memory length: 908487   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 2629   score: 15.0   memory length: 909092   epsilon: 0.009998020008555413    steps: 605    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 2630   score: 10.0   memory length: 909450   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2631   score: 14.0   memory length: 909930   epsilon: 0.009998020008555413    steps: 480    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2632   score: 11.0   memory length: 910466   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2633   score: 15.0   memory length: 911036   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 2634   score: 12.0   memory length: 911551   epsilon: 0.009998020008555413    steps: 515    lr: 0.0001     evaluation reward: 12.96\n",
      "episode: 2635   score: 13.0   memory length: 912069   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2636   score: 20.0   memory length: 912757   epsilon: 0.009998020008555413    steps: 688    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2637   score: 10.0   memory length: 913279   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 2638   score: 16.0   memory length: 913950   epsilon: 0.009998020008555413    steps: 671    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2639   score: 18.0   memory length: 914369   epsilon: 0.009998020008555413    steps: 419    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 2640   score: 10.0   memory length: 914861   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 13.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2641   score: 18.0   memory length: 915530   epsilon: 0.009998020008555413    steps: 669    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2642   score: 5.0   memory length: 915820   epsilon: 0.009998020008555413    steps: 290    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 2643   score: 6.0   memory length: 916181   epsilon: 0.009998020008555413    steps: 361    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2644   score: 16.0   memory length: 916757   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 2645   score: 8.0   memory length: 917173   epsilon: 0.009998020008555413    steps: 416    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2646   score: 8.0   memory length: 917547   epsilon: 0.009998020008555413    steps: 374    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2647   score: 11.0   memory length: 918164   epsilon: 0.009998020008555413    steps: 617    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 2648   score: 12.0   memory length: 918604   epsilon: 0.009998020008555413    steps: 440    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2649   score: 11.0   memory length: 919159   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2650   score: 5.0   memory length: 919468   epsilon: 0.009998020008555413    steps: 309    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2651   score: 28.0   memory length: 920228   epsilon: 0.009998020008555413    steps: 760    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2652   score: 16.0   memory length: 920916   epsilon: 0.009998020008555413    steps: 688    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 2653   score: 12.0   memory length: 921497   epsilon: 0.009998020008555413    steps: 581    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2654   score: 12.0   memory length: 921977   epsilon: 0.009998020008555413    steps: 480    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 2655   score: 11.0   memory length: 922503   epsilon: 0.009998020008555413    steps: 526    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2656   score: 13.0   memory length: 922949   epsilon: 0.009998020008555413    steps: 446    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 2657   score: 7.0   memory length: 923354   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2658   score: 6.0   memory length: 923684   epsilon: 0.009998020008555413    steps: 330    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2659   score: 11.0   memory length: 924260   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 2660   score: 10.0   memory length: 924707   epsilon: 0.009998020008555413    steps: 447    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 2661   score: 17.0   memory length: 925256   epsilon: 0.009998020008555413    steps: 549    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 2662   score: 15.0   memory length: 925917   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2663   score: 12.0   memory length: 926464   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2664   score: 18.0   memory length: 927134   epsilon: 0.009998020008555413    steps: 670    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 2665   score: 21.0   memory length: 927749   epsilon: 0.009998020008555413    steps: 615    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2666   score: 7.0   memory length: 928154   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2667   score: 12.0   memory length: 928712   epsilon: 0.009998020008555413    steps: 558    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 2668   score: 15.0   memory length: 929319   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 2669   score: 10.0   memory length: 929711   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 13.04\n",
      "episode: 2670   score: 13.0   memory length: 930213   epsilon: 0.009998020008555413    steps: 502    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 2671   score: 10.0   memory length: 930722   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 12.96\n",
      "episode: 2672   score: 8.0   memory length: 931127   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 2673   score: 12.0   memory length: 931598   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 12.95\n",
      "episode: 2674   score: 14.0   memory length: 932190   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 12.89\n",
      "episode: 2675   score: 13.0   memory length: 932652   epsilon: 0.009998020008555413    steps: 462    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 2676   score: 10.0   memory length: 933173   epsilon: 0.009998020008555413    steps: 521    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 2677   score: 12.0   memory length: 933692   epsilon: 0.009998020008555413    steps: 519    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 2678   score: 16.0   memory length: 934390   epsilon: 0.009998020008555413    steps: 698    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 2679   score: 10.0   memory length: 934885   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2680   score: 7.0   memory length: 935271   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 2681   score: 12.0   memory length: 935748   epsilon: 0.009998020008555413    steps: 477    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 2682   score: 11.0   memory length: 936267   epsilon: 0.009998020008555413    steps: 519    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 2683   score: 8.0   memory length: 936673   epsilon: 0.009998020008555413    steps: 406    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 2684   score: 12.0   memory length: 937294   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 12.7\n",
      "episode: 2685   score: 13.0   memory length: 937862   epsilon: 0.009998020008555413    steps: 568    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 2686   score: 12.0   memory length: 938511   epsilon: 0.009998020008555413    steps: 649    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 2687   score: 17.0   memory length: 939264   epsilon: 0.009998020008555413    steps: 753    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2688   score: 9.0   memory length: 939736   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2689   score: 15.0   memory length: 940338   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 2690   score: 7.0   memory length: 940710   epsilon: 0.009998020008555413    steps: 372    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2691   score: 10.0   memory length: 941195   epsilon: 0.009998020008555413    steps: 485    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 2692   score: 13.0   memory length: 941841   epsilon: 0.009998020008555413    steps: 646    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 2693   score: 14.0   memory length: 942243   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 2694   score: 7.0   memory length: 942608   epsilon: 0.009998020008555413    steps: 365    lr: 0.0001     evaluation reward: 12.31\n",
      "episode: 2695   score: 15.0   memory length: 943119   epsilon: 0.009998020008555413    steps: 511    lr: 0.0001     evaluation reward: 12.35\n",
      "episode: 2696   score: 12.0   memory length: 943723   epsilon: 0.009998020008555413    steps: 604    lr: 0.0001     evaluation reward: 12.36\n",
      "episode: 2697   score: 26.0   memory length: 944575   epsilon: 0.009998020008555413    steps: 852    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 2698   score: 14.0   memory length: 945115   epsilon: 0.009998020008555413    steps: 540    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2699   score: 6.0   memory length: 945448   epsilon: 0.009998020008555413    steps: 333    lr: 0.0001     evaluation reward: 12.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2700   score: 10.0   memory length: 945914   epsilon: 0.009998020008555413    steps: 466    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2701   score: 15.0   memory length: 946441   epsilon: 0.009998020008555413    steps: 527    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2702   score: 20.0   memory length: 947109   epsilon: 0.009998020008555413    steps: 668    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 2703   score: 15.0   memory length: 947613   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 2704   score: 9.0   memory length: 948032   epsilon: 0.009998020008555413    steps: 419    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2705   score: 18.0   memory length: 948554   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2706   score: 15.0   memory length: 949249   epsilon: 0.009998020008555413    steps: 695    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 2707   score: 6.0   memory length: 949566   epsilon: 0.009998020008555413    steps: 317    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2708   score: 16.0   memory length: 950114   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2709   score: 18.0   memory length: 950746   epsilon: 0.009998020008555413    steps: 632    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2710   score: 14.0   memory length: 951383   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2711   score: 6.0   memory length: 951739   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 2712   score: 11.0   memory length: 952285   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 2713   score: 7.0   memory length: 952672   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 12.26\n",
      "episode: 2714   score: 22.0   memory length: 953279   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 2715   score: 10.0   memory length: 953828   epsilon: 0.009998020008555413    steps: 549    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 2716   score: 8.0   memory length: 954283   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 2717   score: 6.0   memory length: 954659   epsilon: 0.009998020008555413    steps: 376    lr: 0.0001     evaluation reward: 12.26\n",
      "episode: 2718   score: 17.0   memory length: 955284   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 12.29\n",
      "episode: 2719   score: 13.0   memory length: 955853   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 12.35\n",
      "episode: 2720   score: 11.0   memory length: 956380   epsilon: 0.009998020008555413    steps: 527    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 2721   score: 7.0   memory length: 956752   epsilon: 0.009998020008555413    steps: 372    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2722   score: 17.0   memory length: 957354   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 2723   score: 10.0   memory length: 957862   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2724   score: 10.0   memory length: 958331   epsilon: 0.009998020008555413    steps: 469    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 2725   score: 18.0   memory length: 959023   epsilon: 0.009998020008555413    steps: 692    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 2726   score: 15.0   memory length: 959705   epsilon: 0.009998020008555413    steps: 682    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 2727   score: 10.0   memory length: 960213   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 2728   score: 14.0   memory length: 960775   epsilon: 0.009998020008555413    steps: 562    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 2729   score: 11.0   memory length: 961259   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 2730   score: 15.0   memory length: 961847   epsilon: 0.009998020008555413    steps: 588    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 2731   score: 27.0   memory length: 962682   epsilon: 0.009998020008555413    steps: 835    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 2732   score: 18.0   memory length: 963316   epsilon: 0.009998020008555413    steps: 634    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 2733   score: 13.0   memory length: 963966   epsilon: 0.009998020008555413    steps: 650    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2734   score: 12.0   memory length: 964526   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2735   score: 14.0   memory length: 965062   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 2736   score: 16.0   memory length: 965696   epsilon: 0.009998020008555413    steps: 634    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2737   score: 13.0   memory length: 966310   epsilon: 0.009998020008555413    steps: 614    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2738   score: 16.0   memory length: 966868   epsilon: 0.009998020008555413    steps: 558    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 2739   score: 11.0   memory length: 967372   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 2740   score: 14.0   memory length: 968002   epsilon: 0.009998020008555413    steps: 630    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 2741   score: 17.0   memory length: 968611   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 2742   score: 12.0   memory length: 969061   epsilon: 0.009998020008555413    steps: 450    lr: 0.0001     evaluation reward: 12.65\n",
      "episode: 2743   score: 15.0   memory length: 969609   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 12.74\n",
      "episode: 2744   score: 18.0   memory length: 970298   epsilon: 0.009998020008555413    steps: 689    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 2745   score: 18.0   memory length: 971062   epsilon: 0.009998020008555413    steps: 764    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 2746   score: 30.0   memory length: 971889   epsilon: 0.009998020008555413    steps: 827    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 2747   score: 8.0   memory length: 972307   epsilon: 0.009998020008555413    steps: 418    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 2748   score: 17.0   memory length: 972954   epsilon: 0.009998020008555413    steps: 647    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 2749   score: 19.0   memory length: 973526   epsilon: 0.009998020008555413    steps: 572    lr: 0.0001     evaluation reward: 13.18\n",
      "episode: 2750   score: 21.0   memory length: 974182   epsilon: 0.009998020008555413    steps: 656    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2751   score: 24.0   memory length: 974914   epsilon: 0.009998020008555413    steps: 732    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2752   score: 12.0   memory length: 975500   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 13.26\n",
      "episode: 2753   score: 14.0   memory length: 976089   epsilon: 0.009998020008555413    steps: 589    lr: 0.0001     evaluation reward: 13.28\n",
      "episode: 2754   score: 15.0   memory length: 976648   epsilon: 0.009998020008555413    steps: 559    lr: 0.0001     evaluation reward: 13.31\n",
      "episode: 2755   score: 13.0   memory length: 977080   epsilon: 0.009998020008555413    steps: 432    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2756   score: 17.0   memory length: 977583   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 13.37\n",
      "episode: 2757   score: 10.0   memory length: 978064   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2758   score: 18.0   memory length: 978614   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 13.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2759   score: 8.0   memory length: 979036   epsilon: 0.009998020008555413    steps: 422    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2760   score: 9.0   memory length: 979394   epsilon: 0.009998020008555413    steps: 358    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2761   score: 7.0   memory length: 979780   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 2762   score: 9.0   memory length: 980221   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 13.32\n",
      "episode: 2763   score: 11.0   memory length: 980723   epsilon: 0.009998020008555413    steps: 502    lr: 0.0001     evaluation reward: 13.31\n",
      "episode: 2764   score: 14.0   memory length: 981388   epsilon: 0.009998020008555413    steps: 665    lr: 0.0001     evaluation reward: 13.27\n",
      "episode: 2765   score: 15.0   memory length: 981995   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 13.21\n",
      "episode: 2766   score: 19.0   memory length: 982671   epsilon: 0.009998020008555413    steps: 676    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2767   score: 11.0   memory length: 983208   epsilon: 0.009998020008555413    steps: 537    lr: 0.0001     evaluation reward: 13.32\n",
      "episode: 2768   score: 15.0   memory length: 983731   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 13.32\n",
      "episode: 2769   score: 31.0   memory length: 984456   epsilon: 0.009998020008555413    steps: 725    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2770   score: 13.0   memory length: 985009   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2771   score: 10.0   memory length: 985545   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 2772   score: 11.0   memory length: 986045   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2773   score: 12.0   memory length: 986462   epsilon: 0.009998020008555413    steps: 417    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2774   score: 17.0   memory length: 987055   epsilon: 0.009998020008555413    steps: 593    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2775   score: 14.0   memory length: 987589   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.6\n",
      "episode: 2776   score: 9.0   memory length: 988064   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2777   score: 8.0   memory length: 988428   epsilon: 0.009998020008555413    steps: 364    lr: 0.0001     evaluation reward: 13.55\n",
      "episode: 2778   score: 9.0   memory length: 988863   epsilon: 0.009998020008555413    steps: 435    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2779   score: 9.0   memory length: 989278   epsilon: 0.009998020008555413    steps: 415    lr: 0.0001     evaluation reward: 13.47\n",
      "episode: 2780   score: 16.0   memory length: 989884   epsilon: 0.009998020008555413    steps: 606    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2781   score: 19.0   memory length: 990568   epsilon: 0.009998020008555413    steps: 684    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2782   score: 12.0   memory length: 991128   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2783   score: 10.0   memory length: 991631   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 13.66\n",
      "episode: 2784   score: 7.0   memory length: 992018   epsilon: 0.009998020008555413    steps: 387    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2785   score: 14.0   memory length: 992549   epsilon: 0.009998020008555413    steps: 531    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2786   score: 17.0   memory length: 993162   epsilon: 0.009998020008555413    steps: 613    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2787   score: 15.0   memory length: 993820   epsilon: 0.009998020008555413    steps: 658    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2788   score: 10.0   memory length: 994279   epsilon: 0.009998020008555413    steps: 459    lr: 0.0001     evaluation reward: 13.66\n",
      "episode: 2789   score: 11.0   memory length: 994658   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2790   score: 9.0   memory length: 995096   epsilon: 0.009998020008555413    steps: 438    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2791   score: 14.0   memory length: 995746   epsilon: 0.009998020008555413    steps: 650    lr: 0.0001     evaluation reward: 13.68\n",
      "episode: 2792   score: 10.0   memory length: 996225   epsilon: 0.009998020008555413    steps: 479    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2793   score: 19.0   memory length: 996866   epsilon: 0.009998020008555413    steps: 641    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2794   score: 18.0   memory length: 997524   epsilon: 0.009998020008555413    steps: 658    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2795   score: 14.0   memory length: 998076   epsilon: 0.009998020008555413    steps: 552    lr: 0.0001     evaluation reward: 13.8\n",
      "episode: 2796   score: 21.0   memory length: 998721   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 13.89\n",
      "episode: 2797   score: 13.0   memory length: 999335   epsilon: 0.009998020008555413    steps: 614    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 2798   score: 20.0   memory length: 999786   epsilon: 0.009998020008555413    steps: 451    lr: 0.0001     evaluation reward: 13.82\n",
      "episode: 2799   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 556    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2800   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 563    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2801   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 596    lr: 0.0001     evaluation reward: 13.9\n",
      "episode: 2802   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2803   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 347    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2804   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 13.75\n",
      "episode: 2805   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2806   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2807   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 13.75\n",
      "episode: 2808   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 578    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2809   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 461    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 2810   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2811   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2812   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 662    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 2813   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 541    lr: 0.0001     evaluation reward: 13.84\n",
      "episode: 2814   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 453    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2815   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 743    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2816   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 468    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2817   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 497    lr: 0.0001     evaluation reward: 13.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2818   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 581    lr: 0.0001     evaluation reward: 13.94\n",
      "episode: 2819   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 587    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2820   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 631    lr: 0.0001     evaluation reward: 14.0\n",
      "episode: 2821   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 14.13\n",
      "episode: 2822   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 14.13\n",
      "episode: 2823   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 731    lr: 0.0001     evaluation reward: 14.22\n",
      "episode: 2824   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2825   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 615    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2826   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 14.17\n",
      "episode: 2827   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 578    lr: 0.0001     evaluation reward: 14.23\n",
      "episode: 2828   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 653    lr: 0.0001     evaluation reward: 14.24\n",
      "episode: 2829   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 14.28\n",
      "episode: 2830   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 14.31\n",
      "episode: 2831   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 620    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 2832   score: 24.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 768    lr: 0.0001     evaluation reward: 14.25\n",
      "episode: 2833   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 693    lr: 0.0001     evaluation reward: 14.27\n",
      "episode: 2834   score: 24.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 861    lr: 0.0001     evaluation reward: 14.39\n",
      "episode: 2835   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 694    lr: 0.0001     evaluation reward: 14.4\n",
      "episode: 2836   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 384    lr: 0.0001     evaluation reward: 14.32\n",
      "episode: 2837   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 731    lr: 0.0001     evaluation reward: 14.38\n",
      "episode: 2838   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 14.38\n",
      "episode: 2839   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 14.37\n",
      "episode: 2840   score: 22.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 863    lr: 0.0001     evaluation reward: 14.45\n",
      "episode: 2841   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 14.41\n",
      "episode: 2842   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 666    lr: 0.0001     evaluation reward: 14.44\n",
      "episode: 2843   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 404    lr: 0.0001     evaluation reward: 14.36\n",
      "episode: 2844   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 588    lr: 0.0001     evaluation reward: 14.29\n",
      "episode: 2845   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 14.22\n",
      "episode: 2846   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 502    lr: 0.0001     evaluation reward: 14.02\n",
      "episode: 2847   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 620    lr: 0.0001     evaluation reward: 14.07\n",
      "episode: 2848   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 701    lr: 0.0001     evaluation reward: 14.08\n",
      "episode: 2849   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2850   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2851   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 611    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2852   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 13.88\n",
      "episode: 2853   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2854   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 489    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2855   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 13.84\n",
      "episode: 2856   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 493    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2857   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 13.75\n",
      "episode: 2858   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 516    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2859   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 13.75\n",
      "episode: 2860   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2861   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 13.87\n",
      "episode: 2862   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 381    lr: 0.0001     evaluation reward: 13.92\n",
      "episode: 2863   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 13.9\n",
      "episode: 2864   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 410    lr: 0.0001     evaluation reward: 13.84\n",
      "episode: 2865   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2866   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 400    lr: 0.0001     evaluation reward: 13.68\n",
      "episode: 2867   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 328    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2868   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2869   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 598    lr: 0.0001     evaluation reward: 13.44\n",
      "episode: 2870   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 438    lr: 0.0001     evaluation reward: 13.43\n",
      "episode: 2871   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 389    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2872   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 13.45\n",
      "episode: 2873   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 13.47\n",
      "episode: 2874   score: 27.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 777    lr: 0.0001     evaluation reward: 13.57\n",
      "episode: 2875   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 13.53\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2876   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 530    lr: 0.0001     evaluation reward: 13.55\n",
      "episode: 2877   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 463    lr: 0.0001     evaluation reward: 13.57\n",
      "episode: 2878   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 279    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2879   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 690    lr: 0.0001     evaluation reward: 13.68\n",
      "episode: 2880   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 449    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2881   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 715    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2882   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 13.66\n",
      "episode: 2883   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 531    lr: 0.0001     evaluation reward: 13.68\n",
      "episode: 2884   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 2885   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 421    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2886   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2887   score: 32.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 899    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 2888   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 563    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2889   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 365    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 2890   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2891   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 693    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2892   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2893   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 539    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2894   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 13.77\n",
      "episode: 2895   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 448    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2896   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 771    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2897   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2898   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2899   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 541    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 2900   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 664    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2901   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 748    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2902   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 400    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 2903   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 660    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2904   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2905   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 599    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 2906   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 2907   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 657    lr: 0.0001     evaluation reward: 13.8\n",
      "episode: 2908   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 2909   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 420    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2910   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2911   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 13.87\n",
      "episode: 2912   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 13.85\n",
      "episode: 2913   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 13.79\n",
      "episode: 2914   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 625    lr: 0.0001     evaluation reward: 13.87\n",
      "episode: 2915   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2916   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 532    lr: 0.0001     evaluation reward: 13.77\n",
      "episode: 2917   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 448    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2918   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 600    lr: 0.0001     evaluation reward: 13.79\n",
      "episode: 2919   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 2920   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 502    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 2921   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 463    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2922   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 611    lr: 0.0001     evaluation reward: 13.72\n",
      "episode: 2923   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 479    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2924   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 682    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2925   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 606    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 2926   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2927   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 646    lr: 0.0001     evaluation reward: 13.67\n",
      "episode: 2928   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 630    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 2929   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 2930   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 632    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 2931   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 470    lr: 0.0001     evaluation reward: 13.66\n",
      "episode: 2932   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 356    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2933   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 655    lr: 0.0001     evaluation reward: 13.51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2934   score: 22.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 817    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2935   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 13.51\n",
      "episode: 2936   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 377    lr: 0.0001     evaluation reward: 13.5\n",
      "episode: 2937   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 632    lr: 0.0001     evaluation reward: 13.52\n",
      "episode: 2938   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 478    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 2939   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2940   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 13.34\n",
      "episode: 2941   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 659    lr: 0.0001     evaluation reward: 13.35\n",
      "episode: 2942   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 479    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2943   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 463    lr: 0.0001     evaluation reward: 13.33\n",
      "episode: 2944   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 500    lr: 0.0001     evaluation reward: 13.32\n",
      "episode: 2945   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2946   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 514    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2947   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 450    lr: 0.0001     evaluation reward: 13.36\n",
      "episode: 2948   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 13.3\n",
      "episode: 2949   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 13.41\n",
      "episode: 2950   score: 25.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 618    lr: 0.0001     evaluation reward: 13.5\n",
      "episode: 2951   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 499    lr: 0.0001     evaluation reward: 13.47\n",
      "episode: 2952   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 2953   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 539    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 2954   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 740    lr: 0.0001     evaluation reward: 13.46\n",
      "episode: 2955   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 2956   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 668    lr: 0.0001     evaluation reward: 13.51\n",
      "episode: 2957   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 13.58\n",
      "episode: 2958   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 758    lr: 0.0001     evaluation reward: 13.65\n",
      "episode: 2959   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 514    lr: 0.0001     evaluation reward: 13.6\n",
      "episode: 2960   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 639    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 2961   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 487    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 2962   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 13.61\n",
      "episode: 2963   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 535    lr: 0.0001     evaluation reward: 13.63\n",
      "episode: 2964   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 662    lr: 0.0001     evaluation reward: 13.73\n",
      "episode: 2965   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 490    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 2966   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 539    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 2967   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 574    lr: 0.0001     evaluation reward: 13.84\n",
      "episode: 2968   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 13.91\n",
      "episode: 2969   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 583    lr: 0.0001     evaluation reward: 13.94\n",
      "episode: 2970   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2971   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 611    lr: 0.0001     evaluation reward: 14.0\n",
      "episode: 2972   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2973   score: 22.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 682    lr: 0.0001     evaluation reward: 14.01\n",
      "episode: 2974   score: 22.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 838    lr: 0.0001     evaluation reward: 13.96\n",
      "episode: 2975   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 386    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2976   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 713    lr: 0.0001     evaluation reward: 13.97\n",
      "episode: 2977   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 533    lr: 0.0001     evaluation reward: 14.01\n",
      "episode: 2978   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 738    lr: 0.0001     evaluation reward: 14.1\n",
      "episode: 2979   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 13.99\n",
      "episode: 2980   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 444    lr: 0.0001     evaluation reward: 13.99\n",
      "episode: 2981   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2982   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 669    lr: 0.0001     evaluation reward: 13.91\n",
      "episode: 2983   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 528    lr: 0.0001     evaluation reward: 13.93\n",
      "episode: 2984   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 561    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2985   score: 27.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 867    lr: 0.0001     evaluation reward: 14.18\n",
      "episode: 2986   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 14.16\n",
      "episode: 2987   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 14.05\n",
      "episode: 2988   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 687    lr: 0.0001     evaluation reward: 14.1\n",
      "episode: 2989   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 14.13\n",
      "episode: 2990   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 14.12\n",
      "episode: 2991   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 694    lr: 0.0001     evaluation reward: 14.09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 2992   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 14.1\n",
      "episode: 2993   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 14.11\n",
      "episode: 2994   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 14.08\n",
      "episode: 2995   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 14.07\n",
      "episode: 2996   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 13.98\n",
      "episode: 2997   score: 22.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 769    lr: 0.0001     evaluation reward: 14.11\n",
      "episode: 2998   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 522    lr: 0.0001     evaluation reward: 14.17\n",
      "episode: 2999   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 3000   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 512    lr: 0.0001     evaluation reward: 14.11\n",
      "episode: 3001   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 14.11\n",
      "episode: 3002   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 633    lr: 0.0001     evaluation reward: 14.19\n",
      "episode: 3003   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 402    lr: 0.0001     evaluation reward: 14.07\n",
      "episode: 3004   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 537    lr: 0.0001     evaluation reward: 14.09\n",
      "episode: 3005   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 14.07\n",
      "episode: 3006   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 564    lr: 0.0001     evaluation reward: 14.04\n",
      "episode: 3007   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 351    lr: 0.0001     evaluation reward: 13.91\n",
      "episode: 3008   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 285    lr: 0.0001     evaluation reward: 13.77\n",
      "episode: 3009   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 3010   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 712    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 3011   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 637    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 3012   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 595    lr: 0.0001     evaluation reward: 13.76\n",
      "episode: 3013   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 13.83\n",
      "episode: 3014   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 13.86\n",
      "episode: 3015   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 478    lr: 0.0001     evaluation reward: 13.8\n",
      "episode: 3016   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 13.82\n",
      "episode: 3017   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 334    lr: 0.0001     evaluation reward: 13.78\n",
      "episode: 3018   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 535    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 3019   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 641    lr: 0.0001     evaluation reward: 13.71\n",
      "episode: 3020   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 529    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 3021   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 13.68\n",
      "episode: 3022   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 668    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 3023   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 13.68\n",
      "episode: 3024   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 603    lr: 0.0001     evaluation reward: 13.69\n",
      "episode: 3025   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 716    lr: 0.0001     evaluation reward: 13.74\n",
      "episode: 3026   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 730    lr: 0.0001     evaluation reward: 13.81\n",
      "episode: 3027   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 307    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 3028   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 573    lr: 0.0001     evaluation reward: 13.64\n",
      "episode: 3029   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 13.55\n",
      "episode: 3030   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 651    lr: 0.0001     evaluation reward: 13.59\n",
      "episode: 3031   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 616    lr: 0.0001     evaluation reward: 13.62\n",
      "episode: 3032   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 575    lr: 0.0001     evaluation reward: 13.7\n",
      "episode: 3033   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 655    lr: 0.0001     evaluation reward: 13.66\n",
      "episode: 3034   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 13.53\n",
      "episode: 3035   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 13.46\n",
      "episode: 3036   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 465    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 3037   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 13.4\n",
      "episode: 3038   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 459    lr: 0.0001     evaluation reward: 13.35\n",
      "episode: 3039   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 557    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 3040   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 13.45\n",
      "episode: 3041   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 13.37\n",
      "episode: 3042   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 13.39\n",
      "episode: 3043   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 445    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 3044   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 602    lr: 0.0001     evaluation reward: 13.49\n",
      "episode: 3045   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 13.43\n",
      "episode: 3046   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 349    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 3047   score: 24.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 658    lr: 0.0001     evaluation reward: 13.57\n",
      "episode: 3048   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 13.56\n",
      "episode: 3049   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 547    lr: 0.0001     evaluation reward: 13.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3050   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 13.42\n",
      "episode: 3051   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 408    lr: 0.0001     evaluation reward: 13.36\n",
      "episode: 3052   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 770    lr: 0.0001     evaluation reward: 13.5\n",
      "episode: 3053   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 447    lr: 0.0001     evaluation reward: 13.48\n",
      "episode: 3054   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 583    lr: 0.0001     evaluation reward: 13.43\n",
      "episode: 3055   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 519    lr: 0.0001     evaluation reward: 13.35\n",
      "episode: 3056   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 382    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 3057   score: 23.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 591    lr: 0.0001     evaluation reward: 13.38\n",
      "episode: 3058   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 675    lr: 0.0001     evaluation reward: 13.35\n",
      "episode: 3059   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 13.37\n",
      "episode: 3060   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 461    lr: 0.0001     evaluation reward: 13.28\n",
      "episode: 3061   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 542    lr: 0.0001     evaluation reward: 13.31\n",
      "episode: 3062   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 482    lr: 0.0001     evaluation reward: 13.28\n",
      "episode: 3063   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 748    lr: 0.0001     evaluation reward: 13.36\n",
      "episode: 3064   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 545    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 3065   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 394    lr: 0.0001     evaluation reward: 13.29\n",
      "episode: 3066   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 382    lr: 0.0001     evaluation reward: 13.25\n",
      "episode: 3067   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 373    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 3068   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 650    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 3069   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 556    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 3070   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 3071   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 13.09\n",
      "episode: 3072   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 13.19\n",
      "episode: 3073   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 278    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 3074   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 3075   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 649    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 3076   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 596    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 3077   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 613    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 3078   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 649    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 3079   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 459    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 3080   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 321    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 3081   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 545    lr: 0.0001     evaluation reward: 13.1\n",
      "episode: 3082   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 480    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 3083   score: 3.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 246    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 3084   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 409    lr: 0.0001     evaluation reward: 12.95\n",
      "episode: 3085   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 3086   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 3087   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 301    lr: 0.0001     evaluation reward: 12.64\n",
      "episode: 3088   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 393    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 3089   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 606    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 3090   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 563    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 3091   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 490    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 3092   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 427    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 3093   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 3094   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 3095   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 3096   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 326    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 3097   score: 29.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 681    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 3098   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 644    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 3099   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 3100   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 3101   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 3102   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 689    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 3103   score: 30.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 979    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 3104   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 457    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 3105   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 725    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 3106   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 3107   score: 28.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 882    lr: 0.0001     evaluation reward: 13.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3108   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 3109   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 506    lr: 0.0001     evaluation reward: 13.2\n",
      "episode: 3110   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 13.24\n",
      "episode: 3111   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 378    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 3112   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 13.11\n",
      "episode: 3113   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 503    lr: 0.0001     evaluation reward: 13.04\n",
      "episode: 3114   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 592    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 3115   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 516    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 3116   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 352    lr: 0.0001     evaluation reward: 12.95\n",
      "episode: 3117   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 589    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 3118   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 485    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 3119   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.96\n",
      "episode: 3120   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 412    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 3121   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 451    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 3122   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 469    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 3123   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 638    lr: 0.0001     evaluation reward: 13.02\n",
      "episode: 3124   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 3125   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 577    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 3126   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 3127   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 3128   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 511    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 3129   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 679    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 3130   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 3131   score: 3.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 246    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 3132   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 570    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 3133   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 507    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 3134   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 750    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 3135   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 3136   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 552    lr: 0.0001     evaluation reward: 13.06\n",
      "episode: 3137   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 588    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 3138   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 13.16\n",
      "episode: 3139   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 504    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 3140   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 518    lr: 0.0001     evaluation reward: 13.13\n",
      "episode: 3141   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 372    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 3142   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 649    lr: 0.0001     evaluation reward: 13.17\n",
      "episode: 3143   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 521    lr: 0.0001     evaluation reward: 13.14\n",
      "episode: 3144   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 3145   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 546    lr: 0.0001     evaluation reward: 13.01\n",
      "episode: 3146   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 346    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 3147   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 3148   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 3149   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 435    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 3150   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 588    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 3151   score: 23.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 814    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 3152   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 501    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 3153   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 595    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 3154   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 3155   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 3156   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 818    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 3157   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 395    lr: 0.0001     evaluation reward: 12.91\n",
      "episode: 3158   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 583    lr: 0.0001     evaluation reward: 12.93\n",
      "episode: 3159   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 580    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 3160   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 418    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 3161   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 398    lr: 0.0001     evaluation reward: 12.84\n",
      "episode: 3162   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 372    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 3163   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 3164   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 12.72\n",
      "episode: 3165   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 649    lr: 0.0001     evaluation reward: 12.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3166   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 540    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 3167   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 375    lr: 0.0001     evaluation reward: 12.86\n",
      "episode: 3168   score: 4.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 3169   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 661    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 3170   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 451    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 3171   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 484    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 3172   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 12.67\n",
      "episode: 3173   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 624    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 3174   score: 4.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 275    lr: 0.0001     evaluation reward: 12.62\n",
      "episode: 3175   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 385    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 3176   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 429    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 3177   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 3178   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 509    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 3179   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 12.35\n",
      "episode: 3180   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 511    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 3181   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 489    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 3182   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 3183   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 325    lr: 0.0001     evaluation reward: 12.35\n",
      "episode: 3184   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 543    lr: 0.0001     evaluation reward: 12.36\n",
      "episode: 3185   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 3186   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 605    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 3187   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 436    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 3188   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 660    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 3189   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 435    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 3190   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 3191   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 384    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 3192   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 639    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 3193   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 526    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 3194   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 586    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 3195   score: 28.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 955    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 3196   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 723    lr: 0.0001     evaluation reward: 12.78\n",
      "episode: 3197   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 389    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 3198   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 419    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 3199   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 621    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 3200   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 486    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 3201   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 480    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 3202   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 544    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 3203   score: 29.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 781    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 3204   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 3205   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 410    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 3206   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 501    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 3207   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 540    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3208   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 569    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3209   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 645    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3210   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 388    lr: 0.0001     evaluation reward: 12.12\n",
      "episode: 3211   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 427    lr: 0.0001     evaluation reward: 12.13\n",
      "episode: 3212   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 12.16\n",
      "episode: 3213   score: 22.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 766    lr: 0.0001     evaluation reward: 12.29\n",
      "episode: 3214   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 590    lr: 0.0001     evaluation reward: 12.27\n",
      "episode: 3215   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 3216   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 3217   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 533    lr: 0.0001     evaluation reward: 12.36\n",
      "episode: 3218   score: 26.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 735    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 3219   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 384    lr: 0.0001     evaluation reward: 12.45\n",
      "episode: 3220   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 3221   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 322    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 3222   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 12.34\n",
      "episode: 3223   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 444    lr: 0.0001     evaluation reward: 12.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3224   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 636    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 3225   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 352    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 3226   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 12.1\n",
      "episode: 3227   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 498    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 3228   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 550    lr: 0.0001     evaluation reward: 12.13\n",
      "episode: 3229   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 294    lr: 0.0001     evaluation reward: 12.05\n",
      "episode: 3230   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 468    lr: 0.0001     evaluation reward: 12.07\n",
      "episode: 3231   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 682    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3232   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 323    lr: 0.0001     evaluation reward: 12.16\n",
      "episode: 3233   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 12.21\n",
      "episode: 3234   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 640    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3235   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 535    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 3236   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 618    lr: 0.0001     evaluation reward: 12.15\n",
      "episode: 3237   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 536    lr: 0.0001     evaluation reward: 12.09\n",
      "episode: 3238   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 647    lr: 0.0001     evaluation reward: 12.13\n",
      "episode: 3239   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 609    lr: 0.0001     evaluation reward: 12.16\n",
      "episode: 3240   score: 25.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 779    lr: 0.0001     evaluation reward: 12.3\n",
      "episode: 3241   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 346    lr: 0.0001     evaluation reward: 12.29\n",
      "episode: 3242   score: 4.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 302    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 3243   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 583    lr: 0.0001     evaluation reward: 12.21\n",
      "episode: 3244   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 567    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 3245   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 573    lr: 0.0001     evaluation reward: 12.38\n",
      "episode: 3246   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 493    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 3247   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 396    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 3248   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 654    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 3249   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 309    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 3250   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 673    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 3251   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 473    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 3252   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 460    lr: 0.0001     evaluation reward: 12.33\n",
      "episode: 3253   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 681    lr: 0.0001     evaluation reward: 12.32\n",
      "episode: 3254   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 12.28\n",
      "episode: 3255   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 515    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3256   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 437    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 3257   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 12.17\n",
      "episode: 3258   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 483    lr: 0.0001     evaluation reward: 12.11\n",
      "episode: 3259   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 360    lr: 0.0001     evaluation reward: 12.05\n",
      "episode: 3260   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 371    lr: 0.0001     evaluation reward: 12.04\n",
      "episode: 3261   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 458    lr: 0.0001     evaluation reward: 12.04\n",
      "episode: 3262   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 600    lr: 0.0001     evaluation reward: 12.1\n",
      "episode: 3263   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 624    lr: 0.0001     evaluation reward: 12.09\n",
      "episode: 3264   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 619    lr: 0.0001     evaluation reward: 12.15\n",
      "episode: 3265   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 647    lr: 0.0001     evaluation reward: 12.18\n",
      "episode: 3266   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 583    lr: 0.0001     evaluation reward: 12.22\n",
      "episode: 3267   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 669    lr: 0.0001     evaluation reward: 12.29\n",
      "episode: 3268   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 3269   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 3270   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 471    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 3271   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 511    lr: 0.0001     evaluation reward: 12.35\n",
      "episode: 3272   score: 25.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 651    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 3273   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 472    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 3274   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 579    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 3275   score: 25.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 776    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 3276   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 3277   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 3278   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 642    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 3279   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 647    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 3280   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 616    lr: 0.0001     evaluation reward: 12.99\n",
      "episode: 3281   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 525    lr: 0.0001     evaluation reward: 13.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3282   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 560    lr: 0.0001     evaluation reward: 13.0\n",
      "episode: 3283   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 572    lr: 0.0001     evaluation reward: 13.07\n",
      "episode: 3284   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 466    lr: 0.0001     evaluation reward: 13.05\n",
      "episode: 3285   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 13.04\n",
      "episode: 3286   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 534    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 3287   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 607    lr: 0.0001     evaluation reward: 13.08\n",
      "episode: 3288   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 508    lr: 0.0001     evaluation reward: 13.03\n",
      "episode: 3289   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 519    lr: 0.0001     evaluation reward: 13.12\n",
      "episode: 3290   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 548    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 3291   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 635    lr: 0.0001     evaluation reward: 13.28\n",
      "episode: 3292   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 443    lr: 0.0001     evaluation reward: 13.24\n",
      "episode: 3293   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 401    lr: 0.0001     evaluation reward: 13.22\n",
      "episode: 3294   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 624    lr: 0.0001     evaluation reward: 13.15\n",
      "episode: 3295   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 379    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 3296   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 532    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 3297   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 553    lr: 0.0001     evaluation reward: 12.9\n",
      "episode: 3298   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 752    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 3299   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 578    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 3300   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 566    lr: 0.0001     evaluation reward: 12.98\n",
      "episode: 3301   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 505    lr: 0.0001     evaluation reward: 12.94\n",
      "episode: 3302   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 646    lr: 0.0001     evaluation reward: 12.97\n",
      "episode: 3303   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 417    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 3304   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 12.8\n",
      "episode: 3305   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 507    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 3306   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 3307   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 12.77\n",
      "episode: 3308   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 554    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 3309   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.7\n",
      "episode: 3310   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 626    lr: 0.0001     evaluation reward: 12.81\n",
      "episode: 3311   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 12.83\n",
      "episode: 3312   score: 5.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 272    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 3313   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 405    lr: 0.0001     evaluation reward: 12.61\n",
      "episode: 3314   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 502    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 3315   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 551    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 3316   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 585    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 3317   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 678    lr: 0.0001     evaluation reward: 12.63\n",
      "episode: 3318   score: 4.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 275    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 3319   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 597    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 3320   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 656    lr: 0.0001     evaluation reward: 12.57\n",
      "episode: 3321   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 680    lr: 0.0001     evaluation reward: 12.69\n",
      "episode: 3322   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 474    lr: 0.0001     evaluation reward: 12.68\n",
      "episode: 3323   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 3324   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 488    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 3325   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 684    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 3326   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 651    lr: 0.0001     evaluation reward: 12.87\n",
      "episode: 3327   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 456    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 3328   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 394    lr: 0.0001     evaluation reward: 12.79\n",
      "episode: 3329   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 523    lr: 0.0001     evaluation reward: 12.82\n",
      "episode: 3330   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 665    lr: 0.0001     evaluation reward: 12.89\n",
      "episode: 3331   score: 19.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 431    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 3332   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 467    lr: 0.0001     evaluation reward: 12.92\n",
      "episode: 3333   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 576    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 3334   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 724    lr: 0.0001     evaluation reward: 12.88\n",
      "episode: 3335   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 498    lr: 0.0001     evaluation reward: 12.85\n",
      "episode: 3336   score: 4.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 280    lr: 0.0001     evaluation reward: 12.73\n",
      "episode: 3337   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 495    lr: 0.0001     evaluation reward: 12.76\n",
      "episode: 3338   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 463    lr: 0.0001     evaluation reward: 12.75\n",
      "episode: 3339   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 444    lr: 0.0001     evaluation reward: 12.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 3340   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 359    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 3341   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 582    lr: 0.0001     evaluation reward: 12.54\n",
      "episode: 3342   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 455    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 3343   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 400    lr: 0.0001     evaluation reward: 12.6\n",
      "episode: 3344   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 476    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 3345   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 559    lr: 0.0001     evaluation reward: 12.48\n",
      "episode: 3346   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 441    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 3347   score: 20.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 786    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 3348   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 510    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 3349   score: 8.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 392    lr: 0.0001     evaluation reward: 12.58\n",
      "episode: 3350   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 555    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 3351   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 489    lr: 0.0001     evaluation reward: 12.47\n",
      "episode: 3352   score: 15.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 561    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 3353   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 362    lr: 0.0001     evaluation reward: 12.43\n",
      "episode: 3354   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 584    lr: 0.0001     evaluation reward: 12.44\n",
      "episode: 3355   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 583    lr: 0.0001     evaluation reward: 12.5\n",
      "episode: 3356   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 397    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 3357   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 520    lr: 0.0001     evaluation reward: 12.49\n",
      "episode: 3358   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 476    lr: 0.0001     evaluation reward: 12.51\n",
      "episode: 3359   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 490    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 3360   score: 11.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 475    lr: 0.0001     evaluation reward: 12.59\n",
      "episode: 3361   score: 4.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 266    lr: 0.0001     evaluation reward: 12.55\n",
      "episode: 3362   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 494    lr: 0.0001     evaluation reward: 12.52\n",
      "episode: 3363   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 655    lr: 0.0001     evaluation reward: 12.53\n",
      "episode: 3364   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 326    lr: 0.0001     evaluation reward: 12.46\n",
      "episode: 3365   score: 12.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 524    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 3366   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 407    lr: 0.0001     evaluation reward: 12.28\n",
      "episode: 3367   score: 24.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 733    lr: 0.0001     evaluation reward: 12.37\n",
      "episode: 3368   score: 17.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 694    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 3369   score: 18.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 678    lr: 0.0001     evaluation reward: 12.42\n",
      "episode: 3370   score: 6.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 323    lr: 0.0001     evaluation reward: 12.39\n",
      "episode: 3371   score: 28.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 635    lr: 0.0001     evaluation reward: 12.56\n",
      "episode: 3372   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 442    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 3373   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 454    lr: 0.0001     evaluation reward: 12.4\n",
      "episode: 3374   score: 16.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 539    lr: 0.0001     evaluation reward: 12.41\n",
      "episode: 3375   score: 10.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 481    lr: 0.0001     evaluation reward: 12.26\n",
      "episode: 3376   score: 9.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 492    lr: 0.0001     evaluation reward: 12.23\n",
      "episode: 3377   score: 14.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 655    lr: 0.0001     evaluation reward: 12.24\n",
      "episode: 3378   score: 7.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 349    lr: 0.0001     evaluation reward: 12.14\n",
      "episode: 3379   score: 21.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 676    lr: 0.0001     evaluation reward: 12.2\n",
      "episode: 3380   score: 13.0   memory length: 1000000   epsilon: 0.009998020008555413    steps: 598    lr: 0.0001     evaluation reward: 12.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Start training after random sample generation\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(frame \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m train_frame):\n\u001b[1;32m---> 39\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_policy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Update the target network only for Double DQN only\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m double_dqn \u001b[38;5;129;01mand\u001b[39;00m (frame \u001b[38;5;241m%\u001b[39m update_target_network_frequency)\u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mC:\\assignment5\\assignment5_materials\\agent_double.py:82\u001b[0m, in \u001b[0;36mAgent.train_policy_net\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m     80\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(mini_batch[\u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m     81\u001b[0m rewards \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(rewards)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 82\u001b[0m next_states \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.\u001b[39m\n\u001b[0;32m     83\u001b[0m dones \u001b[38;5;241m=\u001b[39m mini_batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;66;03m# checks if the game is over\u001b[39;00m\n\u001b[0;32m     84\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, dones\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)),dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    HISTORY_SIZE = len(history)\n",
    "    get_init_state(history, state[0], HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame):\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_ddqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_ddqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_ddqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
